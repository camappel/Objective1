[
  {
    "objectID": "maps.html",
    "href": "maps.html",
    "title": "Spatial Surge and Meteorological Maps",
    "section": "",
    "text": "This document provides spatial analysis of surge patterns and meteorological conditions for Storm 12 (closure event index 11) and Storm 25 (closure event index 24) for the Eastern Scheldt, converting the MATLAB script master6.m to Python. The analysis:\nThis provides a comprehensive view of the spatial patterns and meteorological conditions that led to the barrier closures for Storms 12 and 25.\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport xarray as xr\nimport os\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n\n# Configuration\ntint = 6  # Time interval for GTSM plots (hours)\ntint_met = 12  # Time interval for meteorological plots (hours)\noutput_dir = 'output'\ndata_dir = '../2_DATA'\n\n# Spatial bounds\nxb_gtsm = [-10, 10]  # GTSM longitude bounds\nyb_gtsm = [45, 60]   # GTSM latitude bounds\nxb_met = [-30, 15]   # ERA5 longitude bounds\nyb_met = [40, 70]    # ERA5 latitude bounds\n\nos.makedirs(output_dir, exist_ok=True)\n\nprint(f\"Analysis configuration:\")\nprint(f\"  Storms to process: Storm 12 (e=11, prefix '12_') and Storm 25 (e=24, prefix '25_')\")\nprint(f\"  GTSM time interval: {tint} hours\")\nprint(f\"  Meteorological time interval: {tint_met} hours\")\nprint(f\"  Output directory: {output_dir}\")\n\nAnalysis configuration:\n  Storms to process: Storm 12 (e=11, prefix '12_') and Storm 25 (e=24, prefix '25_')\n  GTSM time interval: 6 hours\n  Meteorological time interval: 12 hours\n  Output directory: output\n# Load barrier closure data from master1\nprint(\"Loading barrier closure data...\")\nmast1_file = os.path.join(output_dir, 'mast1.pkl')\nif not os.path.exists(mast1_file):\n    raise FileNotFoundError(\n        f\"Required file {mast1_file} not found. \"\n        \"Please run master1.qmd first to generate the required data file.\"\n    )\nwith open(mast1_file, 'rb') as f:\n    data1 = pickle.load(f)\n    OCD = data1['OCD']  # Observed closure dates\n\n# Convert to numpy array if needed\nOCD = np.array(OCD)\n\n# Validate event indices for both storms\ne_storm12 = 11\ne_storm25 = 24\nif e_storm12 &gt;= len(OCD):\n    raise ValueError(f\"Event index {e_storm12} (Storm 12) is out of range. There are {len(OCD)} closure events (indices 0-{len(OCD)-1})\")\nif e_storm25 &gt;= len(OCD):\n    raise ValueError(f\"Event index {e_storm25} (Storm 25) is out of range. There are {len(OCD)} closure events (indices 0-{len(OCD)-1})\")\n\nprint(f\"\\nData loaded:\")\nprint(f\"  Total closure dates: {len(OCD)} closures\")\nprint(f\"  Storm 12 closure date: {OCD[e_storm12]}\")\nprint(f\"  Storm 25 closure date: {OCD[e_storm25]}\")\n\nLoading barrier closure data...\n\nData loaded:\n  Total closure dates: 31 closures\n  Storm 12 closure date: 1992-11-11 00:00:00\n  Storm 25 closure date: 2013-12-05 00:00:00"
  },
  {
    "objectID": "maps.html#storm-12",
    "href": "maps.html#storm-12",
    "title": "Spatial Surge and Meteorological Maps",
    "section": "Storm 12",
    "text": "Storm 12\nStorm 12 corresponds to closure event index 11 (0-indexed) and uses datasets with prefix “12_” in the 2_DATA directory.\n\nLoad GTSM Surge Data\n\n# Storm 12 configuration\ne = 11  # Closure event index (0-indexed, so e=11 means Storm 12)\nstorm_num = 12\nprefix = '12_'\nclosure_date = OCD[e]\ngtsm_start_date = closure_date  # Start date for GTSM plots (use closure date)\nmet_start_date = closure_date - timedelta(hours=6)  # Start date for meteorological plots\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Processing Storm {storm_num} (event index {e}, prefix: {prefix})\")\nprint(f\"{'='*60}\")\nprint(f\"  Closure date: {closure_date}\")\n\n# Load GTSM reanalysis surge data\nfile_in = os.path.join(data_dir, '3_CODEC_GTSM', f'{prefix}reanalysis_surge_hourly_1992_11_v3.nc')\nprint(f\"\\nLoading GTSM data from: {file_in}\")\n\nds_gtsm = xr.open_dataset(file_in)\n\n# Extract data\nTS_GT = pd.to_datetime(ds_gtsm['time'].values)\nX_GT = ds_gtsm['station_x_coordinate'].values\nY_GT = ds_gtsm['station_y_coordinate'].values\nSU_GT = ds_gtsm['surge'].values\n\n# Filter to spatial bounds\nmask = (X_GT &gt;= xb_gtsm[0]) & (X_GT &lt;= xb_gtsm[1]) & (Y_GT &gt;= yb_gtsm[0]) & (Y_GT &lt;= yb_gtsm[1])\nX_GT = X_GT[mask]\nY_GT = Y_GT[mask]\nSU_GT = SU_GT[:, mask]  # Index stations (second axis), not time (first axis)\n\nprint(f\"  GTSM data loaded:\")\nprint(f\"    Time points: {len(TS_GT)}\")\nprint(f\"    Stations: {len(X_GT)}\")\nprint(f\"    Time range: {TS_GT[0]} to {TS_GT[-1]}\")\n\n\n============================================================\nProcessing Storm 12 (event index 11, prefix: 12_)\n============================================================\n  Closure date: 1992-11-11 00:00:00\n\nLoading GTSM data from: ../2_DATA/3_CODEC_GTSM/12_reanalysis_surge_hourly_1992_11_v3.nc\n  GTSM data loaded:\n    Time points: 720\n    Stations: 3873\n    Time range: 1992-11-01 00:00:00 to 1992-11-30 23:00:00\n\n\n\n\nLoad ERA5 Meteorological Data\n\n# Load ERA5 meteorological data\nfilein_met = os.path.join(data_dir, '4_ERA5', f'{prefix}ERA5_1992_11.nc')\nprint(f\"\\nLoading ERA5 data from: {filein_met}\")\n\nds_met = xr.open_dataset(filein_met)\n\n# Extract data\nLON = ds_met['longitude'].values\nLAT = ds_met['latitude'].values\nP_MET = ds_met['msl'].values  # Mean sea level pressure\nU_MET = ds_met['u10'].values   # 10m u-wind component\nV_MET = ds_met['v10'].values   # 10m v-wind component\nTS_MET = pd.to_datetime(ds_met['valid_time'].values)\n\n# Check and handle dimension order - ERA5 typically has (time, lat, lon) or (time, lon, lat)\n# We need to ensure arrays are in (lon, lat, time) order for consistent indexing\nprint(f\"  Original array shapes:\")\nprint(f\"    P_MET: {P_MET.shape}\")\nprint(f\"    U_MET: {U_MET.shape}\")\nprint(f\"    V_MET: {V_MET.shape}\")\n\n# Get dimension names to determine order\ndims = ds_met['msl'].dims\nprint(f\"    Dimension order: {dims}\")\n\n# Reorder to (lon, lat, time) if needed\nif len(P_MET.shape) == 3:\n    # Check if time dimension is first (could be 'time' or 'valid_time')\n    if 'time' in dims[0].lower() or dims[0] == 'valid_time':\n        # If time is first, transpose to (lon, lat, time) or (lat, lon, time)\n        if dims[1] == 'latitude' and dims[2] == 'longitude':\n            # (time, lat, lon) -&gt; (lon, lat, time)\n            P_MET = np.transpose(P_MET, (2, 1, 0))\n            U_MET = np.transpose(U_MET, (2, 1, 0))\n            V_MET = np.transpose(V_MET, (2, 1, 0))\n        elif dims[1] == 'longitude' and dims[2] == 'latitude':\n            # (time, lon, lat) -&gt; (lon, lat, time)\n            P_MET = np.transpose(P_MET, (1, 2, 0))\n            U_MET = np.transpose(U_MET, (1, 2, 0))\n            V_MET = np.transpose(V_MET, (1, 2, 0))\n    elif 'time' in dims[2].lower() or dims[2] == 'valid_time':\n        # Already in (lon, lat, time) or (lat, lon, time) order\n        if dims[0] == 'latitude' and dims[1] == 'longitude':\n            # (lat, lon, time) -&gt; (lon, lat, time)\n            P_MET = np.transpose(P_MET, (1, 0, 2))\n            U_MET = np.transpose(U_MET, (1, 0, 2))\n            V_MET = np.transpose(V_MET, (1, 0, 2))\n\nprint(f\"  After reordering:\")\nprint(f\"    P_MET: {P_MET.shape} (should be lon x lat x time)\")\n\n# Create meshgrid\nX_MET, Y_MET = np.meshgrid(LON, LAT)\nX_MET = X_MET.T\nY_MET = Y_MET.T\n\n# Filter to spatial bounds (for display)\nlon_mask = (LON &gt;= xb_met[0]) & (LON &lt;= xb_met[1])\nlat_mask = (LAT &gt;= yb_met[0]) & (LAT &lt;= yb_met[1])\nlon_idx = np.where(lon_mask)[0]\nlat_idx = np.where(lat_mask)[0]\n\nprint(f\"  ERA5 data loaded:\")\nprint(f\"    Time points: {len(TS_MET)}\")\nprint(f\"    Grid: {len(LON)} x {len(LAT)}\")\nprint(f\"    Time range: {TS_MET[0]} to {TS_MET[-1]}\")\nprint(f\"    P_MET final shape: {P_MET.shape} (lon x lat x time)\")\n\n\nLoading ERA5 data from: ../2_DATA/4_ERA5/12_ERA5_1992_11.nc\n  Original array shapes:\n    P_MET: (720, 121, 181)\n    U_MET: (720, 121, 181)\n    V_MET: (720, 121, 181)\n    Dimension order: ('valid_time', 'latitude', 'longitude')\n  After reordering:\n    P_MET: (181, 121, 720) (should be lon x lat x time)\n  ERA5 data loaded:\n    Time points: 720\n    Grid: 181 x 121\n    Time range: 1992-11-01 00:00:00 to 1992-11-30 23:00:00\n    P_MET final shape: (181, 121, 720) (lon x lat x time)\n\n\n\n\nFind Time Indices\n\n# Find time indices for GTSM plots\n# Find the time in TS_GT closest to the specified start date\ntime_diffs = np.abs([(t - gtsm_start_date).total_seconds() for t in TS_GT])\nt_gtsm_base = np.argmin(time_diffs)\n\n# Create 6 time steps starting from base time: 0, +tint, +2*tint, +3*tint, +4*tint, +5*tint\nt_gtsm = [t_gtsm_base, t_gtsm_base + tint, t_gtsm_base + tint*2, \n          t_gtsm_base + tint*3, t_gtsm_base + tint*4, t_gtsm_base + tint*5]\nt_gtsm = [max(0, min(t, len(TS_GT)-1)) for t in t_gtsm]  # Clamp to valid range\n\nprint(f\"\\nTime indices for GTSM plots:\")\nprint(f\"  Start date: {gtsm_start_date}\")\nprint(f\"  Base time: {TS_GT[t_gtsm_base]} (index {t_gtsm_base})\")\nfor i, t_idx in enumerate(t_gtsm):\n    print(f\"  Time step {i+1}: {TS_GT[t_idx]} (index {t_idx})\")\n\n# Find time indices for meteorological plots\nmax_time_idx = P_MET.shape[2] - 1\nvalid_ts_met = TS_MET[:max_time_idx+1]\n\n# Find 6 time steps around the closure\ntarget_times = [met_start_date + timedelta(hours=h) for h in [0, 12, 24, 36, 48, 60]]\nt_met = [np.argmin(np.abs([(t - target_time).total_seconds() for t in valid_ts_met])) for target_time in target_times]\n\n\nTime indices for GTSM plots:\n  Start date: 1992-11-11 00:00:00\n  Base time: 1992-11-11 00:00:00 (index 240)\n  Time step 1: 1992-11-11 00:00:00 (index 240)\n  Time step 2: 1992-11-11 06:00:00 (index 246)\n  Time step 3: 1992-11-11 12:00:00 (index 252)\n  Time step 4: 1992-11-11 18:00:00 (index 258)\n  Time step 5: 1992-11-12 00:00:00 (index 264)\n  Time step 6: 1992-11-12 06:00:00 (index 270)\n\n\n\n\nGTSM Surge Maps\n\nimport matplotlib.gridspec as gridspec\n\nfig = plt.figure(figsize=(16, 10))\ngs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\naxes = [fig.add_subplot(gs[i // 3, i % 3]) for i in range(6)]\n\nfor i, t_idx in enumerate(t_gtsm):\n    ax = axes[i]\n    scatter = ax.scatter(X_GT, Y_GT, c=SU_GT[t_idx, :], cmap='jet', s=15, vmin=-0.5, vmax=1.0)\n    ax.set_xlim(xb_gtsm)\n    ax.set_ylim(yb_gtsm)\n    ax.set_aspect('equal')\n    ax.grid(True, alpha=0.3)\n    ax.tick_params(labelsize=12)\n    ax.set_title(TS_GT[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n    if i &gt;= 3:  # Bottom row\n        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n    if i % 3 == 0:  # Left column\n        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n\n# Add vertical colorbar at far right\ncbar_ax = fig.add_subplot(gs[:, 3])\ncbar = plt.colorbar(scatter, cax=cbar_ax, orientation='vertical')\ncbar.set_label('Surge (m)', fontweight='bold', fontsize=14)\ncbar.ax.tick_params(labelsize=12)\n\nplt.tight_layout(rect=[0, 0, 0.97, 1])  # Leave space for colorbar\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, f'maps_storm{storm_num}_gtsm_surge.png')\nplt.savefig(fig_file, dpi=100, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n\n/var/folders/m8/cp78hgjj6j937vhj63cdm7_c0000gn/T/ipykernel_10274/1136680818.py:27: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout(rect=[0, 0, 0.97, 1])  # Leave space for colorbar\n\n\n\n\n\nGTSM reanalysis surge maps at 6 time steps around Storm 12 closure event\n\n\n\n\n\nFigure saved to output/maps_storm12_gtsm_surge.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\nTime Step 1Time Step 2Time Step 3Time Step 4Time Step 5Time Step 6\n\n\n\n\n\nGTSM surge at time step 1\n\n\n\n\n\n\n\nGTSM surge at time step 2\n\n\n\n\n\n\n\nGTSM surge at time step 3\n\n\n\n\n\n\n\nGTSM surge at time step 4\n\n\n\n\n\n\n\nGTSM surge at time step 5\n\n\n\n\n\n\n\nGTSM surge at time step 6\n\n\n\n\n\n\n\nERA5 Meteorological Maps\n\nimport matplotlib.gridspec as gridspec\n\nint_skip = 3  # Skip every 3rd grid point for wind vectors\n\n# Prepare data for all time steps\nX_sub = X_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\nY_sub = Y_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\nX_vec = X_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\nY_vec = Y_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\n\n# Create figure with 2x3 grid + colorbar\nfig = plt.figure(figsize=(16, 10))\ngs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\naxes = [fig.add_subplot(gs[i // 3, i % 3]) for i in range(6)]\n\n# Plot each time step\nims = []\nfor i, t_idx in enumerate(t_met):\n    ax = axes[i]\n    \n    # Get data for this time step\n    P_sub = P_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1, t_idx] / 100\n    U_vec = U_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n    V_vec = V_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n    \n    # Plot pressure field\n    im = ax.pcolormesh(X_sub, Y_sub, P_sub, cmap='jet', shading='gouraud', vmin=967, vmax=1020)\n    ims.append(im)\n    \n    # Plot wind vectors\n    ax.quiver(X_vec, Y_vec, U_vec, V_vec, color='k', scale=1200, width=0.002)\n    \n    # Formatting\n    ax.set_xlim(xb_met)\n    ax.set_ylim(yb_met)\n    ax.set_aspect('equal')\n    ax.grid(True, alpha=0.3)\n    ax.tick_params(labelsize=12)\n    ax.set_title(TS_MET[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n    \n    if i &gt;= 3:  # Bottom row\n        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n    if i % 3 == 0:  # Left column\n        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n\n# Add vertical colorbar at far right\ncbar_ax = fig.add_subplot(gs[:, 3])\ncbar = plt.colorbar(ims[0], cax=cbar_ax, orientation='vertical')\ncbar.set_label('Pressure (hPa)', fontweight='bold', fontsize=14)\ncbar.ax.tick_params(labelsize=12)\n\nplt.tight_layout(rect=[0, 0, 0.97, 1])  # Leave space for colorbar\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, f'maps_storm{storm_num}_era5_meteorology.png')\nplt.savefig(fig_file, dpi=100, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n\n/var/folders/m8/cp78hgjj6j937vhj63cdm7_c0000gn/T/ipykernel_10274/2600622236.py:52: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout(rect=[0, 0, 0.97, 1])  # Leave space for colorbar\n\n\n\n\n\nERA5 reanalysis pressure and wind fields at 6 time steps around Storm 12 closure event\n\n\n\n\n\nFigure saved to output/maps_storm12_era5_meteorology.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\nTime Step 1Time Step 2Time Step 3Time Step 4Time Step 5Time Step 6\n\n\n\n\n\nERA5 meteorology at time step 1\n\n\n\n\n\n\n\nERA5 meteorology at time step 2\n\n\n\n\n\n\n\nERA5 meteorology at time step 3\n\n\n\n\n\n\n\nERA5 meteorology at time step 4\n\n\n\n\n\n\n\nERA5 meteorology at time step 5\n\n\n\n\n\n\n\nERA5 meteorology at time step 6"
  },
  {
    "objectID": "maps.html#storm-25",
    "href": "maps.html#storm-25",
    "title": "Spatial Surge and Meteorological Maps",
    "section": "Storm 25",
    "text": "Storm 25\nStorm 25 corresponds to closure event index 24 (0-indexed) and uses datasets with prefix “25_” in the 2_DATA directory.\n\nLoad GTSM Surge Data\n\n# Storm 25 configuration\ne = 24  # Closure event index (0-indexed, so e=24 means Storm 25)\nstorm_num = 25\nprefix = '25_'\nclosure_date = OCD[e]\ngtsm_start_date = closure_date  # Start date for GTSM plots (use closure date)\nmet_start_date = closure_date - timedelta(hours=6)  # Start date for meteorological plots\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Processing Storm {storm_num} (event index {e}, prefix: {prefix})\")\nprint(f\"{'='*60}\")\nprint(f\"  Closure date: {closure_date}\")\n\n# Load GTSM reanalysis surge data\nfile_in = os.path.join(data_dir, '3_CODEC_GTSM', f'{prefix}reanalysis_surge_hourly_2013_12_v3.nc')\nprint(f\"\\nLoading GTSM data from: {file_in}\")\n\nds_gtsm = xr.open_dataset(file_in)\n\n# Extract data\nTS_GT = pd.to_datetime(ds_gtsm['time'].values)\nX_GT = ds_gtsm['station_x_coordinate'].values\nY_GT = ds_gtsm['station_y_coordinate'].values\nSU_GT = ds_gtsm['surge'].values\n\n# Filter to spatial bounds\nmask = (X_GT &gt;= xb_gtsm[0]) & (X_GT &lt;= xb_gtsm[1]) & (Y_GT &gt;= yb_gtsm[0]) & (Y_GT &lt;= yb_gtsm[1])\nX_GT = X_GT[mask]\nY_GT = Y_GT[mask]\nSU_GT = SU_GT[:, mask]  # Index stations (second axis), not time (first axis)\n\nprint(f\"  GTSM data loaded:\")\nprint(f\"    Time points: {len(TS_GT)}\")\nprint(f\"    Stations: {len(X_GT)}\")\nprint(f\"    Time range: {TS_GT[0]} to {TS_GT[-1]}\")\n\n\n============================================================\nProcessing Storm 25 (event index 24, prefix: 25_)\n============================================================\n  Closure date: 2013-12-05 00:00:00\n\nLoading GTSM data from: ../2_DATA/3_CODEC_GTSM/25_reanalysis_surge_hourly_2013_12_v3.nc\n  GTSM data loaded:\n    Time points: 744\n    Stations: 3873\n    Time range: 2013-12-01 00:00:00 to 2013-12-31 23:00:00\n\n\n\n\nLoad ERA5 Meteorological Data\n\n# Load ERA5 meteorological data\nfilein_met = os.path.join(data_dir, '4_ERA5', f'{prefix}ERA5_2013_12.nc')\nprint(f\"\\nLoading ERA5 data from: {filein_met}\")\n\nds_met = xr.open_dataset(filein_met)\n\n# Extract data\nLON = ds_met['longitude'].values\nLAT = ds_met['latitude'].values\nP_MET = ds_met['msl'].values  # Mean sea level pressure\nU_MET = ds_met['u10'].values   # 10m u-wind component\nV_MET = ds_met['v10'].values   # 10m v-wind component\nTS_MET = pd.to_datetime(ds_met['valid_time'].values)\n\n# Check and handle dimension order - ERA5 typically has (time, lat, lon) or (time, lon, lat)\n# We need to ensure arrays are in (lon, lat, time) order for consistent indexing\nprint(f\"  Original array shapes:\")\nprint(f\"    P_MET: {P_MET.shape}\")\nprint(f\"    U_MET: {U_MET.shape}\")\nprint(f\"    V_MET: {V_MET.shape}\")\n\n# Get dimension names to determine order\ndims = ds_met['msl'].dims\nprint(f\"    Dimension order: {dims}\")\n\n# Reorder to (lon, lat, time) if needed\nif len(P_MET.shape) == 3:\n    # Check if time dimension is first (could be 'time' or 'valid_time')\n    if 'time' in dims[0].lower() or dims[0] == 'valid_time':\n        # If time is first, transpose to (lon, lat, time) or (lat, lon, time)\n        if dims[1] == 'latitude' and dims[2] == 'longitude':\n            # (time, lat, lon) -&gt; (lon, lat, time)\n            P_MET = np.transpose(P_MET, (2, 1, 0))\n            U_MET = np.transpose(U_MET, (2, 1, 0))\n            V_MET = np.transpose(V_MET, (2, 1, 0))\n        elif dims[1] == 'longitude' and dims[2] == 'latitude':\n            # (time, lon, lat) -&gt; (lon, lat, time)\n            P_MET = np.transpose(P_MET, (1, 2, 0))\n            U_MET = np.transpose(U_MET, (1, 2, 0))\n            V_MET = np.transpose(V_MET, (1, 2, 0))\n    elif 'time' in dims[2].lower() or dims[2] == 'valid_time':\n        # Already in (lon, lat, time) or (lat, lon, time) order\n        if dims[0] == 'latitude' and dims[1] == 'longitude':\n            # (lat, lon, time) -&gt; (lon, lat, time)\n            P_MET = np.transpose(P_MET, (1, 0, 2))\n            U_MET = np.transpose(U_MET, (1, 0, 2))\n            V_MET = np.transpose(V_MET, (1, 0, 2))\n\nprint(f\"  After reordering:\")\nprint(f\"    P_MET: {P_MET.shape} (should be lon x lat x time)\")\n\n# Create meshgrid\nX_MET, Y_MET = np.meshgrid(LON, LAT)\nX_MET = X_MET.T\nY_MET = Y_MET.T\n\n# Filter to spatial bounds (for display)\nlon_mask = (LON &gt;= xb_met[0]) & (LON &lt;= xb_met[1])\nlat_mask = (LAT &gt;= yb_met[0]) & (LAT &lt;= yb_met[1])\nlon_idx = np.where(lon_mask)[0]\nlat_idx = np.where(lat_mask)[0]\n\nprint(f\"  ERA5 data loaded:\")\nprint(f\"    Time points: {len(TS_MET)}\")\nprint(f\"    Grid: {len(LON)} x {len(LAT)}\")\nprint(f\"    Time range: {TS_MET[0]} to {TS_MET[-1]}\")\nprint(f\"    P_MET final shape: {P_MET.shape} (lon x lat x time)\")\n\n\nLoading ERA5 data from: ../2_DATA/4_ERA5/25_ERA5_2013_12.nc\n  Original array shapes:\n    P_MET: (744, 121, 181)\n    U_MET: (744, 121, 181)\n    V_MET: (744, 121, 181)\n    Dimension order: ('valid_time', 'latitude', 'longitude')\n  After reordering:\n    P_MET: (181, 121, 744) (should be lon x lat x time)\n  ERA5 data loaded:\n    Time points: 744\n    Grid: 181 x 121\n    Time range: 2013-12-01 00:00:00 to 2013-12-31 23:00:00\n    P_MET final shape: (181, 121, 744) (lon x lat x time)\n\n\n\n\nFind Time Indices\n\n# Find time indices for GTSM plots\n# Find the time in TS_GT closest to the specified start date\ntime_diffs = np.abs([(t - gtsm_start_date).total_seconds() for t in TS_GT])\nt_gtsm_base = np.argmin(time_diffs)\n\n# Create 6 time steps starting from base time: 0, +tint, +2*tint, +3*tint, +4*tint, +5*tint\nt_gtsm = [t_gtsm_base, t_gtsm_base + tint, t_gtsm_base + tint*2, \n          t_gtsm_base + tint*3, t_gtsm_base + tint*4, t_gtsm_base + tint*5]\nt_gtsm = [max(0, min(t, len(TS_GT)-1)) for t in t_gtsm]  # Clamp to valid range\n\nprint(f\"\\nTime indices for GTSM plots:\")\nprint(f\"  Start date: {gtsm_start_date}\")\nprint(f\"  Base time: {TS_GT[t_gtsm_base]} (index {t_gtsm_base})\")\nfor i, t_idx in enumerate(t_gtsm):\n    print(f\"  Time step {i+1}: {TS_GT[t_idx]} (index {t_idx})\")\n\n# Find time indices for meteorological plots\nmax_time_idx = P_MET.shape[2] - 1\nvalid_ts_met = TS_MET[:max_time_idx+1]\n\n# Find 6 time steps around the closure\ntarget_times = [met_start_date + timedelta(hours=h) for h in [0, 12, 24, 36, 48, 60]]\nt_met = [np.argmin(np.abs([(t - target_time).total_seconds() for t in valid_ts_met])) for target_time in target_times]\n\n\nTime indices for GTSM plots:\n  Start date: 2013-12-05 00:00:00\n  Base time: 2013-12-05 00:00:00 (index 96)\n  Time step 1: 2013-12-05 00:00:00 (index 96)\n  Time step 2: 2013-12-05 06:00:00 (index 102)\n  Time step 3: 2013-12-05 12:00:00 (index 108)\n  Time step 4: 2013-12-05 18:00:00 (index 114)\n  Time step 5: 2013-12-06 00:00:00 (index 120)\n  Time step 6: 2013-12-06 06:00:00 (index 126)\n\n\n\n\nGTSM Surge Maps\n\nimport matplotlib.gridspec as gridspec\n\nfig = plt.figure(figsize=(16, 10))\ngs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\naxes = [fig.add_subplot(gs[i // 3, i % 3]) for i in range(6)]\n\nfor i, t_idx in enumerate(t_gtsm):\n    ax = axes[i]\n    scatter = ax.scatter(X_GT, Y_GT, c=SU_GT[t_idx, :], cmap='jet', s=15, vmin=-0.5, vmax=1.0)\n    ax.set_xlim(xb_gtsm)\n    ax.set_ylim(yb_gtsm)\n    ax.set_aspect('equal')\n    ax.grid(True, alpha=0.3)\n    ax.tick_params(labelsize=12)\n    ax.set_title(TS_GT[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n    if i &gt;= 3:  # Bottom row\n        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n    if i % 3 == 0:  # Left column\n        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n\n# Add vertical colorbar at far right\ncbar_ax = fig.add_subplot(gs[:, 3])\ncbar = plt.colorbar(scatter, cax=cbar_ax, orientation='vertical')\ncbar.set_label('Surge (m)', fontweight='bold', fontsize=14)\ncbar.ax.tick_params(labelsize=12)\n\nplt.tight_layout(rect=[0, 0, 0.97, 1])  # Leave space for colorbar\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, f'maps_storm{storm_num}_gtsm_surge.png')\nplt.savefig(fig_file, dpi=100, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n\n/var/folders/m8/cp78hgjj6j937vhj63cdm7_c0000gn/T/ipykernel_10274/1136680818.py:27: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout(rect=[0, 0, 0.97, 1])  # Leave space for colorbar\n\n\n\n\n\nGTSM reanalysis surge maps at 6 time steps around Storm 25 closure event\n\n\n\n\n\nFigure saved to output/maps_storm25_gtsm_surge.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\nTime Step 1Time Step 2Time Step 3Time Step 4Time Step 5Time Step 6\n\n\n\n\n\nGTSM surge at time step 1\n\n\n\n\n\n\n\nGTSM surge at time step 2\n\n\n\n\n\n\n\nGTSM surge at time step 3\n\n\n\n\n\n\n\nGTSM surge at time step 4\n\n\n\n\n\n\n\nGTSM surge at time step 5\n\n\n\n\n\n\n\nGTSM surge at time step 6\n\n\n\n\n\n\n\nERA5 Meteorological Maps\n\nimport matplotlib.gridspec as gridspec\n\nint_skip = 3  # Skip every 3rd grid point for wind vectors\n\n# Prepare data for all time steps\nX_sub = X_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\nY_sub = Y_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\nX_vec = X_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\nY_vec = Y_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\n\n# Create figure with 2x3 grid + colorbar\nfig = plt.figure(figsize=(16, 10))\ngs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\naxes = [fig.add_subplot(gs[i // 3, i % 3], projection=ccrs.PlateCarree()) for i in range(6)]\n\n# Plot each time step\nims = []\nfor i, t_idx in enumerate(t_met):\n    ax = axes[i]\n    \n    # Add white coastline and borders\n    ax.add_feature(cfeature.COASTLINE.with_scale('50m'), linewidth=0.8, color='white')\n    ax.add_feature(cfeature.BORDERS.with_scale('50m'), linewidth=0.5, color='white')\n    \n    # Get data for this time step\n    P_sub = P_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1, t_idx] / 100\n    U_vec = U_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n    V_vec = V_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n    \n    # Plot pressure field\n    im = ax.pcolormesh(X_sub, Y_sub, P_sub, cmap='jet', shading='gouraud', \n                       vmin=967, vmax=1020, transform=ccrs.PlateCarree())\n    ims.append(im)\n    \n    # Plot wind vectors\n    ax.quiver(X_vec, Y_vec, U_vec, V_vec, color='k', scale=1200, width=0.002, \n              transform=ccrs.PlateCarree())\n    \n    # Formatting\n    ax.set_xlim(xb_met)\n    ax.set_ylim(yb_met)\n    ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.3, linestyle='--')\n    ax.tick_params(labelsize=12)\n    ax.set_title(TS_MET[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n    \n    if i &gt;= 3:  # Bottom row\n        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n    if i % 3 == 0:  # Left column\n        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n\n# Add vertical colorbar at far right\ncbar_ax = fig.add_subplot(gs[:, 3])\ncbar = plt.colorbar(ims[0], cax=cbar_ax, orientation='vertical')\ncbar.set_label('Pressure (hPa)', fontweight='bold', fontsize=14)\ncbar.ax.tick_params(labelsize=12)\n\nplt.tight_layout(rect=[0, 0, 0.97, 1])  # Leave space for colorbar\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, f'maps_storm{storm_num}_era5_meteorology.png')\nplt.savefig(fig_file, dpi=100, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/var/folders/m8/cp78hgjj6j937vhj63cdm7_c0000gn/T/ipykernel_10274/1233508593.py:57: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout(rect=[0, 0, 0.97, 1])  # Leave space for colorbar\n\n\n\n\n\nERA5 reanalysis pressure and wind fields at 6 time steps around Storm 25 closure event\n\n\n\n\n\nFigure saved to output/maps_storm25_era5_meteorology.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\nTime Step 1Time Step 2Time Step 3Time Step 4Time Step 5Time Step 6\n\n\n\n\n\nERA5 meteorology at time step 1\n\n\n\n\n\n\n\nERA5 meteorology at time step 2\n\n\n\n\n\n\n\nERA5 meteorology at time step 3\n\n\n\n\n\n\n\nERA5 meteorology at time step 4\n\n\n\n\n\n\n\nERA5 meteorology at time step 5\n\n\n\n\n\n\n\nERA5 meteorology at time step 6"
  },
  {
    "objectID": "maps.html#summary",
    "href": "maps.html#summary",
    "title": "Spatial Surge and Meteorological Maps",
    "section": "Summary",
    "text": "Summary\nThis comprehensive analysis provides detailed insight into Storm 12 and Storm 25 closure event spatial patterns:\n\nEvent focus: Detailed analysis of Storm 12 (closure event index 11, prefix “12_”) and Storm 25 (closure event index 24, prefix “25_”) for the Eastern Scheldt\nSpatial surge patterns: GTSM reanalysis shows how surge developed and propagated spatially at 6-hour intervals for each storm\nMeteorological forcing: ERA5 reanalysis shows the pressure patterns and wind fields that drove the surge at 6 time steps displayed in a faceted layout for each storm\nTemporal evolution: The 6 time steps capture the development, peak, and decay of each storm system\nDataset identification: The prefix identifiers (“12_” and “25_”) are used to identify the corresponding datasets in the 2_DATA directory\n\nThis multi-faceted view helps understand the physical processes that led to the barrier closures for Storms 12 and 25, from large-scale meteorological forcing to spatial surge development."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Barrier Book",
    "section": "",
    "text": "Barrier Book is an interactive documentation and analysis site exploring the functioning, operation, meteorological context, and flood defense performance of major storm surge barriers.\nThis site presents: - Barrier closure histories: Analyzing when and why flood barriers were closed over the past decades. - Tidal and water level data: Processing long-term tide gauge observations and predicted astronomical tides. - Meteorological conditions during storms: Examining wind, pressure, and storm data associated with each closure. - Spatio-temporal characteristics: Exploring the spatial and temporal patterns of storm events and barrier operations. - Visualizations: Comprehensive figures showing barrier operations, extreme sea levels, meteorological extremes, and trends. - Workflows and notebooks: Step-by-step technical analyses and reproducible code using Python and Quarto.\nBarrier Book is designed for researchers, civil engineers, and decision-makers interested in flood risk, coastal defense, and climate adaptation.\nAll analysis code and workflows are open and reproducible, with data sourced from official Dutch and UK agencies."
  },
  {
    "objectID": "thames.html",
    "href": "thames.html",
    "title": "Thames Barrier",
    "section": "",
    "text": "This document provides a complete analysis workflow for the Thames barrier system, combining barrier closure analysis, tide gauge data processing, tidal analysis, and visualization. The analysis covers the period from 1986 to 2024 and includes:\nWater years run from July 1 to June 30, which is more appropriate for coastal flood analysis than calendar years."
  },
  {
    "objectID": "thames.html#analysis",
    "href": "thames.html#analysis",
    "title": "Thames Barrier",
    "section": "Analysis",
    "text": "Analysis\n\n# 2. Count closures per water year (July 1 to July 1)\nOCS = []  # Will store [index, year, year+1, num_closures]\nYT = []   # Will store year labels like \"1986/87\"\n\nfor co, y in enumerate(Y, start=1):\n    # Find closures between July 1 of year y and July 1 of year y+1\n    start_date = datetime(y, 7, 1, 0, 0, 0)\n    end_date = datetime(y + 1, 7, 1, 0, 0, 0)\n    \n    # Count closures in this water year\n    closures_in_year = [d for d in OCD if start_date &lt;= d &lt; end_date]\n    num_closures = len(closures_in_year)\n    \n    OCS.append([co, y, y + 1, num_closures])\n    \n    # Create year label (e.g., \"1986/87\")\n    year_str = str(y + 1)\n    YT.append(f\"{y}/{year_str[2:]}\")\n\nOCS = np.array(OCS)\n\nprint(f\"\\nClosures per water year calculated for {len(OCS)} years\")\nprint(f\"  Total closures: {np.sum(OCS[:, 3])}\")\n\n\nClosures per water year calculated for 39 years\n  Total closures: 219\n\n\n\n# 3. Calculate statistics\nE = np.array([\n    np.min(OCS[:, 3]),      # Minimum closures per year\n    np.mean(OCS[:, 3]),     # Mean closures per year\n    np.max(OCS[:, 3]),      # Maximum closures per year\n    np.sum(OCS[:, 3])       # Total closures\n])\n\nprint(f\"\\nClosure statistics:\")\nprint(f\"  Min per year: {E[0]:.1f}\")\nprint(f\"  Mean per year: {E[1]:.2f}\")\nprint(f\"  Max per year: {E[2]:.1f}\")\nprint(f\"  Total: {E[3]:.0f}\")\n\n\nClosure statistics:\n  Min per year: 0.0\n  Mean per year: 5.62\n  Max per year: 50.0\n  Total: 219\n\n\n\n# 4. Save data\noutput_file = os.path.join(output_dir, 'mast1_thames.pkl')\nwith open(output_file, 'wb') as f:\n    pickle.dump({\n        'OCD': OCD,\n        'OCS': OCS,\n        'E': E,\n        'YT': YT\n    }, f)\nprint(f\"\\nData saved to {output_file}\")\n\n\nData saved to output/mast1_thames.pkl"
  },
  {
    "objectID": "thames.html#visualization",
    "href": "thames.html#visualization",
    "title": "Thames Barrier",
    "section": "Visualization",
    "text": "Visualization\nThis bar chart shows the number of barrier closures per water year from 1986/87 to 2024/25. The Thames Barrier closes when water levels exceed a threshold, typically during storm surge events.\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax.bar(OCS[:, 0], OCS[:, 3], color='black')\nax.set_xlim(0.2, len(OCS) + 0.8)\nax.set_xticks(range(1, len(OCS) + 1))\nax.set_xticklabels(YT, rotation=90)\n# Note: y-axis limit may need adjustment based on actual data\nmax_closures = int(np.max(OCS[:, 3])) + 1\nax.set_ylim(0, max_closures)\nax.set_yticks(range(0, max_closures + 1, max(1, max_closures // 10)))\nax.set_ylabel('Number of closures', fontweight='bold', fontsize=18)\nax.set_title('Thames Barrier', fontweight='bold', fontsize=18)\nax.grid(True, alpha=0.3)\nax.tick_params(labelsize=16)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, 'master1_thames_closures.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n\n\n\n\nNumber of Thames Barrier closures per water year (July 1 to June 30), 1986-2024\n\n\n\n\n\nFigure saved to output/master1_thames_closures.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nThe following table lists all barrier closure dates, which correspond to storm events that triggered the Thames Barrier.\n\n# Create DataFrame with closure dates\nstorms_df = pd.DataFrame({\n    'Closure Date': [d.strftime('%Y-%m-%d %H:%M') if isinstance(d, datetime) else str(d) for d in OCD],\n    'Year': [d.year if isinstance(d, datetime) else None for d in OCD],\n    'Month': [d.strftime('%B') if isinstance(d, datetime) else None for d in OCD],\n    'YearMonth': [(d.year, d.month) if isinstance(d, datetime) else None for d in OCD]\n})\n\n# Add water year information\ndef get_water_year(date):\n    \"\"\"Determine water year (July 1 to June 30)\"\"\"\n    if isinstance(date, datetime):\n        if date.month &gt;= 7:\n            return f\"{date.year}/{str(date.year + 1)[2:]}\"\n        else:\n            return f\"{date.year - 1}/{str(date.year)[2:]}\"\n    return None\n\nstorms_df['Water Year'] = [get_water_year(d) for d in OCD]\n\n# Create index that groups storms by year and month\n# Storms in the same year and month get the same index\nunique_year_months = storms_df['YearMonth'].unique()\nyear_month_to_index = {ym: idx + 1 for idx, ym in enumerate(sorted(unique_year_months))}\nstorms_df['Event'] = storms_df['YearMonth'].map(year_month_to_index)\n\n# Reorder columns (Index first, then remove YearMonth helper column)\nstorms_df = storms_df[['Event', 'Closure Date', 'Water Year', 'Year', 'Month']]\n\n# Display table\nprint(f\"\\nTotal number of closures: {len(storms_df)}\")\nstorms_df\n\n\nTotal number of closures: 221\n\n\n\n\n\n\nComplete list of Thames Barrier closures (storm events), 1986-2024\n\n\n\nEvent\nClosure Date\nWater Year\nYear\nMonth\n\n\n\n\n0\n1\n1983-02-01 00:00\n1982/83\n1983\nFebruary\n\n\n1\n2\n1985-12-26 00:00\n1985/86\n1985\nDecember\n\n\n2\n3\n1987-03-29 00:00\n1986/87\n1987\nMarch\n\n\n3\n4\n1988-12-24 00:00\n1988/89\n1988\nDecember\n\n\n4\n5\n1990-02-12 00:00\n1989/90\n1990\nFebruary\n\n\n...\n...\n...\n...\n...\n...\n\n\n216\n83\n2024-03-11 00:00\n2023/24\n2024\nMarch\n\n\n217\n83\n2024-03-14 00:00\n2023/24\n2024\nMarch\n\n\n218\n83\n2024-03-14 00:00\n2023/24\n2024\nMarch\n\n\n219\n84\n2024-04-08 00:00\n2023/24\n2024\nApril\n\n\n220\n84\n2024-04-09 00:00\n2023/24\n2024\nApril\n\n\n\n\n221 rows × 5 columns"
  },
  {
    "objectID": "thames.html#save-results-part-2",
    "href": "thames.html#save-results-part-2",
    "title": "Thames Barrier",
    "section": "Save Results (Part 2)",
    "text": "Save Results (Part 2)\nSave the combined time series to a pickle file for use in subsequent analyses.\n\n# Save data\noutput_file = os.path.join(output_dir, 'mast2_thames.pkl')\nwith open(output_file, 'wb') as f:\n    pickle.dump({\n        'TSP': TSP,\n        'WLP': WLP,\n        'TSCHECK': TSP  # Reference time series\n    }, f)\nprint(f\"\\nData saved to {output_file}\")\n\n\nData saved to output/mast2_thames.pkl"
  },
  {
    "objectID": "thames.html#visualization-part-2",
    "href": "thames.html#visualization-part-2",
    "title": "Thames Barrier",
    "section": "Visualization (Part 2)",
    "text": "Visualization (Part 2)\nThis figure shows the combined tide gauge water level time series from 1986 to 2023 at 10-minute intervals. The data shows tidal variations, storm surges, and long-term water level patterns over the period.\n\nfig, ax = plt.subplots(figsize=(14, 6))\nax.plot(TSP, WLP, 'b', linewidth=0.5)\nax.set_xlabel('Date', fontweight='bold', fontsize=20)\nax.set_ylabel('Water level (m NAP)', fontweight='bold', fontsize=20)\nax.grid(True, alpha=0.3)\nax.tick_params(labelsize=16)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, 'master2_thames_tide_gauge.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n\n\n\n\nCombined tide gauge water level time series for the Thames, 1986-2023. Data at 10-minute intervals.\n\n\n\n\n\nFigure saved to output/master2_thames_tide_gauge.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "thames.html#setup-and-configuration",
    "href": "thames.html#setup-and-configuration",
    "title": "Thames Barrier",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\n\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom pytides.tide import Tide\nimport os\n\n# Configuration\nY = list(range(1986, 2024))  # Years 1986 to 2023\nth = 60  # Data quality threshold (percentage)\nlat = 51.5  # Latitude for tidal analysis (Thames Barrier approximate location)\noutput_dir = 'output'\n\n# Create output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\nprint(f\"Analysis configuration:\")\nprint(f\"  Years: {Y[0]} to {Y[-1]} ({len(Y)} years)\")\nprint(f\"  Data quality threshold: {th}%\")\nprint(f\"  Latitude: {lat}°\")\n\nAnalysis configuration:\n  Years: 1986 to 2023 (38 years)\n  Data quality threshold: 60%\n  Latitude: 51.5°"
  },
  {
    "objectID": "thames.html#load-tide-gauge-data",
    "href": "thames.html#load-tide-gauge-data",
    "title": "Thames Barrier",
    "section": "Load Tide Gauge Data",
    "text": "Load Tide Gauge Data\n\nprint(\"Loading tide gauge data...\")\nmast2_file = os.path.join(output_dir, 'mast2_thames.pkl')\nif not os.path.exists(mast2_file):\n    raise FileNotFoundError(\n        f\"Required file {mast2_file} not found. \"\n        \"Please run Part 2 (Tide Gauge Data Processing) first to generate the required data file.\"\n    )\nwith open(mast2_file, 'rb') as f:\n    data = pickle.load(f)\n    TSP = data['TSP']\n    WLP = data['WLP']\n\n# Convert to numpy arrays if needed\nTSP = np.array(TSP)\nWLP = np.array(WLP)\n\nprint(f\"Loaded {len(TSP):,} data points\")\nprint(f\"  Start: {TSP[0]}\")\nprint(f\"  End: {TSP[-1]}\")\nprint(f\"  Valid data: {np.sum(~np.isnan(WLP)):,} points ({100*np.sum(~np.isnan(WLP))/len(WLP):.1f}%)\")\n\nLoading tide gauge data...\nLoaded 1,998,576 data points\n  Start: 1986-01-01 00:00:00\n  End: 2023-12-31 23:50:00\n  Valid data: 1,124,886 points (56.3%)"
  },
  {
    "objectID": "thames.html#calculate-data-quality-per-year",
    "href": "thames.html#calculate-data-quality-per-year",
    "title": "Thames Barrier",
    "section": "Calculate Data Quality Per Year",
    "text": "Calculate Data Quality Per Year\n\nprint(\"\\nCalculating data quality per year...\")\nDQ = []\n\nfor y in Y:\n    # Find data points in this calendar year (Jan 1 to Jan 1 next year)\n    start_date = datetime(y, 1, 1, 0, 0, 0)\n    end_date = datetime(y + 1, 1, 1, 0, 0, 0)\n    mask = (TSP &gt;= start_date) & (TSP &lt; end_date)\n    j = np.where(mask)[0]\n    \n    if len(j) == 0:\n        quality = 0\n    else:\n        # Count non-NaN values\n        k = np.where(~np.isnan(WLP[j]))[0]\n        quality = (len(k) / len(j)) * 100\n    \n    # Initialize with 3 columns: [year, quality, reference_year]\n    # reference_year will be filled in during tidal analysis\n    DQ.append([y, quality, np.nan])\n\nDQ = np.array(DQ)\n\n# Display summary statistics\nprint(f\"\\nData quality summary:\")\nprint(f\"  Range: {np.min(DQ[:, 1]):.1f}% to {np.max(DQ[:, 1]):.1f}%\")\nprint(f\"  Mean: {np.mean(DQ[:, 1]):.1f}%\")\nprint(f\"  Years with quality &gt;= {th}%: {np.sum(DQ[:, 1] &gt;= th)}/{len(Y)}\")\n\n\nCalculating data quality per year...\n\nData quality summary:\n  Range: 0.0% to 100.0%\n  Mean: 56.3%\n  Years with quality &gt;= 60%: 17/38"
  },
  {
    "objectID": "thames.html#tidal-analysis-and-prediction",
    "href": "thames.html#tidal-analysis-and-prediction",
    "title": "Thames Barrier",
    "section": "Tidal Analysis and Prediction",
    "text": "Tidal Analysis and Prediction\nFor each target year, the analysis determines a reference year, extracts data, performs harmonic decomposition, and generates predictions.\n\nprint(\"\\nPerforming tidal analysis and prediction...\")\nTSP2 = []\nTIP = []\n\nfor co, y in enumerate(Y):\n    print(f\"  Processing year {y} ({co+1}/{len(Y)})...\")\n    \n    # Determine reference year\n    if DQ[co, 1] &gt;= th:\n        # Use target year as reference\n        yr = y\n        DQ[co, 2] = yr\n    else:\n        # Find nearest year with quality &gt;= threshold\n        # Calculate distances from target year\n        distances = np.abs(DQ[:, 0] - y)\n        \n        # Filter to only years with quality &gt;= threshold\n        good_mask = DQ[:, 1] &gt;= th\n        \n        if np.any(good_mask):\n            # Get distances for good years only\n            good_distances = distances[good_mask]\n            good_indices = np.where(good_mask)[0]\n            \n            # Find nearest good year\n            nearest_idx = good_indices[np.argmin(good_distances)]\n            yr = int(DQ[nearest_idx, 0])\n        else:\n            # Fallback: use year with best quality\n            best_idx = np.argmax(DQ[:, 1])\n            yr = int(DQ[best_idx, 0])\n        \n        DQ[co, 2] = yr\n    \n    # Extract reference year data (1 year + 1 day for analysis)\n    ref_start = datetime(yr, 1, 1, 0, 0, 0)\n    ref_end = datetime(yr + 1, 1, 2, 0, 0, 0)  # +1 day\n    \n    mask_ref = (TSP &gt;= ref_start) & (TSP &lt; ref_end)\n    j_ref = np.where(mask_ref)[0]\n    \n    if len(j_ref) == 0:\n        print(f\"    Warning: No data found for reference year {yr}\")\n        continue\n    \n    # Get water levels and times for reference year\n    WLP_ref = WLP[j_ref]\n    TSP_ref = TSP[j_ref]\n    \n    # Remove NaN values for pytides analysis\n    valid_mask = ~np.isnan(WLP_ref)\n    WLP_clean = WLP_ref[valid_mask]\n    TSP_clean = TSP_ref[valid_mask]\n    \n    if len(WLP_clean) &lt; 1000:  # Need sufficient data for analysis\n        print(f\"    Warning: Insufficient data for reference year {yr} ({len(WLP_clean)} points)\")\n        continue\n    \n    # Perform tidal analysis using pytides\n    try:\n        tide_model = Tide.decompose(\n            heights=WLP_clean,\n            t=TSP_clean\n        )\n        print(f\"    Analyzed {len(WLP_clean):,} points from reference year {yr}\")\n    except Exception as e:\n        print(f\"    Error in tidal analysis for year {y}: {e}\")\n        continue\n    \n    # Generate prediction timestamps for target year (10-minute intervals)\n    pred_start = datetime(y, 1, 1, 0, 0, 0)\n    pred_end = datetime(y, 12, 31, 23, 50, 0)\n    tsp2 = pd.date_range(start=pred_start, end=pred_end, freq='10min').to_pydatetime()\n    \n    # Predict tides\n    try:\n        tip = tide_model.at(tsp2)\n        TIP.extend(tip)\n        TSP2.extend(tsp2)\n        print(f\"    Predicted {len(tip):,} points for year {y}\")\n    except Exception as e:\n        print(f\"    Error in prediction for year {y}: {e}\")\n        continue\n\n# Convert to numpy arrays\nTSP2 = np.array(TSP2)\nTIP = np.array(TIP)\n\nprint(f\"\\nTotal predictions: {len(TIP):,} points\")\nif len(TIP) &gt; 0:\n    print(f\"  Start: {TSP2[0]}\")\n    print(f\"  End: {TSP2[-1]}\")\n\n\nPerforming tidal analysis and prediction...\n  Processing year 1986 (1/38)...\n    Analyzed 52,704 points from reference year 1993\n    Predicted 52,560 points for year 1986\n  Processing year 1987 (2/38)...\n    Analyzed 52,704 points from reference year 1993\n    Predicted 52,560 points for year 1987\n  Processing year 1988 (3/38)...\n    Analyzed 52,704 points from reference year 1993\n    Predicted 52,704 points for year 1988\n  Processing year 1989 (4/38)...\n    Analyzed 52,704 points from reference year 1993\n    Predicted 52,560 points for year 1989\n  Processing year 1990 (5/38)...\n    Analyzed 52,704 points from reference year 1993\n    Predicted 52,560 points for year 1990\n  Processing year 1991 (6/38)...\n    Analyzed 52,704 points from reference year 1993\n    Predicted 52,560 points for year 1991\n  Processing year 1992 (7/38)...\n    Analyzed 52,704 points from reference year 1993\n    Predicted 52,704 points for year 1992\n  Processing year 1993 (8/38)...\n    Analyzed 52,704 points from reference year 1993\n    Predicted 52,560 points for year 1993\n  Processing year 1994 (9/38)...\n    Analyzed 52,297 points from reference year 1994\n    Predicted 52,560 points for year 1994\n  Processing year 1995 (10/38)...\n    Analyzed 52,704 points from reference year 1995\n    Predicted 52,560 points for year 1995\n  Processing year 1996 (11/38)...\n    Analyzed 51,828 points from reference year 1996\n    Predicted 52,704 points for year 1996\n  Processing year 1997 (12/38)...\n    Analyzed 51,974 points from reference year 1997\n    Predicted 52,560 points for year 1997\n  Processing year 1998 (13/38)...\n    Analyzed 52,704 points from reference year 1998\n    Predicted 52,560 points for year 1998\n  Processing year 1999 (14/38)...\n    Analyzed 51,878 points from reference year 1999\n    Predicted 52,560 points for year 1999\n  Processing year 2000 (15/38)...\n    Analyzed 49,732 points from reference year 2000\n    Predicted 52,704 points for year 2000\n  Processing year 2001 (16/38)...\n    Analyzed 40,906 points from reference year 2001\n    Predicted 52,560 points for year 2001\n  Processing year 2002 (17/38)...\n    Analyzed 37,410 points from reference year 2002\n    Predicted 52,560 points for year 2002\n  Processing year 2003 (18/38)...\n    Analyzed 52,625 points from reference year 2003\n    Predicted 52,560 points for year 2003\n  Processing year 2004 (19/38)...\n    Analyzed 52,848 points from reference year 2004\n    Predicted 52,704 points for year 2004\n  Processing year 2005 (20/38)...\n    Analyzed 52,704 points from reference year 2005\n    Predicted 52,560 points for year 2005\n  Processing year 2006 (21/38)...\n    Analyzed 52,249 points from reference year 2006\n    Predicted 52,560 points for year 2006\n  Processing year 2007 (22/38)...\n    Analyzed 39,679 points from reference year 2007\n    Predicted 52,560 points for year 2007\n  Processing year 2008 (23/38)...\n    Analyzed 33,152 points from reference year 2008\n    Predicted 52,704 points for year 2008\n  Processing year 2009 (24/38)...\n    Analyzed 33,152 points from reference year 2008\n    Predicted 52,560 points for year 2009\n  Processing year 2010 (25/38)...\n    Analyzed 33,152 points from reference year 2008\n    Predicted 52,560 points for year 2010\n  Processing year 2011 (26/38)...\n    Analyzed 33,152 points from reference year 2008\n    Predicted 52,560 points for year 2011\n  Processing year 2012 (27/38)...\n    Analyzed 33,152 points from reference year 2008\n    Predicted 52,704 points for year 2012\n  Processing year 2013 (28/38)...\n    Analyzed 33,152 points from reference year 2008\n    Predicted 52,560 points for year 2013\n  Processing year 2014 (29/38)...\n    Analyzed 33,152 points from reference year 2008\n    Predicted 52,560 points for year 2014\n  Processing year 2015 (30/38)...\n    Analyzed 33,152 points from reference year 2008\n    Predicted 52,560 points for year 2015\n  Processing year 2016 (31/38)...\n    Analyzed 47,468 points from reference year 2022\n    Predicted 52,704 points for year 2016\n  Processing year 2017 (32/38)...\n    Analyzed 47,468 points from reference year 2022\n    Predicted 52,560 points for year 2017\n  Processing year 2018 (33/38)...\n    Analyzed 47,468 points from reference year 2022\n    Predicted 52,560 points for year 2018\n  Processing year 2019 (34/38)...\n    Analyzed 47,468 points from reference year 2022\n    Predicted 52,560 points for year 2019\n  Processing year 2020 (35/38)...\n    Analyzed 47,468 points from reference year 2022\n    Predicted 52,704 points for year 2020\n  Processing year 2021 (36/38)...\n    Analyzed 47,468 points from reference year 2022\n    Predicted 52,560 points for year 2021\n  Processing year 2022 (37/38)...\n    Analyzed 47,468 points from reference year 2022\n    Predicted 52,560 points for year 2022\n  Processing year 2023 (38/38)...\n    Analyzed 47,468 points from reference year 2022\n    Predicted 52,560 points for year 2023\n\nTotal predictions: 1,998,576 points\n  Start: 1986-01-01 00:00:00\n  End: 2023-12-31 23:50:00"
  },
  {
    "objectID": "thames.html#save-results-part-3",
    "href": "thames.html#save-results-part-3",
    "title": "Thames Barrier",
    "section": "Save Results (Part 3)",
    "text": "Save Results (Part 3)\n\noutput_file = os.path.join(output_dir, 'mast3_thames.pkl')\nwith open(output_file, 'wb') as f:\n    pickle.dump({\n        'TSP': TSP,\n        'WLP': WLP,\n        'TIP': TIP,\n        'TSP2': TSP2,\n        'DQ': DQ\n    }, f)\nprint(f\"Data saved to {output_file}\")\n\nData saved to output/mast3_thames.pkl"
  },
  {
    "objectID": "thames.html#visualization-part-3",
    "href": "thames.html#visualization-part-3",
    "title": "Thames Barrier",
    "section": "Visualization (Part 3)",
    "text": "Visualization (Part 3)\nThis figure compares observed water levels (blue) with predicted astronomical tides (red) from harmonic analysis. The predicted tides represent the astronomical component only, while observed water levels include both tides and meteorological effects.\n\nfig, ax = plt.subplots(figsize=(14, 6))\nax.plot(TSP2, TIP, 'r', linewidth=0.5, label='Predicted tide', alpha=0.7)\nax.plot(TSP, WLP, 'b', linewidth=0.5, label='Observed water level', alpha=0.7)\nax.set_xlabel('Date', fontweight='bold', fontsize=16)\nax.set_ylabel('Level (m NAP)', fontweight='bold', fontsize=16)\nax.legend(fontsize=14)\nax.grid(True, alpha=0.3)\nax.tick_params(labelsize=14)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, 'master3_thames_tidal_analysis.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n\n\n\n\nComparison of observed water levels (blue) and predicted astronomical tides (red) for the Thames, 1986-2023\n\n\n\n\n\nFigure saved to output/master3_thames_tidal_analysis.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "thames.html#introduction",
    "href": "thames.html#introduction",
    "title": "Thames Barrier",
    "section": "Introduction",
    "text": "Introduction\nThis section plots time-series of predicted high waters and barrier closures for the Thames. The analysis:\n\nLoads barrier closure dates and tide gauge data from previous analyses\nCreates a visualization showing:\n\nBarrier closure dates as vertical dashed lines\nPredicted astronomical tides (TIP)\nObserved water levels (WLP)\nThe difference between observed and predicted levels (surge component)\n\n\nThis visualization helps identify when barrier closures occurred relative to predicted high waters and actual water levels."
  },
  {
    "objectID": "thames.html#setup-and-configuration-1",
    "href": "thames.html#setup-and-configuration-1",
    "title": "Thames Barrier",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\n\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport os\n\n# Output directory\noutput_dir = 'output'\nos.makedirs(output_dir, exist_ok=True)\n\nprint(f\"Analysis configuration:\")\nprint(f\"  Output directory: {output_dir}\")\n\nAnalysis configuration:\n  Output directory: output"
  },
  {
    "objectID": "thames.html#load-data",
    "href": "thames.html#load-data",
    "title": "Thames Barrier",
    "section": "Load Data",
    "text": "Load Data\n\n# Load barrier closure data from master1\nprint(\"Loading barrier closure data...\")\nmast1_file = os.path.join(output_dir, 'mast1_thames.pkl')\nif not os.path.exists(mast1_file):\n    raise FileNotFoundError(\n        f\"Required file {mast1_file} not found. \"\n        \"Please run Part 1 (Barrier Closure Analysis) first to generate the required data file.\"\n    )\nwith open(mast1_file, 'rb') as f:\n    data1 = pickle.load(f)\n    OCD = data1['OCD']  # Observed closure dates\n\n# Load tide gauge and tidal prediction data from master3\nprint(\"Loading tide gauge and tidal prediction data...\")\nmast3_file = os.path.join(output_dir, 'mast3_thames.pkl')\nif not os.path.exists(mast3_file):\n    raise FileNotFoundError(\n        f\"Required file {mast3_file} not found. \"\n        \"Please run Part 1, Part 2, and Part 3 first to generate the required data files.\"\n    )\nwith open(mast3_file, 'rb') as f:\n    data3 = pickle.load(f)\n    TSP = data3['TSP']  # Time series for observed water levels\n    TSP2 = data3['TSP2']  # Time series for predictions\n    TIP = data3['TIP']  # Predicted tides\n    WLP = data3['WLP']  # Observed water levels\n\n# Convert to numpy arrays if needed\nOCD = np.array(OCD)\nTSP = np.array(TSP)\nTSP2 = np.array(TSP2)\nTIP = np.array(TIP)\nWLP = np.array(WLP)\n\nprint(f\"\\nData loaded:\")\nprint(f\"  Closure dates: {len(OCD)} closures\")\nif len(OCD) &gt; 0:\n    print(f\"    First closure: {OCD[0]}\")\n    print(f\"    Last closure: {OCD[-1]}\")\nprint(f\"  Time series points (observed): {len(TSP):,}\")\nprint(f\"  Time series points (predictions): {len(TSP2):,}\")\nprint(f\"  Predicted tides: {len(TIP):,}\")\nprint(f\"  Observed water levels: {len(WLP):,}\")\n\nLoading barrier closure data...\nLoading tide gauge and tidal prediction data...\n\nData loaded:\n  Closure dates: 221 closures\n    First closure: 1983-02-01 00:00:00\n    Last closure: 2024-04-09 00:00:00\n  Time series points (observed): 1,998,576\n  Time series points (predictions): 1,998,576\n  Predicted tides: 1,998,576\n  Observed water levels: 1,998,576"
  },
  {
    "objectID": "thames.html#figure---predicted-high-waters-and-closures",
    "href": "thames.html#figure---predicted-high-waters-and-closures",
    "title": "Thames Barrier",
    "section": "Figure - Predicted High Waters and Closures",
    "text": "Figure - Predicted High Waters and Closures\nThis figure shows predicted astronomical tides (red), observed water levels (blue), the difference between them (green), and barrier closure dates (vertical dashed magenta lines). The difference (WLP-TIP) represents the non-tidal component, primarily storm surge.\n\nfig, ax = plt.subplots(figsize=(14, 8))\n\n# Plot vertical lines for barrier closure dates\nfor i in range(len(OCD)):\n    ax.axvline(OCD[i], color='m', linestyle='--', linewidth=2, alpha=0.7)\n\n# Check if we have predictions\nif len(TIP) &gt; 0 and len(TSP2) &gt; 0:\n    # Plot predicted tides (TIP) - use TSP2 for time axis\n    ax.plot(TSP2, TIP, 'r', linewidth=1, label='Predicted tide (TIP)', alpha=0.8)\n    \n    # For observed water levels, plot the full time series\n    ax.plot(TSP, WLP, 'b', linewidth=1, label='Observed water level (WLP)', alpha=0.8)\n    \n    # For surge calculation, interpolate TIP to match TSP timestamps using pandas\n    # Find overlapping time period\n    mask_overlap = (TSP &gt;= TSP2[0]) & (TSP &lt;= TSP2[-1])\n    if np.any(mask_overlap):\n        TSP_overlap = TSP[mask_overlap]\n        WLP_overlap = WLP[mask_overlap]\n        # Use pandas for interpolation\n        df_tip = pd.Series(TIP, index=pd.DatetimeIndex(TSP2))\n        # Reindex to overlap timestamps and interpolate\n        tip_interp = df_tip.reindex(pd.DatetimeIndex(TSP_overlap), method='nearest', \n                                    tolerance=pd.Timedelta(minutes=15))\n        # Calculate surge where we have both values\n        valid_mask = ~tip_interp.isna()\n        if np.any(valid_mask):\n            surge = WLP_overlap[valid_mask] - tip_interp.values[valid_mask]\n            ax.plot(TSP_overlap[valid_mask], surge, 'g', \n                   linewidth=1, label='Surge (WLP - TIP)', alpha=0.8)\nelse:\n    # If no predictions, just plot observed water levels\n    print(\"Warning: No tidal predictions available. Plotting only observed water levels.\")\n    ax.plot(TSP, WLP, 'b', linewidth=1, label='Observed water level (WLP)', alpha=0.8)\n\n# Formatting\nax.grid(True, alpha=0.3)\nax.set_xlabel('Date', fontweight='bold', fontsize=16)\nax.set_ylabel('Level (m)', fontweight='bold', fontsize=16)\nax.set_ylim(-3, 4)\nax.legend(fontsize=14, loc='best')\nax.tick_params(labelsize=14)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, 'master4_thames_predicted_high_waters.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n\n\n\n\nPredicted high waters, observed water levels, and barrier closures for the Thames\n\n\n\n\n\nFigure saved to output/master4_thames_predicted_high_waters.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "thames.html#part-1-barrier-closure-analysis",
    "href": "thames.html#part-1-barrier-closure-analysis",
    "title": "Thames Barrier",
    "section": "Part 1: Barrier Closure Analysis",
    "text": "Part 1: Barrier Closure Analysis\n\nTotal closures: Recorded across water years 1986/87 to 2024/25\nAnnual statistics: Minimum, mean, and maximum closures per water year\nTemporal patterns: Variability in closure frequency over time"
  },
  {
    "objectID": "thames.html#part-2-tide-gauge-data-processing",
    "href": "thames.html#part-2-tide-gauge-data-processing",
    "title": "Thames Barrier",
    "section": "Part 2: Tide Gauge Data Processing",
    "text": "Part 2: Tide Gauge Data Processing\n\nCombined time series: 1986-2023 at 10-minute intervals\nData sources: Available Thames tide gauge measurements\nData quality: Percentage of valid data points across the time series"
  },
  {
    "objectID": "thames.html#part-3-tidal-analysis",
    "href": "thames.html#part-3-tidal-analysis",
    "title": "Thames Barrier",
    "section": "Part 3: Tidal Analysis",
    "text": "Part 3: Tidal Analysis\n\nData quality: Calculated for each year to determine suitable reference years\nHarmonic analysis: Performed using pytides (Python equivalent of MATLAB’s t_tide)\nPredictions: Generated at 10-minute intervals for the entire period\nSurge calculation: The difference between observed and predicted water levels represents the non-tidal component (surge)"
  },
  {
    "objectID": "thames.html#part-4-visualization",
    "href": "thames.html#part-4-visualization",
    "title": "Thames Barrier",
    "section": "Part 4: Visualization",
    "text": "Part 4: Visualization\n\nClosure timing: When closures occurred relative to predicted high waters\nSurge contribution: The non-tidal component shows how much storm surge contributed to water levels\nTidal vs. meteorological effects: The difference between observed and predicted levels highlights meteorological forcing\n\nThe complete workflow provides a foundation for understanding barrier closure patterns, water level variability, and storm surge dynamics in the Thames estuary. The results can be used for further analysis of individual closure events, surge characteristics, and long-term trends."
  },
  {
    "objectID": "eastern_scheldt.html",
    "href": "eastern_scheldt.html",
    "title": "Eastern Scheldt Barrier",
    "section": "",
    "text": "This document provides a complete analysis workflow for the Eastern Scheldt barrier system, combining barrier closure analysis, tide gauge data processing, tidal analysis, and visualization. The analysis covers the period from 1986 to 2024 and includes:\n\nBarrier Closure Analysis: Loading and analyzing past barrier closure dates, counting closures per water year, and calculating statistics\nTide Gauge Data Processing: Loading and combining tide gauge data from multiple sources (OS4 and RPBU gauges)\nTidal Analysis: Performing harmonic tidal decomposition and generating predicted astronomical tides\nVisualization: Creating comprehensive visualizations showing predicted high waters, observed water levels, and barrier closures\n\nWater years run from July 1 to June 30, which is more appropriate for coastal flood analysis than calendar years."
  },
  {
    "objectID": "eastern_scheldt.html#overview",
    "href": "eastern_scheldt.html#overview",
    "title": "Eastern Scheldt Barrier",
    "section": "",
    "text": "This document provides a complete analysis workflow for the Eastern Scheldt barrier system, combining barrier closure analysis, tide gauge data processing, tidal analysis, and visualization. The analysis covers the period from 1986 to 2024 and includes:\n\nBarrier Closure Analysis: Loading and analyzing past barrier closure dates, counting closures per water year, and calculating statistics\nTide Gauge Data Processing: Loading and combining tide gauge data from multiple sources (OS4 and RPBU gauges)\nTidal Analysis: Performing harmonic tidal decomposition and generating predicted astronomical tides\nVisualization: Creating comprehensive visualizations showing predicted high waters, observed water levels, and barrier closures\n\nWater years run from July 1 to June 30, which is more appropriate for coastal flood analysis than calendar years."
  },
  {
    "objectID": "eastern_scheldt.html#analysis",
    "href": "eastern_scheldt.html#analysis",
    "title": "Eastern Scheldt Barrier",
    "section": "Analysis",
    "text": "Analysis\n\n# 1. Observed closure data - Eastern Scheldt\ndata_dir = '../2_DATA/1_BARRIER_CLOSURES'\nexcel_file = os.path.join(data_dir, 'Eastern_Scheldt_Barrier_Past_Closures_2024.xlsx')\ndf = pd.read_excel(excel_file, sheet_name='Closures')\n\n# Extract closure dates from column 2 (index 1)\nclosure_dates_col = df.iloc[:, 1]  # Column 2 (0-indexed)\n\n# Convert dates: pandas.read_excel automatically converts Excel dates to datetime objects\nOCD = [d.to_pydatetime() if isinstance(d, pd.Timestamp) else d \n       for d in closure_dates_col if pd.notna(d)]\n\nprint(f\"Loaded {len(OCD)} closure dates\")\nif len(OCD) &gt; 0:\n    print(f\"  First closure: {OCD[0]}\")\n    print(f\"  Last closure: {OCD[-1]}\")\n\n# 2. Count closures per water year (July 1 to July 1)\nOCS = []  # Will store [index, year, year+1, num_closures]\nYT = []   # Will store year labels like \"1986/87\"\n\nfor co, y in enumerate(Y, start=1):\n    # Find closures between July 1 of year y and July 1 of year y+1\n    start_date = datetime(y, 7, 1, 0, 0, 0)\n    end_date = datetime(y + 1, 7, 1, 0, 0, 0)\n    \n    # Count closures in this water year\n    closures_in_year = [d for d in OCD if start_date &lt;= d &lt; end_date]\n    num_closures = len(closures_in_year)\n    \n    OCS.append([co, y, y + 1, num_closures])\n    \n    # Create year label (e.g., \"1986/87\")\n    year_str = str(y + 1)\n    YT.append(f\"{y}/{year_str[2:]}\")\n\nOCS = np.array(OCS)\n\nprint(f\"\\nClosures per water year calculated for {len(OCS)} years\")\n\nprint(f\"  Total closures: {np.sum(OCS[:, 3])}\")\n\n# 3. Calculate statistics\nE = np.array([\n    np.min(OCS[:, 3]),      # Minimum closures per year\n    np.mean(OCS[:, 3]),     # Mean closures per year\n    np.max(OCS[:, 3]),      # Maximum closures per year\n    np.sum(OCS[:, 3])       # Total closures\n])\n\nprint(f\"\\nClosure statistics:\")\nprint(f\"  Min per year: {E[0]:.1f}\")\nprint(f\"  Mean per year: {E[1]:.2f}\")\nprint(f\"  Max per year: {E[2]:.1f}\")\nprint(f\"  Total: {E[3]:.0f}\")\n\n#| label: save-master1\n#| include: true\n\n# 4. Save data\noutput_file = os.path.join(output_dir, 'mast1.pkl')\nwith open(output_file, 'wb') as f:\n    pickle.dump({\n        'OCD': OCD,\n        'OCS': OCS,\n        'E': E,\n        'YT': YT\n    }, f)\nprint(f\"\\nData saved to {output_file}\")\n\nLoaded 31 closure dates\n  First closure: 1986-10-20 00:00:00\n  Last closure: 2023-12-21 00:00:00\n\nClosures per water year calculated for 39 years\n  Total closures: 31\n\nClosure statistics:\n  Min per year: 0.0\n  Mean per year: 0.79\n  Max per year: 4.0\n  Total: 31\n\nData saved to output/mast1.pkl"
  },
  {
    "objectID": "eastern_scheldt.html#visualization",
    "href": "eastern_scheldt.html#visualization",
    "title": "Eastern Scheldt Barrier",
    "section": "Visualization",
    "text": "Visualization\nThis bar chart shows the number of barrier closures per water year from 1986/87 to 2024/25. The Eastern Scheldt barrier closes when water levels exceed a threshold, typically during storm surge events.\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax.bar(OCS[:, 0], OCS[:, 3], color='black')\nax.set_xlim(0.2, len(OCS) + 0.8)\nax.set_xticks(range(1, len(OCS) + 1))\nax.set_xticklabels(YT, rotation=90)\nax.set_ylim(0, 5)\nax.set_yticks(range(6))\nax.set_ylabel('Number of closures', fontweight='bold', fontsize=18)\nax.set_title('Eastern Scheldt', fontweight='bold', fontsize=18)\nax.grid(True, alpha=0.3)\nax.tick_params(labelsize=16)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, 'master1_closures.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n\n\n\n\nNumber of Eastern Scheldt barrier closures per water year (July 1 to June 30), 1986-2024\n\n\n\n\n\nFigure saved to output/master1_closures.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nThe following table lists all barrier closure dates, which correspond to storm events that triggered the Eastern Scheldt barrier.\n\n# Create DataFrame with closure dates\nstorms_df = pd.DataFrame({\n    'Closure Date': [d.strftime('%Y-%m-%d %H:%M') if isinstance(d, datetime) else str(d) for d in OCD],\n    'Year': [d.year if isinstance(d, datetime) else None for d in OCD],\n    'Month': [d.strftime('%B') if isinstance(d, datetime) else None for d in OCD],\n    'YearMonth': [(d.year, d.month) if isinstance(d, datetime) else None for d in OCD]\n})\n\n# Add water year information\ndef get_water_year(date):\n    \"\"\"Determine water year (July 1 to June 30)\"\"\"\n    if isinstance(date, datetime):\n        if date.month &gt;= 7:\n            return f\"{date.year}/{str(date.year + 1)[2:]}\"\n        else:\n            return f\"{date.year - 1}/{str(date.year)[2:]}\"\n    return None\n\nstorms_df['Water Year'] = [get_water_year(d) for d in OCD]\n\n# Create index that groups storms by year and month\n# Storms in the same year and month get the same index\nunique_year_months = storms_df['YearMonth'].unique()\nyear_month_to_index = {ym: idx + 1 for idx, ym in enumerate(sorted(unique_year_months))}\nstorms_df['Event'] = storms_df['YearMonth'].map(year_month_to_index)\n\n# Reorder columns (Index first, then remove YearMonth helper column)\nstorms_df = storms_df[['Event', 'Closure Date', 'Water Year', 'Year', 'Month']]\n\n# Display table\nprint(f\"\\nTotal number of closures: {len(storms_df)}\")\nstorms_df\n\n\nTotal number of closures: 31\n\n\n\n\n\n\nComplete list of Eastern Scheldt barrier closures (storm events), 1986-2024\n\n\n\nEvent\nClosure Date\nWater Year\nYear\nMonth\n\n\n\n\n0\n1\n1986-10-20 00:00\n1986/87\n1986\nOctober\n\n\n1\n2\n1986-12-19 00:00\n1986/87\n1986\nDecember\n\n\n2\n2\n1986-12-19 00:00\n1986/87\n1986\nDecember\n\n\n3\n3\n1989-02-14 00:00\n1988/89\n1989\nFebruary\n\n\n4\n4\n1990-02-27 00:00\n1989/90\n1990\nFebruary\n\n\n5\n4\n1990-02-27 00:00\n1989/90\n1990\nFebruary\n\n\n6\n4\n1990-02-28 00:00\n1989/90\n1990\nFebruary\n\n\n7\n5\n1990-03-01 00:00\n1989/90\n1990\nMarch\n\n\n8\n6\n1990-09-21 00:00\n1990/91\n1990\nSeptember\n\n\n9\n7\n1990-12-12 00:00\n1990/91\n1990\nDecember\n\n\n10\n7\n1990-12-12 00:00\n1990/91\n1990\nDecember\n\n\n11\n8\n1992-11-11 00:00\n1992/93\n1992\nNovember\n\n\n12\n9\n1993-01-25 00:00\n1992/93\n1993\nJanuary\n\n\n13\n10\n1993-02-21 00:00\n1992/93\n1993\nFebruary\n\n\n14\n11\n1993-11-14 00:00\n1993/94\n1993\nNovember\n\n\n15\n11\n1993-11-15 00:00\n1993/94\n1993\nNovember\n\n\n16\n12\n1994-01-28 00:00\n1993/94\n1994\nJanuary\n\n\n17\n13\n1995-01-02 00:00\n1994/95\n1995\nJanuary\n\n\n18\n13\n1995-01-02 00:00\n1994/95\n1995\nJanuary\n\n\n19\n14\n1996-10-29 00:00\n1996/97\n1996\nOctober\n\n\n20\n15\n2002-10-27 00:00\n2002/03\n2002\nOctober\n\n\n21\n16\n2003-12-21 00:00\n2003/04\n2003\nDecember\n\n\n22\n17\n2004-02-08 00:00\n2003/04\n2004\nFebruary\n\n\n23\n18\n2007-11-09 00:00\n2007/08\n2007\nNovember\n\n\n24\n19\n2013-12-05 00:00\n2013/14\n2013\nDecember\n\n\n25\n20\n2014-10-21 00:00\n2014/15\n2014\nOctober\n\n\n26\n21\n2018-01-03 00:00\n2017/18\n2018\nJanuary\n\n\n27\n22\n2020-02-10 00:00\n2019/20\n2020\nFebruary\n\n\n28\n23\n2022-01-31 00:00\n2021/22\n2022\nJanuary\n\n\n29\n24\n2022-02-21 00:00\n2021/22\n2022\nFebruary\n\n\n30\n25\n2023-12-21 00:00\n2023/24\n2023\nDecember"
  },
  {
    "objectID": "eastern_scheldt.html#load-rpbu-1987-2022-data",
    "href": "eastern_scheldt.html#load-rpbu-1987-2022-data",
    "title": "Eastern Scheldt Barrier",
    "section": "Load RPBU 1987-2022 Data",
    "text": "Load RPBU 1987-2022 Data\nLoad the main tide gauge dataset from the RPBU gauge covering April 1987 to December 2022.\n\n# 1. Load in Data 1987 to 2022 (RPBU gauge)\nfile_in = os.path.join(data_dir, 'RPBU_1987_2022.dat')\nD = np.loadtxt(file_in)\n\n# Create time series: April 15, 1987 to December 31, 2022, 10-minute intervals\nstart_date = datetime(1987, 4, 15, 0, 0, 0)\nend_date = datetime(2022, 12, 31, 23, 50, 0)\nTS1 = pd.date_range(start=start_date, end=end_date, freq='10min').to_pydatetime()\n\n# Extract water level data from column 3 (index 2)\nWL1 = D[:, 2]\n\n# Remove default values (9999 indicates missing)\nWL1[WL1 == 9999] = np.nan\n\n# Convert to meters (from cm)\nWL1 = WL1 / 100.0\n\nprint(f\"Loaded RPBU 1987-2022: {len(TS1):,} data points\")\nprint(f\"  Start: {TS1[0]}\")\nprint(f\"  End: {TS1[-1]}\")\nprint(f\"  Valid data: {np.sum(~np.isnan(WL1)):,} points ({100*np.sum(~np.isnan(WL1))/len(WL1):.1f}%)\")\n\nLoaded RPBU 1987-2022: 1,878,480 data points\n  Start: 1987-04-15 00:00:00\n  End: 2022-12-31 23:50:00\n  Valid data: 1,876,721 points (99.9%)"
  },
  {
    "objectID": "eastern_scheldt.html#load-rpbu-2023-data",
    "href": "eastern_scheldt.html#load-rpbu-2023-data",
    "title": "Eastern Scheldt Barrier",
    "section": "Load RPBU 2023 Data",
    "text": "Load RPBU 2023 Data\nLoad the 2023 data from the RPBU gauge to extend the time series.\n\n# 2. Load in Data 2023 (RPBU gauge)\nfile_in = os.path.join(data_dir, 'RPBU_2023.dat')\nD = np.loadtxt(file_in)\n\n# Create time series: January 1, 2023 to December 31, 2023, 10-minute intervals\nstart_date = datetime(2023, 1, 1, 0, 0, 0)\nend_date = datetime(2023, 12, 31, 23, 50, 0)\nTS2 = pd.date_range(start=start_date, end=end_date, freq='10min').to_pydatetime()\n\n# Extract water level data from column 3 (index 2)\nWL2 = D[:, 2]\n\n# Remove default values (9999 indicates missing)\nWL2[WL2 == 9999] = np.nan\n\n# Convert to meters (from cm)\nWL2 = WL2 / 100.0\n\nprint(f\"Loaded RPBU 2023: {len(TS2):,} data points\")\nprint(f\"  Start: {TS2[0]}\")\nprint(f\"  End: {TS2[-1]}\")\nprint(f\"  Valid data: {np.sum(~np.isnan(WL2)):,} points ({100*np.sum(~np.isnan(WL2))/len(WL2):.1f}%)\")\n\nLoaded RPBU 2023: 52,560 data points\n  Start: 2023-01-01 00:00:00\n  End: 2023-12-31 23:50:00\n  Valid data: 52,560 points (100.0%)"
  },
  {
    "objectID": "eastern_scheldt.html#load-os4-1982-2023-data",
    "href": "eastern_scheldt.html#load-os4-1982-2023-data",
    "title": "Eastern Scheldt Barrier",
    "section": "Load OS4 1982-2023 Data",
    "text": "Load OS4 1982-2023 Data\nLoad data from the OS4 gauge, which provides coverage for the early period before RPBU data begins.\n\n# 3. Load in Data 1982 to 2023 for OS4 gauge\nfile_in = os.path.join(data_dir, 'OS4_1982_2023.dat')\nD = np.loadtxt(file_in)\n\n# Create time series: January 1, 1982 to December 31, 2023, 10-minute intervals\nstart_date = datetime(1982, 1, 1, 0, 0, 0)\nend_date = datetime(2023, 12, 31, 23, 50, 0)\nTSO = pd.date_range(start=start_date, end=end_date, freq='10min').to_pydatetime()\n\n# Extract water level data from column 3 (index 2)\nWLO = D[:, 2]\n\n# Remove default values (values outside [-500, 500] are invalid)\nWLO[(WLO &lt; -500) | (WLO &gt; 500)] = np.nan\n\n# Convert to meters (from cm)\nWLO = WLO / 100.0\n\nprint(f\"Loaded OS4 1982-2023: {len(TSO):,} data points\")\nprint(f\"  Start: {TSO[0]}\")\nprint(f\"  End: {TSO[-1]}\")\nprint(f\"  Valid data: {np.sum(~np.isnan(WLO)):,} points ({100*np.sum(~np.isnan(WLO))/len(WLO):.1f}%)\")\n\nLoaded OS4 1982-2023: 2,208,960 data points\n  Start: 1982-01-01 00:00:00\n  End: 2023-12-31 23:50:00\n  Valid data: 2,161,485 points (97.9%)"
  },
  {
    "objectID": "eastern_scheldt.html#combine-time-series",
    "href": "eastern_scheldt.html#combine-time-series",
    "title": "Eastern Scheldt Barrier",
    "section": "Combine Time Series",
    "text": "Combine Time Series\nCombine the time series from different gauges to create a continuous record. Use OS4 data for the early period (1986-1987) before RPBU starts, then switch to RPBU data for the main period.\n\n# 4. Combine time series\n# Use OS4 data for period 1986-01-01 to 1987-04-15 (before RPBU starts)\nstart_combine = datetime(1986, 1, 1, 0, 0, 0)\nend_combine = datetime(1987, 4, 15, 0, 0, 0)\nmask_early = (TSO &gt;= start_combine) & (TSO &lt; end_combine)\nTSO_early = TSO[mask_early]\nWLO_early = WLO[mask_early]\n\n# Combine: OS4 early period + RPBU 1987-2022 + RPBU 2023\nTSP = np.concatenate([TSO_early, TS1, TS2])\nWLP = np.concatenate([WLO_early, WL1, WL2])\n\n# Create reference time series for checking\nTSCHECK = pd.date_range(\n    start=datetime(1986, 1, 1, 0, 0, 0),\n    end=datetime(2023, 12, 31, 23, 50, 0),\n    freq='10min'\n).to_pydatetime()\n\nprint(f\"\\nCombined time series: {len(TSP):,} data points\")\nprint(f\"  Start: {TSP[0]}\")\nprint(f\"  End: {TSP[-1]}\")\nprint(f\"  Valid data: {np.sum(~np.isnan(WLP)):,} points ({100*np.sum(~np.isnan(WLP))/len(WLP):.1f}%)\")\nprint(f\"  Time span: {(TSP[-1] - TSP[0]).days} days\")\n\n\nCombined time series: 1,998,576 data points\n  Start: 1986-01-01 00:00:00\n  End: 2023-12-31 23:50:00\n  Valid data: 1,996,699 points (99.9%)\n  Time span: 13878 days\n\n\nSave the combined time series to a pickle file for use in subsequent analyses.\n\n# 5. Save data\noutput_file = os.path.join(output_dir, 'mast2.pkl')\nwith open(output_file, 'wb') as f:\n    pickle.dump({\n        'TSP': TSP,\n        'WLP': WLP,\n        'TSCHECK': TSCHECK\n    }, f)\nprint(f\"\\nData saved to {output_file}\")\n\n\nData saved to output/mast2.pkl"
  },
  {
    "objectID": "eastern_scheldt.html#visualization-1",
    "href": "eastern_scheldt.html#visualization-1",
    "title": "Eastern Scheldt Barrier",
    "section": "Visualization",
    "text": "Visualization\nThis figure shows the combined tide gauge water level time series from 1986 to 2023 at 10-minute intervals. The data combines OS4 gauge measurements (1986-1987) and RPBU gauge measurements (1987-2023) for the Eastern Scheldt. Water levels are shown in meters relative to NAP (Normal Amsterdam Peil, the Dutch reference datum). The time series shows tidal variations, storm surges, and long-term water level patterns over the 38-year period.\n\nfig, ax = plt.subplots(figsize=(14, 6))\nax.plot(TSP, WLP, 'b', linewidth=0.5)\nax.set_xlabel('Date', fontweight='bold', fontsize=20)\nax.set_ylabel('Water level (m NAP)', fontweight='bold', fontsize=20)\nax.grid(True, alpha=0.3)\nax.tick_params(labelsize=16)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, 'master2_tide_gauge.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n\n\n\n\nCombined tide gauge water level time series for the Eastern Scheldt, 1986-2023. Data combines OS4 gauge (1986-1987) and RPBU gauge (1987-2023) measurements at 10-minute intervals.\n\n\n\n\n\nFigure saved to output/master2_tide_gauge.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "eastern_scheldt.html#setup-and-configuration",
    "href": "eastern_scheldt.html#setup-and-configuration",
    "title": "Eastern Scheldt Barrier",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\n\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom pytides.tide import Tide\nimport os\n\n# Configuration\nY = list(range(1986, 2024))  # Years 1986 to 2023\nth = 60  # Data quality threshold (percentage)\nlat = 51.64  # Latitude for tidal analysis\noutput_dir = 'output'\n\n# Create output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\nprint(f\"Analysis configuration:\")\nprint(f\"  Years: {Y[0]} to {Y[-1]} ({len(Y)} years)\")\nprint(f\"  Data quality threshold: {th}%\")\nprint(f\"  Latitude: {lat}°\")\n\nAnalysis configuration:\n  Years: 1986 to 2023 (38 years)\n  Data quality threshold: 60%\n  Latitude: 51.64°"
  },
  {
    "objectID": "eastern_scheldt.html#load-tide-gauge-data",
    "href": "eastern_scheldt.html#load-tide-gauge-data",
    "title": "Eastern Scheldt Barrier",
    "section": "Load Tide Gauge Data",
    "text": "Load Tide Gauge Data\n\nprint(\"Loading tide gauge data...\")\nmast2_file = os.path.join(output_dir, 'mast2.pkl')\nif not os.path.exists(mast2_file):\n    raise FileNotFoundError(\n        f\"Required file {mast2_file} not found. \"\n        \"Please run Part 2 (Tide Gauge Data Processing) first to generate the required data file.\"\n    )\nwith open(mast2_file, 'rb') as f:\n    data = pickle.load(f)\n    TSP = data['TSP']\n    WLP = data['WLP']\n\n# Convert to numpy arrays if needed\nTSP = np.array(TSP)\nWLP = np.array(WLP)\n\nprint(f\"Loaded {len(TSP):,} data points\")\nprint(f\"  Start: {TSP[0]}\")\nprint(f\"  End: {TSP[-1]}\")\nprint(f\"  Valid data: {np.sum(~np.isnan(WLP)):,} points ({100*np.sum(~np.isnan(WLP))/len(WLP):.1f}%)\")\n\nLoading tide gauge data...\nLoaded 1,998,576 data points\n  Start: 1986-01-01 00:00:00\n  End: 2023-12-31 23:50:00\n  Valid data: 1,996,699 points (99.9%)"
  },
  {
    "objectID": "eastern_scheldt.html#calculate-data-quality-per-year",
    "href": "eastern_scheldt.html#calculate-data-quality-per-year",
    "title": "Eastern Scheldt Barrier",
    "section": "Calculate Data Quality Per Year",
    "text": "Calculate Data Quality Per Year\n\nprint(\"\\nCalculating data quality per year...\")\nDQ = []\n\nfor y in Y:\n    # Find data points in this calendar year (Jan 1 to Jan 1 next year)\n    start_date = datetime(y, 1, 1, 0, 0, 0)\n    end_date = datetime(y + 1, 1, 1, 0, 0, 0)\n    mask = (TSP &gt;= start_date) & (TSP &lt; end_date)\n    j = np.where(mask)[0]\n    \n    if len(j) == 0:\n        quality = 0\n    else:\n        # Count non-NaN values\n        k = np.where(~np.isnan(WLP[j]))[0]\n        quality = (len(k) / len(j)) * 100\n    \n    # Initialize with 3 columns: [year, quality, reference_year]\n    # reference_year will be filled in during tidal analysis\n    DQ.append([y, quality, np.nan])\n\nDQ = np.array(DQ)\n\n# Display summary statistics\nprint(f\"\\nData quality summary:\")\nprint(f\"  Range: {np.min(DQ[:, 1]):.1f}% to {np.max(DQ[:, 1]):.1f}%\")\nprint(f\"  Mean: {np.mean(DQ[:, 1]):.1f}%\")\nprint(f\"  Years with quality &gt;= {th}%: {np.sum(DQ[:, 1] &gt;= th)}/{len(Y)}\")\n\n\nCalculating data quality per year...\n\nData quality summary:\n  Range: 98.8% to 100.0%\n  Mean: 99.9%\n  Years with quality &gt;= 60%: 38/38"
  },
  {
    "objectID": "eastern_scheldt.html#tidal-analysis-and-prediction",
    "href": "eastern_scheldt.html#tidal-analysis-and-prediction",
    "title": "Eastern Scheldt Barrier",
    "section": "Tidal Analysis and Prediction",
    "text": "Tidal Analysis and Prediction\nFor each target year, the analysis determines a reference year, extracts data, performs harmonic decomposition, and generates predictions.\n\nprint(\"\\nPerforming tidal analysis and prediction...\")\nTSP2 = []\nTIP = []\n\nfor co, y in enumerate(Y):\n    print(f\"  Processing year {y} ({co+1}/{len(Y)})...\")\n    \n    # Determine reference year\n    if DQ[co, 1] &gt;= th:\n        # Use target year as reference\n        yr = y\n        DQ[co, 2] = yr\n    else:\n        # Find nearest year with quality &gt;= threshold\n        # Calculate distances from target year\n        distances = np.abs(DQ[:, 0] - y)\n        \n        # Filter to only years with quality &gt;= threshold\n        good_mask = DQ[:, 1] &gt;= th\n        \n        if np.any(good_mask):\n            # Get distances for good years only\n            good_distances = distances[good_mask]\n            good_indices = np.where(good_mask)[0]\n            \n            # Find nearest good year\n            nearest_idx = good_indices[np.argmin(good_distances)]\n            yr = int(DQ[nearest_idx, 0])\n        else:\n            # Fallback: use year with best quality\n            best_idx = np.argmax(DQ[:, 1])\n            yr = int(DQ[best_idx, 0])\n        \n        DQ[co, 2] = yr\n    \n    # Extract reference year data (1 year + 1 day for analysis)\n    ref_start = datetime(yr, 1, 1, 0, 0, 0)\n    ref_end = datetime(yr + 1, 1, 2, 0, 0, 0)  # +1 day\n    \n    mask_ref = (TSP &gt;= ref_start) & (TSP &lt; ref_end)\n    j_ref = np.where(mask_ref)[0]\n    \n    if len(j_ref) == 0:\n        print(f\"    Warning: No data found for reference year {yr}\")\n        continue\n    \n    # Get water levels and times for reference year\n    WLP_ref = WLP[j_ref]\n    TSP_ref = TSP[j_ref]\n    \n    # Remove NaN values for pytides analysis\n    valid_mask = ~np.isnan(WLP_ref)\n    WLP_clean = WLP_ref[valid_mask]\n    TSP_clean = TSP_ref[valid_mask]\n    \n    if len(WLP_clean) &lt; 1000:  # Need sufficient data for analysis\n        print(f\"    Warning: Insufficient data for reference year {yr} ({len(WLP_clean)} points)\")\n        continue\n    \n    # Perform tidal analysis using pytides\n    try:\n        tide_model = Tide.decompose(\n            heights=WLP_clean,\n            t=TSP_clean\n        )\n        print(f\"    Analyzed {len(WLP_clean):,} points from reference year {yr}\")\n    except Exception as e:\n        print(f\"    Error in tidal analysis for year {y}: {e}\")\n        continue\n    \n    # Generate prediction timestamps for target year (10-minute intervals)\n    pred_start = datetime(y, 1, 1, 0, 0, 0)\n    pred_end = datetime(y, 12, 31, 23, 50, 0)\n    tsp2 = pd.date_range(start=pred_start, end=pred_end, freq='10min').to_pydatetime()\n    \n    # Predict tides\n    try:\n        tip = tide_model.at(tsp2)\n        TIP.extend(tip)\n        TSP2.extend(tsp2)\n        print(f\"    Predicted {len(tip):,} points for year {y}\")\n    except Exception as e:\n        print(f\"    Error in prediction for year {y}: {e}\")\n        continue\n\n# Convert to numpy arrays\nTSP2 = np.array(TSP2)\nTIP = np.array(TIP)\n\nprint(f\"\\nTotal predictions: {len(TIP):,} points\")\nif len(TIP) &gt; 0:\n    print(f\"  Start: {TSP2[0]}\")\n    print(f\"  End: {TSP2[-1]}\")\n\n\nPerforming tidal analysis and prediction...\n  Processing year 1986 (1/38)...\n    Analyzed 52,586 points from reference year 1986\n    Predicted 52,560 points for year 1986\n  Processing year 1987 (2/38)...\n    Analyzed 52,704 points from reference year 1987\n    Predicted 52,560 points for year 1987\n  Processing year 1988 (3/38)...\n    Analyzed 52,848 points from reference year 1988\n    Predicted 52,704 points for year 1988\n  Processing year 1989 (4/38)...\n    Analyzed 52,704 points from reference year 1989\n    Predicted 52,560 points for year 1989\n  Processing year 1990 (5/38)...\n    Analyzed 52,704 points from reference year 1990\n    Predicted 52,560 points for year 1990\n  Processing year 1991 (6/38)...\n    Analyzed 52,672 points from reference year 1991\n    Predicted 52,560 points for year 1991\n  Processing year 1992 (7/38)...\n    Analyzed 52,829 points from reference year 1992\n    Predicted 52,704 points for year 1992\n  Processing year 1993 (8/38)...\n    Analyzed 52,704 points from reference year 1993\n    Predicted 52,560 points for year 1993\n  Processing year 1994 (9/38)...\n    Analyzed 52,519 points from reference year 1994\n    Predicted 52,560 points for year 1994\n  Processing year 1995 (10/38)...\n    Analyzed 52,594 points from reference year 1995\n    Predicted 52,560 points for year 1995\n  Processing year 1996 (11/38)...\n    Analyzed 52,848 points from reference year 1996\n    Predicted 52,704 points for year 1996\n  Processing year 1997 (12/38)...\n    Analyzed 52,651 points from reference year 1997\n    Predicted 52,560 points for year 1997\n  Processing year 1998 (13/38)...\n    Analyzed 52,067 points from reference year 1998\n    Predicted 52,560 points for year 1998\n  Processing year 1999 (14/38)...\n    Analyzed 52,696 points from reference year 1999\n    Predicted 52,560 points for year 1999\n  Processing year 2000 (15/38)...\n    Analyzed 52,848 points from reference year 2000\n    Predicted 52,704 points for year 2000\n  Processing year 2001 (16/38)...\n    Analyzed 52,704 points from reference year 2001\n    Predicted 52,560 points for year 2001\n  Processing year 2002 (17/38)...\n    Analyzed 52,704 points from reference year 2002\n    Predicted 52,560 points for year 2002\n  Processing year 2003 (18/38)...\n    Analyzed 52,627 points from reference year 2003\n    Predicted 52,560 points for year 2003\n  Processing year 2004 (19/38)...\n    Analyzed 52,820 points from reference year 2004\n    Predicted 52,704 points for year 2004\n  Processing year 2005 (20/38)...\n    Analyzed 52,651 points from reference year 2005\n    Predicted 52,560 points for year 2005\n  Processing year 2006 (21/38)...\n    Analyzed 52,694 points from reference year 2006\n    Predicted 52,560 points for year 2006\n  Processing year 2007 (22/38)...\n    Analyzed 52,598 points from reference year 2007\n    Predicted 52,560 points for year 2007\n  Processing year 2008 (23/38)...\n    Analyzed 52,815 points from reference year 2008\n    Predicted 52,704 points for year 2008\n  Processing year 2009 (24/38)...\n    Analyzed 52,607 points from reference year 2009\n    Predicted 52,560 points for year 2009\n  Processing year 2010 (25/38)...\n    Analyzed 52,700 points from reference year 2010\n    Predicted 52,560 points for year 2010\n  Processing year 2011 (26/38)...\n    Analyzed 52,683 points from reference year 2011\n    Predicted 52,560 points for year 2011\n  Processing year 2012 (27/38)...\n    Analyzed 52,832 points from reference year 2012\n    Predicted 52,704 points for year 2012\n  Processing year 2013 (28/38)...\n    Analyzed 52,704 points from reference year 2013\n    Predicted 52,560 points for year 2013\n  Processing year 2014 (29/38)...\n    Analyzed 52,648 points from reference year 2014\n    Predicted 52,560 points for year 2014\n  Processing year 2015 (30/38)...\n    Analyzed 52,601 points from reference year 2015\n    Predicted 52,560 points for year 2015\n  Processing year 2016 (31/38)...\n    Analyzed 52,737 points from reference year 2016\n    Predicted 52,704 points for year 2016\n  Processing year 2017 (32/38)...\n    Analyzed 52,704 points from reference year 2017\n    Predicted 52,560 points for year 2017\n  Processing year 2018 (33/38)...\n    Analyzed 52,704 points from reference year 2018\n    Predicted 52,560 points for year 2018\n  Processing year 2019 (34/38)...\n    Analyzed 52,704 points from reference year 2019\n    Predicted 52,560 points for year 2019\n  Processing year 2020 (35/38)...\n    Analyzed 52,848 points from reference year 2020\n    Predicted 52,704 points for year 2020\n  Processing year 2021 (36/38)...\n    Analyzed 52,704 points from reference year 2021\n    Predicted 52,560 points for year 2021\n  Processing year 2022 (37/38)...\n    Analyzed 52,704 points from reference year 2022\n    Predicted 52,560 points for year 2022\n  Processing year 2023 (38/38)...\n    Analyzed 52,560 points from reference year 2023\n    Predicted 52,560 points for year 2023\n\nTotal predictions: 1,998,576 points\n  Start: 1986-01-01 00:00:00\n  End: 2023-12-31 23:50:00"
  },
  {
    "objectID": "eastern_scheldt.html#save-results-part-3",
    "href": "eastern_scheldt.html#save-results-part-3",
    "title": "Eastern Scheldt Barrier",
    "section": "Save Results (Part 3)",
    "text": "Save Results (Part 3)\n\noutput_file = os.path.join(output_dir, 'mast3.pkl')\nwith open(output_file, 'wb') as f:\n    pickle.dump({\n        'TSP': TSP,\n        'WLP': WLP,\n        'TIP': TIP,\n        'TSP2': TSP2,\n        'DQ': DQ\n    }, f)\nprint(f\"Data saved to {output_file}\")\n\nData saved to output/mast3.pkl"
  },
  {
    "objectID": "eastern_scheldt.html#visualization-part-3",
    "href": "eastern_scheldt.html#visualization-part-3",
    "title": "Eastern Scheldt Barrier",
    "section": "Visualization (Part 3)",
    "text": "Visualization (Part 3)\nThis figure compares observed water levels (blue) with predicted astronomical tides (red) from harmonic analysis. The predicted tides represent the astronomical component only, while observed water levels include both tides and meteorological effects.\n\nfig, ax = plt.subplots(figsize=(14, 6))\nax.plot(TSP2, TIP, 'r', linewidth=0.5, label='Predicted tide', alpha=0.7)\nax.plot(TSP, WLP, 'b', linewidth=0.5, label='Observed water level', alpha=0.7)\nax.set_xlabel('Date', fontweight='bold', fontsize=16)\nax.set_ylabel('Level (m NAP)', fontweight='bold', fontsize=16)\nax.legend(fontsize=14)\nax.grid(True, alpha=0.3)\nax.tick_params(labelsize=14)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, 'master3_tidal_analysis.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/IPython/core/pylabtools.py:170: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\nComparison of observed water levels (blue) and predicted astronomical tides (red) for the Eastern Scheldt, 1986-2023\n\n\n\n\n\nFigure saved to output/master3_tidal_analysis.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "eastern_scheldt.html#setup-and-configuration-1",
    "href": "eastern_scheldt.html#setup-and-configuration-1",
    "title": "Eastern Scheldt Barrier",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\n\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport os\n\n# Output directory\noutput_dir = 'output'\nos.makedirs(output_dir, exist_ok=True)\n\nprint(f\"Analysis configuration:\")\nprint(f\"  Output directory: {output_dir}\")\n\nAnalysis configuration:\n  Output directory: output"
  },
  {
    "objectID": "eastern_scheldt.html#load-data",
    "href": "eastern_scheldt.html#load-data",
    "title": "Eastern Scheldt Barrier",
    "section": "Load Data",
    "text": "Load Data\n\n# Load barrier closure data from master1\nprint(\"Loading barrier closure data...\")\nmast1_file = os.path.join(output_dir, 'mast1.pkl')\nif not os.path.exists(mast1_file):\n    raise FileNotFoundError(\n        f\"Required file {mast1_file} not found. \"\n        \"Please run Part 1 (Barrier Closure Analysis) first to generate the required data file.\"\n    )\nwith open(mast1_file, 'rb') as f:\n    data1 = pickle.load(f)\n    OCD = data1['OCD']  # Observed closure dates\n\n# Load tide gauge and tidal prediction data from master3\nprint(\"Loading tide gauge and tidal prediction data...\")\nmast3_file = os.path.join(output_dir, 'mast3.pkl')\nif not os.path.exists(mast3_file):\n    raise FileNotFoundError(\n        f\"Required file {mast3_file} not found. \"\n        \"Please run Part 1, Part 2, and Part 3 first to generate the required data files.\"\n    )\nwith open(mast3_file, 'rb') as f:\n    data3 = pickle.load(f)\n    TSP = data3['TSP']  # Time series for predictions\n    TIP = data3['TIP']  # Predicted tides\n    WLP = data3['WLP']  # Observed water levels\n\n# Convert to numpy arrays if needed\nOCD = np.array(OCD)\nTSP = np.array(TSP)\nTIP = np.array(TIP)\nWLP = np.array(WLP)\n\nprint(f\"\\nData loaded:\")\nprint(f\"  Closure dates: {len(OCD)} closures\")\nif len(OCD) &gt; 0:\n    print(f\"    First closure: {OCD[0]}\")\n    print(f\"    Last closure: {OCD[-1]}\")\nprint(f\"  Time series points: {len(TSP):,}\")\nprint(f\"  Predicted tides: {len(TIP):,}\")\nprint(f\"  Observed water levels: {len(WLP):,}\")\n\nLoading barrier closure data...\nLoading tide gauge and tidal prediction data...\n\nData loaded:\n  Closure dates: 31 closures\n    First closure: 1986-10-20 00:00:00\n    Last closure: 2023-12-21 00:00:00\n  Time series points: 1,998,576\n  Predicted tides: 1,998,576\n  Observed water levels: 1,998,576"
  },
  {
    "objectID": "eastern_scheldt.html#figure---predicted-high-waters-and-closures",
    "href": "eastern_scheldt.html#figure---predicted-high-waters-and-closures",
    "title": "Eastern Scheldt Barrier",
    "section": "Figure - Predicted High Waters and Closures",
    "text": "Figure - Predicted High Waters and Closures\nThis figure shows predicted astronomical tides (red), observed water levels (blue), the difference between them (green), and barrier closure dates (vertical dashed magenta lines). The difference (WLP-TIP) represents the non-tidal component, primarily storm surge.\n\nfig, ax = plt.subplots(figsize=(14, 8))\n\n# Plot vertical lines for barrier closure dates\nfor i in range(len(OCD)):\n    ax.axvline(OCD[i], color='m', linestyle='--', linewidth=2, alpha=0.7)\n\n# Plot predicted tides (TIP)\nax.plot(TSP, TIP, 'r', linewidth=1, label='Predicted tide (TIP)', alpha=0.8)\n\n# Plot observed water levels (WLP)\nax.plot(TSP, WLP, 'b', linewidth=1, label='Observed water level (WLP)', alpha=0.8)\n\n# Plot difference (WLP - TIP) - surge component\nax.plot(TSP, WLP - TIP, 'g', linewidth=1, label='Surge (WLP - TIP)', alpha=0.8)\n\n# Formatting\nax.grid(True, alpha=0.3)\nax.set_xlabel('Date', fontweight='bold', fontsize=16)\nax.set_ylabel('Level (m)', fontweight='bold', fontsize=16)\nax.set_ylim(-3, 4)\nax.legend(fontsize=14, loc='best')\nax.tick_params(labelsize=14)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, 'master4_predicted_high_waters.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n\n\n\n\nPredicted high waters, observed water levels, and barrier closures for the Eastern Scheldt\n\n\n\n\n\nFigure saved to output/master4_predicted_high_waters.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "eastern_scheldt.html#part-1-barrier-closure-analysis",
    "href": "eastern_scheldt.html#part-1-barrier-closure-analysis",
    "title": "Eastern Scheldt Barrier",
    "section": "Part 1: Barrier Closure Analysis",
    "text": "Part 1: Barrier Closure Analysis\n\nTotal closures: Recorded across water years 1986/87 to 2024/25\nAnnual statistics: Minimum, mean, and maximum closures per water year\nTemporal patterns: Variability in closure frequency over time"
  },
  {
    "objectID": "eastern_scheldt.html#part-2-tide-gauge-data-processing",
    "href": "eastern_scheldt.html#part-2-tide-gauge-data-processing",
    "title": "Eastern Scheldt Barrier",
    "section": "Part 2: Tide Gauge Data Processing",
    "text": "Part 2: Tide Gauge Data Processing\n\nCombined time series: 1986-2023 at 10-minute intervals\nData sources: OS4 gauge (1986-1987) and RPBU gauge (1987-2023)\nData quality: Percentage of valid data points across the time series"
  },
  {
    "objectID": "eastern_scheldt.html#part-3-tidal-analysis",
    "href": "eastern_scheldt.html#part-3-tidal-analysis",
    "title": "Eastern Scheldt Barrier",
    "section": "Part 3: Tidal Analysis",
    "text": "Part 3: Tidal Analysis\n\nData quality: Calculated for each year to determine suitable reference years\nHarmonic analysis: Performed using pytides (Python equivalent of MATLAB’s t_tide)\nPredictions: Generated at 10-minute intervals for the entire period\nSurge calculation: The difference between observed and predicted water levels represents the non-tidal component (surge)"
  },
  {
    "objectID": "eastern_scheldt.html#part-4-visualization",
    "href": "eastern_scheldt.html#part-4-visualization",
    "title": "Eastern Scheldt Barrier",
    "section": "Part 4: Visualization",
    "text": "Part 4: Visualization\n\nClosure timing: When closures occurred relative to predicted high waters\nSurge contribution: The non-tidal component shows how much storm surge contributed to water levels\nTidal vs. meteorological effects: The difference between observed and predicted levels highlights meteorological forcing\n\nThe complete workflow provides a foundation for understanding barrier closure patterns, water level variability, and storm surge dynamics in the Eastern Scheldt estuary. The results can be used for further analysis of individual closure events, surge characteristics, and long-term trends."
  },
  {
    "objectID": "high_waters.html",
    "href": "high_waters.html",
    "title": "Storm Surge and Tidal Harmonic Analysis",
    "section": "",
    "text": "This document provides a detailed analysis of water levels and surge for Storm 12 (closure event index 11), Storm 23 (closure event index 23, 2007), and Storm 25 (closure event index 24) for the Eastern Scheldt, converting the MATLAB script master5.m to Python. The analysis:\nThis provides a comprehensive view of the water level conditions that led to the barrier closures for Storms 12, 23, and 25.\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport os\n\n# Configuration\noutput_dir = 'output'\nos.makedirs(output_dir, exist_ok=True)\n\nprint(f\"Analysis configuration:\")\nprint(f\"  Storms to process: Storm 12 (e=11, prefix '12_'), Storm 23 (e=23, prefix '23_'), and Storm 25 (e=24, prefix '25_')\")\nprint(f\"  Output directory: {output_dir}\")\n\nAnalysis configuration:\n  Storms to process: Storm 12 (e=11, prefix '12_'), Storm 23 (e=23, prefix '23_'), and Storm 25 (e=24, prefix '25_')\n  Output directory: output\n# Load barrier closure data from master1\nprint(\"Loading barrier closure data...\")\nmast1_file = os.path.join(output_dir, 'mast1.pkl')\nif not os.path.exists(mast1_file):\n    raise FileNotFoundError(\n        f\"Required file {mast1_file} not found. \"\n        \"Please run master1.qmd first to generate the required data file.\"\n    )\nwith open(mast1_file, 'rb') as f:\n    data1 = pickle.load(f)\n    OCD = data1['OCD']  # Observed closure dates\n\n# Load tide gauge and tidal prediction data from master3\nprint(\"Loading tide gauge and tidal prediction data...\")\nmast3_file = os.path.join(output_dir, 'mast3.pkl')\nif not os.path.exists(mast3_file):\n    raise FileNotFoundError(\n        f\"Required file {mast3_file} not found. \"\n        \"Please run master1.qmd, master2.qmd, and master3.qmd first to generate the required data files.\"\n    )\nwith open(mast3_file, 'rb') as f:\n    data3 = pickle.load(f)\n    TSP = data3['TSP']  # Time series for predictions\n    TIP = data3['TIP']  # Predicted tides\n    WLP = data3['WLP']  # Observed water levels\n\n# Convert to numpy arrays if needed\nOCD = np.array(OCD)\nTSP = np.array(TSP)\nTIP = np.array(TIP)\nWLP = np.array(WLP)\n\n# Calculate surge\nSUP = WLP - TIP\n\nprint(f\"\\nData loaded:\")\nprint(f\"  Total closure dates: {len(OCD)} closures\")\nprint(f\"  Time series points: {len(TSP):,}\")\n\n# Validate event indices for all storms\ne_storm12 = 11\ne_storm23 = 23\ne_storm25 = 24\nif e_storm12 &gt;= len(OCD):\n    raise ValueError(f\"Event index {e_storm12} (Storm 12) is out of range. There are {len(OCD)} closure events (indices 0-{len(OCD)-1})\")\nif e_storm23 &gt;= len(OCD):\n    raise ValueError(f\"Event index {e_storm23} (Storm 23) is out of range. There are {len(OCD)} closure events (indices 0-{len(OCD)-1})\")\nif e_storm25 &gt;= len(OCD):\n    raise ValueError(f\"Event index {e_storm25} (Storm 25) is out of range. There are {len(OCD)} closure events (indices 0-{len(OCD)-1})\")\n\nLoading barrier closure data...\nLoading tide gauge and tidal prediction data...\n\nData loaded:\n  Total closure dates: 31 closures\n  Time series points: 1,998,576"
  },
  {
    "objectID": "high_waters.html#storm-12",
    "href": "high_waters.html#storm-12",
    "title": "Storm Surge and Tidal Harmonic Analysis",
    "section": "Storm 12",
    "text": "Storm 12\nStorm 12 corresponds to closure event index 11 (0-indexed) and uses datasets with prefix “12_” in the 2_DATA directory.\n\n# Storm 12 configuration\ne = 11  # Closure event index (0-indexed, so e=11 means Storm 12)\nstorm_num = 12\nprefix = '12_'\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Processing Storm {storm_num} (event index {e}, prefix: {prefix})\")\nprint(f\"{'='*60}\")\n\n# Get the closure event date\nclosure_date = OCD[e]\nclosure_date_plus_one = closure_date + timedelta(days=1)\n\nprint(f\"  Closure date: {closure_date}\")\nprint(f\"  Closure date + 1 day: {closure_date_plus_one}\")\n\nfig, ax = plt.subplots(figsize=(14, 8))\n\n# Plot vertical lines for closure date and closure date + 1 day\nax.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\nax.axvline(closure_date_plus_one, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\n\n# Plot predicted tides (TIP)\nax.plot(TSP, TIP, 'r', linewidth=1, label='Predicted tide (TIP)', alpha=0.8)\n\n# Plot observed water levels (WLP)\nax.plot(TSP, WLP, 'b', linewidth=1, label='Observed water level (WLP)', alpha=0.8)\n\n# Plot difference (WLP - TIP) - surge component\nax.plot(TSP, WLP - TIP, 'g', linewidth=1, label='Surge (WLP - TIP)', alpha=0.8)\n\n# Set zoom window: 2 days before to 3 days after closure\nxlim_start = closure_date - timedelta(days=2)\nxlim_end = closure_date + timedelta(days=3)\n\n# Formatting\nax.grid(True, alpha=0.3)\nax.set_xlabel('Date', fontweight='bold', fontsize=16)\nax.set_ylabel('Level (m)', fontweight='bold', fontsize=16)\nax.set_title(f'Storm {storm_num} - Predicted High Waters (Zoomed View)', fontweight='bold', fontsize=18)\nax.set_ylim(-3, 4)\nax.set_xlim(xlim_start, xlim_end)\nax.legend(fontsize=14, loc='best')\nax.tick_params(labelsize=14)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure with storm number in filename\nfig_file = os.path.join(output_dir, f'high_waters_storm{storm_num}_predicted_high_waters_zoomed.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"  Figure saved to {fig_file}\")\n\n\n============================================================\nProcessing Storm 12 (event index 11, prefix: 12_)\n============================================================\n  Closure date: 1992-11-11 00:00:00\n  Closure date + 1 day: 1992-11-12 00:00:00\n\n\n\n\n\nPredicted high waters, observed water levels, and barrier closure (zoomed view) for Storm 12 - Eastern Scheldt\n\n\n\n\n  Figure saved to output/high_waters_storm12_predicted_high_waters_zoomed.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n# Storm 12 configuration\ne = 11  # Closure event index (0-indexed, so e=11 means Storm 12)\nstorm_num = 12\nprefix = '12_'\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Processing Storm {storm_num} (event index {e}, prefix: {prefix})\")\nprint(f\"{'='*60}\")\n\n# Get the closure event date\nclosure_date = OCD[e]\nclosure_date_plus_one = closure_date + timedelta(days=1)\n\nprint(f\"  Closure date: {closure_date}\")\nprint(f\"  Closure date + 1 day: {closure_date_plus_one}\")\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# Top panel: Water level and predicted tide\nax1.plot(TSP, WLP, 'b', linewidth=2, label='Water level')\nax1.plot(TSP, TIP, 'r', linewidth=2, label='Astronomical tide')\nax1.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\nax1.axvline(closure_date_plus_one, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\nax1.set_ylabel('Water level (m)', fontweight='bold', fontsize=20)\nax1.set_title(f'Storm {storm_num} - Water Level and Surge', fontweight='bold', fontsize=18)\nax1.legend(fontsize=14)\nax1.grid(True, alpha=0.3)\nax1.tick_params(labelsize=16)\nax1.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n\n# Bottom panel: Surge\nsurge_color = np.array([255, 103, 40]) / 255\nax2.plot(TSP, SUP, color=surge_color, linewidth=2)\nax2.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7)\nax2.axvline(closure_date_plus_one, color='m', linestyle='--', linewidth=2, alpha=0.7)\nax2.set_xlabel('Date', fontweight='bold', fontsize=20)\nax2.set_ylabel('Surge (m)', fontweight='bold', fontsize=20)\nax2.grid(True, alpha=0.3)\nax2.tick_params(labelsize=16)\nax2.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n\nplt.tight_layout()\nplt.show()\n\n# Save figure with storm number in filename\nfig_file = os.path.join(output_dir, f'high_waters_storm{storm_num}_water_level_surge.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"  Figure saved to {fig_file}\")\n\n\n============================================================\nProcessing Storm 12 (event index 11, prefix: 12_)\n============================================================\n  Closure date: 1992-11-11 00:00:00\n  Closure date + 1 day: 1992-11-12 00:00:00\n\n\n\n\n\nWater level and surge time series for Storm 12 closure event\n\n\n\n\n  Figure saved to output/high_waters_storm12_water_level_surge.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "high_waters.html#storm-23",
    "href": "high_waters.html#storm-23",
    "title": "Storm Surge and Tidal Harmonic Analysis",
    "section": "Storm 23",
    "text": "Storm 23\nStorm 23 corresponds to closure event index 23 (0-indexed) and uses datasets with prefix “23_” in the 2_DATA directory. This is the 2007 storm event.\n\n# Storm 23 configuration\ne = 23  # Closure event index (0-indexed, so e=23 means Storm 23)\nstorm_num = 23\nprefix = '23_'\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Processing Storm {storm_num} (event index {e}, prefix: {prefix})\")\nprint(f\"{'='*60}\")\n\n# Get the closure event date\nclosure_date = OCD[e]\nclosure_date_plus_one = closure_date + timedelta(days=1)\n\nprint(f\"  Closure date: {closure_date}\")\nprint(f\"  Closure date + 1 day: {closure_date_plus_one}\")\n\nfig, ax = plt.subplots(figsize=(14, 8))\n\n# Plot vertical lines for closure date and closure date + 1 day\nax.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\nax.axvline(closure_date_plus_one, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\n\n# Plot predicted tides (TIP)\nax.plot(TSP, TIP, 'r', linewidth=1, label='Predicted tide (TIP)', alpha=0.8)\n\n# Plot observed water levels (WLP)\nax.plot(TSP, WLP, 'b', linewidth=1, label='Observed water level (WLP)', alpha=0.8)\n\n# Plot difference (WLP - TIP) - surge component\nax.plot(TSP, WLP - TIP, 'g', linewidth=1, label='Surge (WLP - TIP)', alpha=0.8)\n\n# Set zoom window: 2 days before to 3 days after closure\nxlim_start = closure_date - timedelta(days=2)\nxlim_end = closure_date + timedelta(days=3)\n\n# Formatting\nax.grid(True, alpha=0.3)\nax.set_xlabel('Date', fontweight='bold', fontsize=16)\nax.set_ylabel('Level (m)', fontweight='bold', fontsize=16)\nax.set_title(f'Storm {storm_num} - Predicted High Waters (Zoomed View)', fontweight='bold', fontsize=18)\nax.set_ylim(-3, 4)\nax.set_xlim(xlim_start, xlim_end)\nax.legend(fontsize=14, loc='best')\nax.tick_params(labelsize=14)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure with storm number in filename\nfig_file = os.path.join(output_dir, f'high_waters_storm{storm_num}_predicted_high_waters_zoomed.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"  Figure saved to {fig_file}\")\n\n\n============================================================\nProcessing Storm 23 (event index 23, prefix: 23_)\n============================================================\n  Closure date: 2007-11-09 00:00:00\n  Closure date + 1 day: 2007-11-10 00:00:00\n\n\n\n\n\nPredicted high waters, observed water levels, and barrier closure (zoomed view) for Storm 23 - Eastern Scheldt\n\n\n\n\n  Figure saved to output/high_waters_storm23_predicted_high_waters_zoomed.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n# Storm 23 configuration\ne = 23  # Closure event index (0-indexed, so e=23 means Storm 23)\nstorm_num = 23\nprefix = '23_'\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Processing Storm {storm_num} (event index {e}, prefix: {prefix})\")\nprint(f\"{'='*60}\")\n\n# Get the closure event date\nclosure_date = OCD[e]\nclosure_date_plus_one = closure_date + timedelta(days=1)\n\nprint(f\"  Closure date: {closure_date}\")\nprint(f\"  Closure date + 1 day: {closure_date_plus_one}\")\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# Top panel: Water level and predicted tide\nax1.plot(TSP, WLP, 'b', linewidth=2, label='Water level')\nax1.plot(TSP, TIP, 'r', linewidth=2, label='Astronomical tide')\nax1.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\nax1.axvline(closure_date_plus_one, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\nax1.set_ylabel('Water level (m)', fontweight='bold', fontsize=20)\nax1.set_title(f'Storm {storm_num} - Water Level and Surge', fontweight='bold', fontsize=18)\nax1.legend(fontsize=14)\nax1.grid(True, alpha=0.3)\nax1.tick_params(labelsize=16)\nax1.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n\n# Bottom panel: Surge\nsurge_color = np.array([255, 103, 40]) / 255\nax2.plot(TSP, SUP, color=surge_color, linewidth=2)\nax2.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7)\nax2.axvline(closure_date_plus_one, color='m', linestyle='--', linewidth=2, alpha=0.7)\nax2.set_xlabel('Date', fontweight='bold', fontsize=20)\nax2.set_ylabel('Surge (m)', fontweight='bold', fontsize=20)\nax2.grid(True, alpha=0.3)\nax2.tick_params(labelsize=16)\nax2.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n\nplt.tight_layout()\nplt.show()\n\n# Save figure with storm number in filename\nfig_file = os.path.join(output_dir, f'high_waters_storm{storm_num}_water_level_surge.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"  Figure saved to {fig_file}\")\n\n\n============================================================\nProcessing Storm 23 (event index 23, prefix: 23_)\n============================================================\n  Closure date: 2007-11-09 00:00:00\n  Closure date + 1 day: 2007-11-10 00:00:00\n\n\n\n\n\nWater level and surge time series for Storm 23 closure event\n\n\n\n\n  Figure saved to output/high_waters_storm23_water_level_surge.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "high_waters.html#storm-25",
    "href": "high_waters.html#storm-25",
    "title": "Storm Surge and Tidal Harmonic Analysis",
    "section": "Storm 25",
    "text": "Storm 25\nStorm 25 corresponds to closure event index 24 (0-indexed) and uses datasets with prefix “25_” in the 2_DATA directory.\n\n# Storm 25 configuration\ne = 24  # Closure event index (0-indexed, so e=24 means Storm 25)\nstorm_num = 25\nprefix = '25_'\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Processing Storm {storm_num} (event index {e}, prefix: {prefix})\")\nprint(f\"{'='*60}\")\n\n# Get the closure event date\nclosure_date = OCD[e]\nclosure_date_plus_one = closure_date + timedelta(days=1)\n\nprint(f\"  Closure date: {closure_date}\")\nprint(f\"  Closure date + 1 day: {closure_date_plus_one}\")\n\nfig, ax = plt.subplots(figsize=(14, 8))\n\n# Plot vertical lines for closure date and closure date + 1 day\nax.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\nax.axvline(closure_date_plus_one, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\n\n# Plot predicted tides (TIP)\nax.plot(TSP, TIP, 'r', linewidth=1, label='Predicted tide (TIP)', alpha=0.8)\n\n# Plot observed water levels (WLP)\nax.plot(TSP, WLP, 'b', linewidth=1, label='Observed water level (WLP)', alpha=0.8)\n\n# Plot difference (WLP - TIP) - surge component\nax.plot(TSP, WLP - TIP, 'g', linewidth=1, label='Surge (WLP - TIP)', alpha=0.8)\n\n# Set zoom window: 2 days before to 3 days after closure\nxlim_start = closure_date - timedelta(days=2)\nxlim_end = closure_date + timedelta(days=3)\n\n# Formatting\nax.grid(True, alpha=0.3)\nax.set_xlabel('Date', fontweight='bold', fontsize=16)\nax.set_ylabel('Level (m)', fontweight='bold', fontsize=16)\nax.set_title(f'Storm {storm_num} - Predicted High Waters (Zoomed View)', fontweight='bold', fontsize=18)\nax.set_ylim(-3, 4)\nax.set_xlim(xlim_start, xlim_end)\nax.legend(fontsize=14, loc='best')\nax.tick_params(labelsize=14)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure with storm number in filename\nfig_file = os.path.join(output_dir, f'high_waters_storm{storm_num}_predicted_high_waters_zoomed.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"  Figure saved to {fig_file}\")\n\n\n============================================================\nProcessing Storm 25 (event index 24, prefix: 25_)\n============================================================\n  Closure date: 2013-12-05 00:00:00\n  Closure date + 1 day: 2013-12-06 00:00:00\n\n\n\n\n\nPredicted high waters, observed water levels, and barrier closure (zoomed view) for Storm 25 - Eastern Scheldt\n\n\n\n\n  Figure saved to output/high_waters_storm25_predicted_high_waters_zoomed.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n# Storm 25 configuration\ne = 24  # Closure event index (0-indexed, so e=24 means Storm 25)\nstorm_num = 25\nprefix = '25_'\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Processing Storm {storm_num} (event index {e}, prefix: {prefix})\")\nprint(f\"{'='*60}\")\n\n# Get the closure event date\nclosure_date = OCD[e]\nclosure_date_plus_one = closure_date + timedelta(days=1)\n\nprint(f\"  Closure date: {closure_date}\")\nprint(f\"  Closure date + 1 day: {closure_date_plus_one}\")\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# Top panel: Water level and predicted tide\nax1.plot(TSP, WLP, 'b', linewidth=2, label='Water level')\nax1.plot(TSP, TIP, 'r', linewidth=2, label='Astronomical tide')\nax1.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\nax1.axvline(closure_date_plus_one, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\nax1.set_ylabel('Water level (m)', fontweight='bold', fontsize=20)\nax1.set_title(f'Storm {storm_num} - Water Level and Surge', fontweight='bold', fontsize=18)\nax1.legend(fontsize=14)\nax1.grid(True, alpha=0.3)\nax1.tick_params(labelsize=16)\nax1.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n\n# Bottom panel: Surge\nsurge_color = np.array([255, 103, 40]) / 255\nax2.plot(TSP, SUP, color=surge_color, linewidth=2)\nax2.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7)\nax2.axvline(closure_date_plus_one, color='m', linestyle='--', linewidth=2, alpha=0.7)\nax2.set_xlabel('Date', fontweight='bold', fontsize=20)\nax2.set_ylabel('Surge (m)', fontweight='bold', fontsize=20)\nax2.grid(True, alpha=0.3)\nax2.tick_params(labelsize=16)\nax2.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n\nplt.tight_layout()\nplt.show()\n\n# Save figure with storm number in filename\nfig_file = os.path.join(output_dir, f'high_waters_storm{storm_num}_water_level_surge.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"  Figure saved to {fig_file}\")\n\n\n============================================================\nProcessing Storm 25 (event index 24, prefix: 25_)\n============================================================\n  Closure date: 2013-12-05 00:00:00\n  Closure date + 1 day: 2013-12-06 00:00:00\n\n\n/var/folders/m8/cp78hgjj6j937vhj63cdm7_c0000gn/T/ipykernel_10159/1153962993.py:42: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  plt.tight_layout()\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/IPython/core/pylabtools.py:170: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\nWater level and surge time series for Storm 25 closure event\n\n\n\n\n  Figure saved to output/high_waters_storm25_water_level_surge.png\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "high_waters.html#summary",
    "href": "high_waters.html#summary",
    "title": "Storm Surge and Tidal Harmonic Analysis",
    "section": "Summary",
    "text": "Summary\nThis analysis provides detailed insight into Storm 12, Storm 23, and Storm 25 closure event water levels:\n\nEvent focus: Detailed analysis of Storm 12 (closure event index 11, prefix “12_”), Storm 23 (closure event index 23, prefix “23_”, 2007), and Storm 25 (closure event index 24, prefix “25_”) for the Eastern Scheldt\nWater level context: Shows how observed water levels, predicted tides, and surge evolved around each closure event with closure date markers\nTime window: 5-day zoomed view (2 days before to 3 days after closure) captures the full storm event for each storm\nSurge analysis: The surge component (WLP-TIP) shows the meteorological forcing that contributed to each closure\nTidal vs. meteorological effects: The difference between observed and predicted levels highlights the meteorological forcing that triggered each closure\n\nThis detailed view helps understand the specific water level conditions that led to the barrier closures for Storms 12, 23, and 25, including the timing relative to predicted tides and the magnitude of the storm surge component. The prefix identifiers (“12_”, “23_”, and “25_”) can be used to identify the corresponding datasets in the 2_DATA directory."
  }
]