{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Eastern Scheldt Barrier\"\n",
        "execute:\n",
        "  echo: false\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Overview\n",
        "\n",
        "This document provides a complete analysis workflow for the Eastern Scheldt barrier system, combining barrier closure analysis, tide gauge data processing, tidal analysis, and visualization. The analysis covers the period from 1986 to 2024 and includes:\n",
        "\n",
        "1. **Barrier Closure Analysis**: Loading and analyzing past barrier closure dates, counting closures per water year, and calculating statistics\n",
        "2. **Tide Gauge Data Processing**: Loading and combining tide gauge data from multiple sources (OS4 and RPBU gauges)\n",
        "3. **Tidal Analysis**: Performing harmonic tidal decomposition and generating predicted astronomical tides\n",
        "4. **Visualization**: Creating comprehensive visualizations showing predicted high waters, observed water levels, and barrier closures\n",
        "\n",
        "Water years run from July 1 to June 30, which is more appropriate for coastal flood analysis than calendar years.\n"
      ],
      "id": "b073f992"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: shared-utilities\n",
        "#| include: true\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "# Shared configuration\n",
        "output_dir = 'output'\n",
        "data_dir = '../2_DATA'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Spatial bounds for maps\n",
        "XB_GTSM = [-10, 10]  # GTSM longitude bounds\n",
        "YB_GTSM = [45, 60]   # GTSM latitude bounds\n",
        "XB_MET = [-30, 15]   # ERA5 longitude bounds\n",
        "YB_MET = [40, 70]    # ERA5 latitude bounds\n",
        "\n",
        "# Time intervals for spatial plots\n",
        "TINT = 6   # Time interval for GTSM plots (hours)\n",
        "TINT_MET = 12  # Time interval for meteorological plots (hours)\n",
        "\n",
        "def load_master1_data():\n",
        "    \"\"\"Load barrier closure data from mast1.pkl\"\"\"\n",
        "    mast1_file = os.path.join(output_dir, 'mast1.pkl')\n",
        "    if not os.path.exists(mast1_file):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Required file {mast1_file} not found. \"\n",
        "            \"Please run Part 1 (Barrier Closure Analysis) first to generate the required data file.\"\n",
        "        )\n",
        "    with open(mast1_file, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data['OCD']  # Observed closure dates\n",
        "\n",
        "def load_master3_data():\n",
        "    \"\"\"Load tide gauge and tidal prediction data from mast3.pkl\"\"\"\n",
        "    mast3_file = os.path.join(output_dir, 'mast3.pkl')\n",
        "    if not os.path.exists(mast3_file):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Required file {mast3_file} not found. \"\n",
        "            \"Please run Part 1, Part 2, and Part 3 first to generate the required data files.\"\n",
        "        )\n",
        "    with open(mast3_file, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return {\n",
        "        'TSP': np.array(data['TSP']),\n",
        "        'TIP': np.array(data['TIP']),\n",
        "        'WLP': np.array(data['WLP'])\n",
        "    }\n",
        "\n",
        "def load_all_data():\n",
        "    \"\"\"Load both master1 and master3 data, return as dict\"\"\"\n",
        "    OCD = load_master1_data()\n",
        "    OCD = np.array(OCD)\n",
        "    data3 = load_master3_data()\n",
        "    return {\n",
        "        'OCD': OCD,\n",
        "        'TSP': data3['TSP'],\n",
        "        'TIP': data3['TIP'],\n",
        "        'WLP': data3['WLP']\n",
        "    }\n",
        "\n",
        "def create_storms_df(OCD):\n",
        "    \"\"\"Create storms DataFrame from closure dates\"\"\"\n",
        "    storms_df = pd.DataFrame({\n",
        "        'Closure Date': [d.strftime('%Y-%m-%d %H:%M') if isinstance(d, datetime) else str(d) for d in OCD],\n",
        "        'Year': [d.year if isinstance(d, datetime) else None for d in OCD],\n",
        "        'Month': [d.strftime('%B') if isinstance(d, datetime) else None for d in OCD],\n",
        "        'YearMonth': [(d.year, d.month) if isinstance(d, datetime) else None for d in OCD]\n",
        "    })\n",
        "    \n",
        "    # Add water year information\n",
        "    def get_water_year(date):\n",
        "        \"\"\"Determine water year (July 1 to June 30)\"\"\"\n",
        "        if isinstance(date, datetime):\n",
        "            if date.month >= 7:\n",
        "                return f\"{date.year}/{str(date.year + 1)[2:]}\"\n",
        "            else:\n",
        "                return f\"{date.year - 1}/{str(date.year)[2:]}\"\n",
        "        return None\n",
        "    \n",
        "    storms_df['Water Year'] = [get_water_year(d) for d in OCD]\n",
        "    \n",
        "    # Create Event index that groups storms by year and month\n",
        "    unique_year_months = storms_df['YearMonth'].unique()\n",
        "    year_month_to_index = {ym: idx + 1 for idx, ym in enumerate(sorted(unique_year_months))}\n",
        "    storms_df['Event'] = storms_df['YearMonth'].map(year_month_to_index)\n",
        "    \n",
        "    # Reorder columns\n",
        "    storms_df = storms_df[['Event', 'Closure Date', 'Water Year', 'Year', 'Month']]\n",
        "    \n",
        "    return storms_df\n",
        "\n",
        "def get_or_create_storms_df():\n",
        "    \"\"\"Get storms_df from pickle file or create it if it doesn't exist\"\"\"\n",
        "    storms_df_file = os.path.join(output_dir, 'storms_df.pkl')\n",
        "    \n",
        "    if os.path.exists(storms_df_file):\n",
        "        with open(storms_df_file, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    else:\n",
        "        # Load OCD and create storms_df\n",
        "        OCD = load_master1_data()\n",
        "        storms_df = create_storms_df(OCD)\n",
        "        # Save for future use\n",
        "        with open(storms_df_file, 'wb') as f:\n",
        "            pickle.dump(storms_df, f)\n",
        "        return storms_df\n",
        "\n",
        "print(\"Shared utilities and configuration loaded\")"
      ],
      "id": "shared-utilities",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Barrier Closures\n",
        "\n",
        "This section analyzes past closures of the Eastern Scheldt barrier. The analysis:\n",
        "\n",
        "1. Loads barrier closure dates from an Excel file\n",
        "2. Counts closures per water year (July 1 to July 1)\n",
        "3. Calculates statistics (min, mean, max, total closures)\n",
        "4. Creates a bar chart visualization\n"
      ],
      "id": "ce7f8c1f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup-master1\n",
        "#| include: true\n",
        "# Configuration - years from 1986 to 2024\n",
        "Y = list(range(1986, 2025))  # [1986, 1987, ..., 2024]\n",
        "\n",
        "print(f\"Analysis configuration:\")\n",
        "print(f\"  Water years: {Y[0]}/{str(Y[0]+1)[2:]} to {Y[-1]}/{str(Y[-1]+1)[2:]} ({len(Y)} years)\")"
      ],
      "id": "setup-master1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis\n"
      ],
      "id": "50ef93ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-closures\n",
        "#| include: true\n",
        "\n",
        "# 1. Observed closure data - Eastern Scheldt\n",
        "data_dir = '../2_DATA/1_BARRIER_CLOSURES'\n",
        "excel_file = os.path.join(data_dir, 'Eastern_Scheldt_Barrier_Past_Closures_2024.xlsx')\n",
        "df = pd.read_excel(excel_file, sheet_name='Closures')\n",
        "\n",
        "# Extract closure dates (date only) from column 2 (index 1)\n",
        "closure_dates_col = df.iloc[:, 1]  # Column 2 (0-indexed)\n",
        "\n",
        "# Extract closure times (should be the 'Start of closure' column)\n",
        "# Let's assume the time is in column 3 (index 2). Adjust if necessary.\n",
        "if df.shape[1] > 2:\n",
        "    closure_times_col = df.iloc[:, 3]  # Column 4 (Start of closure TIME/hh:mm if present)\n",
        "else:\n",
        "    # If there is no time column, fallback to NaT\n",
        "    closure_times_col = [pd.NaT] * len(closure_dates_col)\n",
        "\n",
        "# Combine closure date and time to make closure datetime\n",
        "OCD = []\n",
        "for d, t in zip(closure_dates_col, closure_times_col):\n",
        "    if pd.isna(d):\n",
        "        continue\n",
        "    date_val = d.to_pydatetime() if isinstance(d, pd.Timestamp) else d\n",
        "    if not pd.isna(t):\n",
        "        # t can be a time or datetime or string like \"13:00\"\n",
        "        if isinstance(t, pd.Timestamp):\n",
        "            time_val = t.time()\n",
        "        elif isinstance(t, str):\n",
        "            try:\n",
        "                time_val = pd.to_datetime(t).time()\n",
        "            except Exception:\n",
        "                time_val = None\n",
        "        elif hasattr(t, 'time'):\n",
        "            time_val = t.time()\n",
        "        else:\n",
        "            time_val = None\n",
        "        if time_val is not None:\n",
        "            combined_dt = datetime.combine(date_val.date(), time_val)\n",
        "        else:\n",
        "            combined_dt = date_val\n",
        "    else:\n",
        "        combined_dt = date_val\n",
        "    OCD.append(combined_dt)\n",
        "\n",
        "print(f\"Loaded {len(OCD)} closure datetimes\")\n",
        "if len(OCD) > 0:\n",
        "    print(f\"  First closure: {OCD[0]}\")\n",
        "    print(f\"  Last closure: {OCD[-1]}\")\n",
        "\n",
        "# 2. Count closures per water year (July 1 to July 1)\n",
        "OCS = []  # Will store [index, year, year+1, num_closures]\n",
        "YT = []   # Will store year labels like \"1986/87\"\n",
        "\n",
        "for co, y in enumerate(Y, start=1):\n",
        "    # Find closures between July 1 of year y and July 1 of year y+1\n",
        "    start_date = datetime(y, 7, 1, 0, 0, 0)\n",
        "    end_date = datetime(y + 1, 7, 1, 0, 0, 0)\n",
        "    \n",
        "    # Count closures in this water year\n",
        "    closures_in_year = [d for d in OCD if start_date <= d < end_date]\n",
        "    num_closures = len(closures_in_year)\n",
        "    \n",
        "    OCS.append([co, y, y + 1, num_closures])\n",
        "    \n",
        "    # Create year label (e.g., \"1986/87\")\n",
        "    year_str = str(y + 1)\n",
        "    YT.append(f\"{y}/{year_str[2:]}\")\n",
        "\n",
        "OCS = np.array(OCS)\n",
        "\n",
        "print(f\"\\nClosures per water year calculated for {len(OCS)} years\")\n",
        "\n",
        "print(f\"  Total closures: {np.sum(OCS[:, 3])}\")\n",
        "\n",
        "# 3. Calculate statistics\n",
        "E = np.array([\n",
        "    np.min(OCS[:, 3]),      # Minimum closures per year\n",
        "    np.mean(OCS[:, 3]),     # Mean closures per year\n",
        "    np.max(OCS[:, 3]),      # Maximum closures per year\n",
        "    np.sum(OCS[:, 3])       # Total closures\n",
        "])\n",
        "\n",
        "print(f\"\\nClosure statistics:\")\n",
        "print(f\"  Min per year: {E[0]:.1f}\")\n",
        "print(f\"  Mean per year: {E[1]:.2f}\")\n",
        "print(f\"  Max per year: {E[2]:.1f}\")\n",
        "print(f\"  Total: {E[3]:.0f}\")\n",
        "\n",
        "#| label: save-master1\n",
        "#| include: true\n",
        "\n",
        "# 4. Save data\n",
        "output_file = os.path.join(output_dir, 'mast1.pkl')\n",
        "with open(output_file, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'OCD': OCD,\n",
        "        'OCS': OCS,\n",
        "        'E': E,\n",
        "        'YT': YT\n",
        "    }, f)\n",
        "print(f\"\\nData saved to {output_file}\")"
      ],
      "id": "load-closures",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization \n",
        "\n",
        "This bar chart shows the number of barrier closures per water year from 1986/87 to 2024/25. The Eastern Scheldt barrier closes when water levels exceed a threshold, typically during storm surge events.\n"
      ],
      "id": "8318b669"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 12,
        "fig-height": 6
      },
      "source": [
        "#| label: plot-closures\n",
        "#| fig-cap: Number of Eastern Scheldt barrier closures per water year (July 1 to June 30), 1986-2024\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.bar(OCS[:, 0], OCS[:, 3], color='black')\n",
        "ax.set_xlim(0.2, len(OCS) + 0.8)\n",
        "ax.set_xticks(range(1, len(OCS) + 1))\n",
        "ax.set_xticklabels(YT, rotation=90)\n",
        "ax.set_ylim(0, 5)\n",
        "ax.set_yticks(range(6))\n",
        "ax.set_ylabel('Number of closures', fontweight='bold', fontsize=18)\n",
        "ax.set_title('Eastern Scheldt', fontweight='bold', fontsize=18)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.tick_params(labelsize=16)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save figure\n",
        "fig_file = os.path.join(output_dir, 'master1_closures.png')\n",
        "plt.savefig(fig_file, dpi=150, bbox_inches='tight')\n",
        "print(f\"\\nFigure saved to {fig_file}\")"
      ],
      "id": "plot-closures",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following table lists all barrier closure dates, which correspond to storm events that triggered the Eastern Scheldt barrier.\n"
      ],
      "id": "710ca8e0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: table-storms\n",
        "#| tbl-cap: Complete list of Eastern Scheldt barrier closures (storm events), 1986-2024\n",
        "\n",
        "# Get or create storms DataFrame\n",
        "storms_df = get_or_create_storms_df()\n",
        "\n",
        "# Save storms_df to mast1.pkl for consistency (update existing file)\n",
        "mast1_file = os.path.join(output_dir, 'mast1.pkl')\n",
        "if os.path.exists(mast1_file):\n",
        "    with open(mast1_file, 'rb') as f:\n",
        "        mast1_data = pickle.load(f)\n",
        "    mast1_data['storms_df'] = storms_df\n",
        "    with open(mast1_file, 'wb') as f:\n",
        "        pickle.dump(mast1_data, f)\n",
        "\n",
        "# Display table\n",
        "print(f\"\\nTotal number of closures: {len(storms_df)}\")\n",
        "storms_df"
      ],
      "id": "table-storms",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Tidal Harmonic Analysis\n",
        "\n",
        "This section performs comprehensive tidal harmonic analysis on tide gauge data from the Eastern Scheldt, including data processing, harmonic decomposition, and visualization. The analysis:\n",
        "\n",
        "1. Loads and processes raw tide gauge data from multiple sources (OS4 and RPBU gauges)\n",
        "2. Performs harmonic tidal decomposition using `pytides`\n",
        "3. Generates tidal predictions for all years\n",
        "4. Compares observed water levels with predicted astronomical tides\n",
        "5. Visualizes predicted high waters and barrier closures\n",
        "\n",
        "The predicted tides represent only the astronomical component, while observed water levels include both tides and meteorological effects (storm surges, wind setup, etc.).\n",
        "\n",
        "## Tide Gauge Data Processing\n",
        "\n",
        "This section loads and processes raw tide gauge data from the Eastern Scheldt. The analysis:\n",
        "\n",
        "1. Loads data from multiple tide gauge files (RPBU and OS4 gauges)\n",
        "2. Cleans data by removing invalid values\n",
        "3. Combines time series from different gauges to create a continuous record\n",
        "4. Creates a visualization of the complete water level time series\n",
        "\n",
        "The combined time series spans from 1986 to 2023, using OS4 gauge data for the early period (1986-1987) and RPBU gauge data for the main period (1987-2023).\n"
      ],
      "id": "3143aade"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup-master2\n",
        "#| include: true\n",
        "\n",
        "# Configuration\n",
        "data_dir = '../2_DATA'\n",
        "\n",
        "print(f\"Data directory: {data_dir}\")\n",
        "print(f\"Output directory: {output_dir}\")"
      ],
      "id": "setup-master2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load RPBU 1987-2022 Data\n",
        "\n",
        "Load the main tide gauge dataset from the RPBU gauge covering April 1987 to December 2022.\n"
      ],
      "id": "659c375e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-rpbu-1987-2022\n",
        "#| include: true\n",
        "\n",
        "# 1. Load in Data 1987 to 2022 (RPBU gauge)\n",
        "file_in = os.path.join(data_dir, '2_TIDE_GAUGE/ES/RPBU_1987_2022.dat')\n",
        "D = np.loadtxt(file_in)\n",
        "\n",
        "# Create time series: April 15, 1987 to December 31, 2022, 10-minute intervals\n",
        "start_date = datetime(1987, 4, 15, 0, 0, 0)\n",
        "end_date = datetime(2022, 12, 31, 23, 50, 0)\n",
        "TS1 = pd.date_range(start=start_date, end=end_date, freq='10min').to_pydatetime()\n",
        "\n",
        "# Extract water level data from column 3 (index 2)\n",
        "WL1 = D[:, 2]\n",
        "\n",
        "# Remove default values (9999 indicates missing)\n",
        "WL1[WL1 == 9999] = np.nan\n",
        "\n",
        "# Convert to meters (from cm)\n",
        "WL1 = WL1 / 100.0\n",
        "\n",
        "print(f\"Loaded RPBU 1987-2022: {len(TS1):,} data points\")\n",
        "print(f\"  Start: {TS1[0]}\")\n",
        "print(f\"  End: {TS1[-1]}\")\n",
        "print(f\"  Valid data: {np.sum(~np.isnan(WL1)):,} points ({100*np.sum(~np.isnan(WL1))/len(WL1):.1f}%)\")"
      ],
      "id": "load-rpbu-1987-2022",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load RPBU 2023 Data\n",
        "\n",
        "Load the 2023 data from the RPBU gauge to extend the time series.\n"
      ],
      "id": "2b518b11"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-rpbu-2023\n",
        "#| include: true\n",
        "\n",
        "# 2. Load in Data 2023 (RPBU gauge)\n",
        "file_in = os.path.join(data_dir, '2_TIDE_GAUGE/ES/RPBU_2023.dat')\n",
        "D = np.loadtxt(file_in)\n",
        "\n",
        "# Create time series: January 1, 2023 to December 31, 2023, 10-minute intervals\n",
        "start_date = datetime(2023, 1, 1, 0, 0, 0)\n",
        "end_date = datetime(2023, 12, 31, 23, 50, 0)\n",
        "TS2 = pd.date_range(start=start_date, end=end_date, freq='10min').to_pydatetime()\n",
        "\n",
        "# Extract water level data from column 3 (index 2)\n",
        "WL2 = D[:, 2]\n",
        "\n",
        "# Remove default values (9999 indicates missing)\n",
        "WL2[WL2 == 9999] = np.nan\n",
        "\n",
        "# Convert to meters (from cm)\n",
        "WL2 = WL2 / 100.0\n",
        "\n",
        "print(f\"Loaded RPBU 2023: {len(TS2):,} data points\")\n",
        "print(f\"  Start: {TS2[0]}\")\n",
        "print(f\"  End: {TS2[-1]}\")\n",
        "print(f\"  Valid data: {np.sum(~np.isnan(WL2)):,} points ({100*np.sum(~np.isnan(WL2))/len(WL2):.1f}%)\")"
      ],
      "id": "load-rpbu-2023",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load OS4 1982-2023 Data\n",
        "\n",
        "Load data from the OS4 gauge, which provides coverage for the early period before RPBU data begins.\n"
      ],
      "id": "f9f3afce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-os4\n",
        "#| include: true\n",
        "\n",
        "# 3. Load in Data 1982 to 2023 for OS4 gauge\n",
        "file_in = os.path.join(data_dir, '2_TIDE_GAUGE/ES/OS4_1982_2023.dat')\n",
        "D = np.loadtxt(file_in)\n",
        "\n",
        "# Create time series: January 1, 1982 to December 31, 2023, 10-minute intervals\n",
        "start_date = datetime(1982, 1, 1, 0, 0, 0)\n",
        "end_date = datetime(2023, 12, 31, 23, 50, 0)\n",
        "TSO = pd.date_range(start=start_date, end=end_date, freq='10min').to_pydatetime()\n",
        "\n",
        "# Extract water level data from column 3 (index 2)\n",
        "WLO = D[:, 2]\n",
        "\n",
        "# Remove default values (values outside [-500, 500] are invalid)\n",
        "WLO[(WLO < -500) | (WLO > 500)] = np.nan\n",
        "\n",
        "# Convert to meters (from cm)\n",
        "WLO = WLO / 100.0\n",
        "\n",
        "print(f\"Loaded OS4 1982-2023: {len(TSO):,} data points\")\n",
        "print(f\"  Start: {TSO[0]}\")\n",
        "print(f\"  End: {TSO[-1]}\")\n",
        "print(f\"  Valid data: {np.sum(~np.isnan(WLO)):,} points ({100*np.sum(~np.isnan(WLO))/len(WLO):.1f}%)\")"
      ],
      "id": "load-os4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Time Series\n",
        "\n",
        "Combine the time series from different gauges to create a continuous record. Use OS4 data for the early period (1986-1987) before RPBU starts, then switch to RPBU data for the main period.\n"
      ],
      "id": "1cf32cdd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: combine-series\n",
        "#| include: true\n",
        "\n",
        "# 4. Combine time series\n",
        "# Use OS4 data for period 1986-01-01 to 1987-04-15 (before RPBU starts)\n",
        "start_combine = datetime(1986, 1, 1, 0, 0, 0)\n",
        "end_combine = datetime(1987, 4, 15, 0, 0, 0)\n",
        "mask_early = (TSO >= start_combine) & (TSO < end_combine)\n",
        "TSO_early = TSO[mask_early]\n",
        "WLO_early = WLO[mask_early]\n",
        "\n",
        "# Combine: OS4 early period + RPBU 1987-2022 + RPBU 2023\n",
        "TSP = np.concatenate([TSO_early, TS1, TS2])\n",
        "WLP = np.concatenate([WLO_early, WL1, WL2])\n",
        "\n",
        "# Create reference time series for checking\n",
        "TSCHECK = pd.date_range(\n",
        "    start=datetime(1986, 1, 1, 0, 0, 0),\n",
        "    end=datetime(2023, 12, 31, 23, 50, 0),\n",
        "    freq='10min'\n",
        ").to_pydatetime()\n",
        "\n",
        "print(f\"\\nCombined time series: {len(TSP):,} data points\")\n",
        "print(f\"  Start: {TSP[0]}\")\n",
        "print(f\"  End: {TSP[-1]}\")\n",
        "print(f\"  Valid data: {np.sum(~np.isnan(WLP)):,} points ({100*np.sum(~np.isnan(WLP))/len(WLP):.1f}%)\")\n",
        "print(f\"  Time span: {(TSP[-1] - TSP[0]).days} days\")"
      ],
      "id": "combine-series",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the combined time series to a pickle file for use in subsequent analyses.\n"
      ],
      "id": "5f480577"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: save-master2\n",
        "#| include: true\n",
        "\n",
        "# 5. Save data\n",
        "output_file = os.path.join(output_dir, 'mast2.pkl')\n",
        "with open(output_file, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'TSP': TSP,\n",
        "        'WLP': WLP,\n",
        "        'TSCHECK': TSCHECK\n",
        "    }, f)\n",
        "print(f\"\\nData saved to {output_file}\")"
      ],
      "id": "save-master2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization \n",
        "\n",
        "This figure shows the combined tide gauge water level time series from 1986 to 2023 at 10-minute intervals. The data combines OS4 gauge measurements (1986-1987) and RPBU gauge measurements (1987-2023) for the Eastern Scheldt. Water levels are shown in meters relative to NAP (Normal Amsterdam Peil, the Dutch reference datum). The time series shows tidal variations, storm surges, and long-term water level patterns over the 38-year period.\n"
      ],
      "id": "54e7cd6b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 6
      },
      "source": [
        "#| label: plot-time-series\n",
        "#| fig-cap: Combined tide gauge water level time series for the Eastern Scheldt, 1986-2023. Data combines OS4 gauge (1986-1987) and RPBU gauge (1987-2023) measurements at 10-minute intervals.\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "ax.plot(TSP, WLP, 'b', linewidth=0.5)\n",
        "ax.set_xlabel('Date', fontweight='bold', fontsize=20)\n",
        "ax.set_ylabel('Water level (m NAP)', fontweight='bold', fontsize=20)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.tick_params(labelsize=16)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save figure\n",
        "fig_file = os.path.join(output_dir, 'master2_tide_gauge.png')\n",
        "plt.savefig(fig_file, dpi=150, bbox_inches='tight')\n",
        "print(f\"\\nFigure saved to {fig_file}\")"
      ],
      "id": "plot-time-series",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tidal Analysis\n",
        "\n",
        "This section performs harmonic tidal analysis on tide gauge data from the Eastern Scheldt. The analysis:\n",
        "\n",
        "1. Calculates data quality for each year (1986-2023)\n",
        "2. Performs harmonic tidal decomposition using `pytides` (replacing MATLAB's `t_tide`)\n",
        "3. Generates tidal predictions for all years\n",
        "4. Compares observed water levels with predicted astronomical tides\n",
        "\n",
        "The predicted tides represent only the astronomical component, while observed water levels include both tides and meteorological effects (storm surges, wind setup, etc.).\n",
        "\n",
        "### Setup and Configuration\n"
      ],
      "id": "81092039"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup-master3\n",
        "#| include: true\n",
        "\n",
        "from pytides.tide import Tide\n",
        "\n",
        "# Configuration\n",
        "Y = list(range(1986, 2024))  # Years 1986 to 2023\n",
        "th = 60  # Data quality threshold (percentage)\n",
        "lat = 51.64  # Latitude for tidal analysis\n",
        "\n",
        "print(f\"Analysis configuration:\")\n",
        "print(f\"  Years: {Y[0]} to {Y[-1]} ({len(Y)} years)\")\n",
        "print(f\"  Data quality threshold: {th}%\")\n",
        "print(f\"  Latitude: {lat}Â°\")"
      ],
      "id": "setup-master3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Tide Gauge Data\n"
      ],
      "id": "e1dfef00"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-data-master3\n",
        "#| include: true\n",
        "\n",
        "print(\"Loading tide gauge data...\")\n",
        "mast2_file = os.path.join(output_dir, 'mast2.pkl')\n",
        "if not os.path.exists(mast2_file):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Required file {mast2_file} not found. \"\n",
        "        \"Please run Part 2 (Tide Gauge Data Processing) first to generate the required data file.\"\n",
        "    )\n",
        "with open(mast2_file, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "    TSP = data['TSP']\n",
        "    WLP = data['WLP']\n",
        "\n",
        "# Convert to numpy arrays if needed\n",
        "TSP = np.array(TSP)\n",
        "WLP = np.array(WLP)\n",
        "\n",
        "print(f\"Loaded {len(TSP):,} data points\")\n",
        "print(f\"  Start: {TSP[0]}\")\n",
        "print(f\"  End: {TSP[-1]}\")\n",
        "print(f\"  Valid data: {np.sum(~np.isnan(WLP)):,} points ({100*np.sum(~np.isnan(WLP))/len(WLP):.1f}%)\")"
      ],
      "id": "load-data-master3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate Data Quality Per Year\n"
      ],
      "id": "ca0fc70e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: data-quality\n",
        "#| include: true\n",
        "\n",
        "print(\"\\nCalculating data quality per year...\")\n",
        "DQ = []\n",
        "\n",
        "for y in Y:\n",
        "    # Find data points in this calendar year (Jan 1 to Jan 1 next year)\n",
        "    start_date = datetime(y, 1, 1, 0, 0, 0)\n",
        "    end_date = datetime(y + 1, 1, 1, 0, 0, 0)\n",
        "    mask = (TSP >= start_date) & (TSP < end_date)\n",
        "    j = np.where(mask)[0]\n",
        "    \n",
        "    if len(j) == 0:\n",
        "        quality = 0\n",
        "    else:\n",
        "        # Count non-NaN values\n",
        "        k = np.where(~np.isnan(WLP[j]))[0]\n",
        "        quality = (len(k) / len(j)) * 100\n",
        "    \n",
        "    # Initialize with 3 columns: [year, quality, reference_year]\n",
        "    # reference_year will be filled in during tidal analysis\n",
        "    DQ.append([y, quality, np.nan])\n",
        "\n",
        "DQ = np.array(DQ)\n",
        "\n",
        "# Display summary statistics\n",
        "print(f\"\\nData quality summary:\")\n",
        "print(f\"  Range: {np.min(DQ[:, 1]):.1f}% to {np.max(DQ[:, 1]):.1f}%\")\n",
        "print(f\"  Mean: {np.mean(DQ[:, 1]):.1f}%\")\n",
        "print(f\"  Years with quality >= {th}%: {np.sum(DQ[:, 1] >= th)}/{len(Y)}\")"
      ],
      "id": "data-quality",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tidal Analysis and Prediction\n",
        "\n",
        "For each target year, the analysis determines a reference year, extracts data, performs harmonic decomposition, and generates predictions.\n"
      ],
      "id": "70f6131f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tidal-analysis\n",
        "#| include: true\n",
        "\n",
        "print(\"\\nPerforming tidal analysis and prediction...\")\n",
        "TSP2 = []\n",
        "TIP = []\n",
        "\n",
        "for co, y in enumerate(Y):\n",
        "    print(f\"  Processing year {y} ({co+1}/{len(Y)})...\")\n",
        "    \n",
        "    # Determine reference year\n",
        "    if DQ[co, 1] >= th:\n",
        "        # Use target year as reference\n",
        "        yr = y\n",
        "        DQ[co, 2] = yr\n",
        "    else:\n",
        "        # Find nearest year with quality >= threshold\n",
        "        # Calculate distances from target year\n",
        "        distances = np.abs(DQ[:, 0] - y)\n",
        "        \n",
        "        # Filter to only years with quality >= threshold\n",
        "        good_mask = DQ[:, 1] >= th\n",
        "        \n",
        "        if np.any(good_mask):\n",
        "            # Get distances for good years only\n",
        "            good_distances = distances[good_mask]\n",
        "            good_indices = np.where(good_mask)[0]\n",
        "            \n",
        "            # Find nearest good year\n",
        "            nearest_idx = good_indices[np.argmin(good_distances)]\n",
        "            yr = int(DQ[nearest_idx, 0])\n",
        "        else:\n",
        "            # Fallback: use year with best quality\n",
        "            best_idx = np.argmax(DQ[:, 1])\n",
        "            yr = int(DQ[best_idx, 0])\n",
        "        \n",
        "        DQ[co, 2] = yr\n",
        "    \n",
        "    # Extract reference year data (1 year + 1 day for analysis)\n",
        "    ref_start = datetime(yr, 1, 1, 0, 0, 0)\n",
        "    ref_end = datetime(yr + 1, 1, 2, 0, 0, 0)  # +1 day\n",
        "    \n",
        "    mask_ref = (TSP >= ref_start) & (TSP < ref_end)\n",
        "    j_ref = np.where(mask_ref)[0]\n",
        "    \n",
        "    if len(j_ref) == 0:\n",
        "        print(f\"    Warning: No data found for reference year {yr}\")\n",
        "        continue\n",
        "    \n",
        "    # Get water levels and times for reference year\n",
        "    WLP_ref = WLP[j_ref]\n",
        "    TSP_ref = TSP[j_ref]\n",
        "    \n",
        "    # Remove NaN values for pytides analysis\n",
        "    valid_mask = ~np.isnan(WLP_ref)\n",
        "    WLP_clean = WLP_ref[valid_mask]\n",
        "    TSP_clean = TSP_ref[valid_mask]\n",
        "    \n",
        "    if len(WLP_clean) < 1000:  # Need sufficient data for analysis\n",
        "        print(f\"    Warning: Insufficient data for reference year {yr} ({len(WLP_clean)} points)\")\n",
        "        continue\n",
        "    \n",
        "    # Perform tidal analysis using pytides\n",
        "    try:\n",
        "        tide_model = Tide.decompose(\n",
        "            heights=WLP_clean,\n",
        "            t=TSP_clean\n",
        "        )\n",
        "        print(f\"    Analyzed {len(WLP_clean):,} points from reference year {yr}\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Error in tidal analysis for year {y}: {e}\")\n",
        "        continue\n",
        "    \n",
        "    # Generate prediction timestamps for target year (10-minute intervals)\n",
        "    pred_start = datetime(y, 1, 1, 0, 0, 0)\n",
        "    pred_end = datetime(y, 12, 31, 23, 50, 0)\n",
        "    tsp2 = pd.date_range(start=pred_start, end=pred_end, freq='10min').to_pydatetime()\n",
        "    \n",
        "    # Predict tides\n",
        "    try:\n",
        "        tip = tide_model.at(tsp2)\n",
        "        TIP.extend(tip)\n",
        "        TSP2.extend(tsp2)\n",
        "        print(f\"    Predicted {len(tip):,} points for year {y}\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Error in prediction for year {y}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Convert to numpy arrays\n",
        "TSP2 = np.array(TSP2)\n",
        "TIP = np.array(TIP)\n",
        "\n",
        "print(f\"\\nTotal predictions: {len(TIP):,} points\")\n",
        "if len(TIP) > 0:\n",
        "    print(f\"  Start: {TSP2[0]}\")\n",
        "    print(f\"  End: {TSP2[-1]}\")"
      ],
      "id": "tidal-analysis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: save-master3\n",
        "#| include: true\n",
        "\n",
        "output_file = os.path.join(output_dir, 'mast3.pkl')\n",
        "with open(output_file, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'TSP': TSP,\n",
        "        'WLP': WLP,\n",
        "        'TIP': TIP,\n",
        "        'TSP2': TSP2,\n",
        "        'DQ': DQ\n",
        "    }, f)\n",
        "print(f\"Data saved to {output_file}\")"
      ],
      "id": "save-master3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization \n",
        "\n",
        "This figure compares observed water levels (blue) with predicted astronomical tides (red) from harmonic analysis. The predicted tides represent the astronomical component only, while observed water levels include both tides and meteorological effects.\n"
      ],
      "id": "2db028d4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 6
      },
      "source": [
        "#| label: plot-comparison\n",
        "#| fig-cap: Comparison of observed water levels (blue) and predicted astronomical tides (red) for the Eastern Scheldt, 1986-2023\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "ax.plot(TSP2, TIP, 'r', linewidth=0.5, label='Predicted tide', alpha=0.7)\n",
        "ax.plot(TSP, WLP, 'b', linewidth=0.5, label='Observed water level', alpha=0.7)\n",
        "ax.set_xlabel('Date', fontweight='bold', fontsize=16)\n",
        "ax.set_ylabel('Level (m NAP)', fontweight='bold', fontsize=16)\n",
        "ax.legend(fontsize=14)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.tick_params(labelsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save figure\n",
        "fig_file = os.path.join(output_dir, 'master3_tidal_analysis.png')\n",
        "plt.savefig(fig_file, dpi=150, bbox_inches='tight')\n",
        "print(f\"\\nFigure saved to {fig_file}\")"
      ],
      "id": "plot-comparison",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predicted High Waters and Barrier Closures\n",
        "\n",
        "This section plots time-series of predicted high waters and barrier closures for the Eastern Scheldt. The analysis:\n",
        "\n",
        "1. Loads barrier closure dates and tide gauge data from previous analyses\n",
        "2. Creates a visualization showing:\n",
        "   - Barrier closure dates as vertical dashed lines\n",
        "   - Predicted astronomical tides (TIP)\n",
        "   - Observed water levels (WLP)\n",
        "   - The difference between observed and predicted levels (surge component)\n",
        "\n",
        "This visualization helps identify when barrier closures occurred relative to predicted high waters and actual water levels.\n"
      ],
      "id": "90f8f5a4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-data-master4\n",
        "#| include: true\n",
        "\n",
        "# Load all data using shared function\n",
        "print(\"Loading data...\")\n",
        "data = load_all_data()\n",
        "OCD = data['OCD']\n",
        "TSP = data['TSP']\n",
        "TIP = data['TIP']\n",
        "WLP = data['WLP']\n",
        "\n",
        "print(f\"\\nData loaded:\")\n",
        "print(f\"  Closure dates: {len(OCD)} closures\")\n",
        "if len(OCD) > 0:\n",
        "    print(f\"    First closure: {OCD[0]}\")\n",
        "    print(f\"    Last closure: {OCD[-1]}\")\n",
        "print(f\"  Time series points: {len(TSP):,}\")\n",
        "print(f\"  Predicted tides: {len(TIP):,}\")\n",
        "print(f\"  Observed water levels: {len(WLP):,}\")"
      ],
      "id": "load-data-master4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization \n",
        "\n",
        "This figure shows predicted astronomical tides (red), observed water levels (blue), the difference between them (green), and barrier closure dates (vertical dashed magenta lines). The difference (WLP-TIP) represents the non-tidal component, primarily storm surge.\n"
      ],
      "id": "58f33368"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 8
      },
      "source": [
        "#| label: plot-predicted-high-waters\n",
        "#| fig-cap: Predicted high waters, observed water levels, and barrier closures for the Eastern Scheldt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Plot vertical lines for barrier closure dates\n",
        "for i in range(len(OCD)):\n",
        "    ax.axvline(OCD[i], color='m', linestyle='--', linewidth=2, alpha=0.7)\n",
        "\n",
        "# Plot predicted tides (TIP)\n",
        "ax.plot(TSP, TIP, 'r', linewidth=1, label='Predicted tide (TIP)', alpha=0.8)\n",
        "\n",
        "# Plot observed water levels (WLP)\n",
        "ax.plot(TSP, WLP, 'b', linewidth=1, label='Observed water level (WLP)', alpha=0.8)\n",
        "\n",
        "# Plot difference (WLP - TIP) - surge component\n",
        "ax.plot(TSP, WLP - TIP, 'g', linewidth=1, label='Surge (WLP - TIP)', alpha=0.8)\n",
        "\n",
        "# Formatting\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlabel('Date', fontweight='bold', fontsize=16)\n",
        "ax.set_ylabel('Level (m)', fontweight='bold', fontsize=16)\n",
        "ax.set_ylim(-3, 4)\n",
        "ax.legend(fontsize=14, loc='best')\n",
        "ax.tick_params(labelsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save figure\n",
        "fig_file = os.path.join(output_dir, 'master4_predicted_high_waters.png')\n",
        "plt.savefig(fig_file, dpi=150, bbox_inches='tight')\n",
        "print(f\"\\nFigure saved to {fig_file}\")"
      ],
      "id": "plot-predicted-high-waters",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Storm Water Level Analysis\n",
        "\n",
        "This section provides a detailed analysis of water levels and surge for Storm 5, Storm 6, Storm 7, Storm 8, Storm 9, Storm 10, Storm 11, Storm 12, Storm 13, Storm 18 (2007), and Storm 19 for the Eastern Scheldt, converting the MATLAB script `master5.m` to Python. The analysis:\n",
        "\n",
        "1. Loads barrier closure dates and tide gauge data from previous analyses\n",
        "2. Calculates surge component (WLP - TIP)\n",
        "3. Creates detailed water level visualizations with closure date markers for each storm:\n",
        "   - Zoomed water level and surge time series with closure markers (5-day window)\n",
        "   - Detailed two-panel view showing water level vs. tide and surge separately\n",
        "\n",
        "This provides a comprehensive view of the water level conditions that led to the barrier closures for Storms 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, and 19.\n",
        "\n",
        "## Setup\n"
      ],
      "id": "3202d726"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-closure-tide-data\n",
        "#| include: true\n",
        "\n",
        "# Load all data using shared functions\n",
        "print(\"Loading data...\")\n",
        "data = load_all_data()\n",
        "OCD = data['OCD']\n",
        "TSP = data['TSP']\n",
        "TIP = data['TIP']\n",
        "WLP = data['WLP']\n",
        "\n",
        "# Calculate surge\n",
        "SUP = WLP - TIP\n",
        "\n",
        "# Get storms DataFrame\n",
        "storms_df = get_or_create_storms_df()\n",
        "\n",
        "print(f\"\\nData loaded:\")\n",
        "print(f\"  Total closure dates: {len(OCD)} closures\")\n",
        "print(f\"  Time series points: {len(TSP):,}\")"
      ],
      "id": "load-closure-tide-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spatial Surge and Meteorological Maps\n",
        "\n",
        "This document provides spatial analysis of surge patterns and meteorological conditions for Storm 8 (closure event index 11), Storm 18 (closure event index 23, 2007), and Storm 19 (closure event index 24) for the Eastern Scheldt, converting the MATLAB script `master6.m` to Python. The analysis:\n",
        "\n",
        "1. Loads barrier closure dates to identify the storm events\n",
        "2. Loads GTSM reanalysis surge data (spatial surge patterns) for each storm\n",
        "3. Loads ERA5 meteorological reanalysis data (pressure and wind fields) for each storm\n",
        "4. Creates multiple visualizations for each storm:\n",
        "   - GTSM surge maps at 6 time steps around the closure\n",
        "   - ERA5 meteorological maps (pressure + wind vectors) at 6 time steps\n",
        "\n",
        "This provides a comprehensive view of the spatial patterns and meteorological conditions that led to the barrier closures for Storms 8, 18, and 19.\n"
      ],
      "id": "7a396afd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup-maps\n",
        "#| include: true\n",
        "\n",
        "import xarray as xr\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "print(f\"Analysis configuration:\")\n",
        "print(f\"  Storms to process: Storm 8 (e=11, prefix '8_'), Storm 18 (e=23, prefix '18_'), and Storm 19 (e=24, prefix '19_')\")\n",
        "print(f\"  GTSM time interval: {TINT} hours\")\n",
        "print(f\"  Meteorological time interval: {TINT_MET} hours\")\n",
        "print(f\"  Output directory: {output_dir}\")"
      ],
      "id": "setup-maps",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-closure-data\n",
        "#| include: true\n",
        "\n",
        "# Load barrier closure data using shared function\n",
        "OCD = load_master1_data()\n",
        "OCD = np.array(OCD)\n",
        "\n",
        "# Validate event indices for all storms\n",
        "e_storm8 = 11\n",
        "e_storm18 = 23\n",
        "e_storm19 = 24\n",
        "if e_storm8 >= len(OCD):\n",
        "    raise ValueError(f\"Event index {e_storm8} (Storm 8) is out of range. There are {len(OCD)} closure events (indices 0-{len(OCD)-1})\")\n",
        "if e_storm18 >= len(OCD):\n",
        "    raise ValueError(f\"Event index {e_storm18} (Storm 18) is out of range. There are {len(OCD)} closure events (indices 0-{len(OCD)-1})\")\n",
        "if e_storm19 >= len(OCD):\n",
        "    raise ValueError(f\"Event index {e_storm19} (Storm 19) is out of range. There are {len(OCD)} closure events (indices 0-{len(OCD)-1})\")\n",
        "\n",
        "print(f\"\\nData loaded:\")\n",
        "print(f\"  Total closure dates: {len(OCD)} closures\")\n",
        "print(f\"  Storm 8 closure date: {OCD[e_storm8]}\")\n",
        "print(f\"  Storm 18 closure date: {OCD[e_storm18]}\")\n",
        "print(f\"  Storm 19 closure date: {OCD[e_storm19]}\")"
      ],
      "id": "load-closure-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Storms \n",
        "\n",
        "## Storm 1\n",
        "\n",
        "Storm 1 uses datasets with prefix \"1_\" in the 2_DATA directory. The storm number corresponds directly to the row index in the closure dates table.\n",
        "\n",
        "### Water Level and Surge Analysis\n"
      ],
      "id": "9638d66e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 8
      },
      "source": [
        "#| label: plot-predicted-high-waters-zoomed-storm1\n",
        "#| fig-cap: Predicted high waters, observed water levels, and barrier closure (zoomed view) for Storm 1 - Eastern Scheldt\n",
        "\n",
        "# Load data\n",
        "data = load_all_data()\n",
        "OCD = data['OCD']\n",
        "TSP = data['TSP']\n",
        "TIP = data['TIP']\n",
        "WLP = data['WLP']\n",
        "storms_df = get_or_create_storms_df()\n",
        "\n",
        "# First storm: closure index 0, prefix '1_'\n",
        "e = 0\n",
        "closure_date = OCD[e]\n",
        "closure_dates = [closure_date]\n",
        "closure_dates_plus_one = [closure_date + timedelta(days=1)]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Plot vertical lines for closure date\n",
        "ax.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\n",
        "ax.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\n",
        "\n",
        "# Plot predicted tides, observed water levels, and surge\n",
        "ax.plot(TSP, TIP, 'r', linewidth=1, label='Predicted tide (TIP)', alpha=0.8)\n",
        "ax.plot(TSP, WLP, 'b', linewidth=1, label='Observed water level (WLP)', alpha=0.8)\n",
        "ax.plot(TSP, WLP - TIP, 'g', linewidth=1, label='Surge (WLP - TIP)', alpha=0.8)\n",
        "\n",
        "# Set zoom window: 2 days before to 3 days after closure\n",
        "ax.set_xlim(closure_date - timedelta(days=2), closure_date + timedelta(days=3))\n",
        "ax.set_ylim(-3, 4)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlabel('Date', fontweight='bold', fontsize=16)\n",
        "ax.set_ylabel('Level (m)', fontweight='bold', fontsize=16)\n",
        "ax.set_title('Storm 1 - Predicted High Waters (Zoomed View)', fontweight='bold', fontsize=18)\n",
        "ax.legend(fontsize=14, loc='best')\n",
        "ax.tick_params(labelsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig_file = os.path.join(output_dir, 'high_waters_storm1_predicted_high_waters_zoomed.png')\n",
        "plt.savefig(fig_file, dpi=150, bbox_inches='tight')\n",
        "print(f\"Figure saved to {fig_file}\")"
      ],
      "id": "plot-predicted-high-waters-zoomed-storm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 10
      },
      "source": [
        "#| label: plot-water-level-surge-storm1\n",
        "#| fig-cap: Water level and surge time series for Storm 1 closure event\n",
        "\n",
        "# First storm: closure index 0\n",
        "e = 0\n",
        "closure_date = OCD[e]\n",
        "closure_dates_plus_one = [closure_date + timedelta(days=1)]\n",
        "\n",
        "# Calculate surge\n",
        "SUP = WLP - TIP\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
        "\n",
        "# Top panel: Water level and predicted tide\n",
        "ax1.plot(TSP, WLP, 'b', linewidth=2, label='Water level')\n",
        "ax1.plot(TSP, TIP, 'r', linewidth=2, label='Astronomical tide')\n",
        "ax1.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\n",
        "ax1.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\n",
        "ax1.set_ylabel('Water level (m)', fontweight='bold', fontsize=20)\n",
        "ax1.set_title('Storm 1 - Water Level and Surge', fontweight='bold', fontsize=18)\n",
        "ax1.legend(fontsize=14)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.tick_params(labelsize=16)\n",
        "ax1.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n",
        "\n",
        "# Bottom panel: Surge\n",
        "surge_color = np.array([255, 103, 40]) / 255\n",
        "ax2.plot(TSP, SUP, color=surge_color, linewidth=2)\n",
        "ax2.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7)\n",
        "ax2.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7)\n",
        "ax2.set_xlabel('Date', fontweight='bold', fontsize=20)\n",
        "ax2.set_ylabel('Surge (m)', fontweight='bold', fontsize=20)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.tick_params(labelsize=16)\n",
        "ax2.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig_file = os.path.join(output_dir, 'high_waters_storm1_water_level_surge.png')\n",
        "plt.savefig(fig_file, dpi=150, bbox_inches='tight')\n",
        "print(f\"Figure saved to {fig_file}\")"
      ],
      "id": "plot-water-level-surge-storm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spatial Surge and Meteorological Maps\n"
      ],
      "id": "ba5f5006"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-spatial-data-storm1\n",
        "#| include: true\n",
        "\n",
        "# Load data\n",
        "OCD = load_master1_data()\n",
        "OCD = np.array(OCD)\n",
        "\n",
        "# First storm: closure index 0, prefix '1_'\n",
        "e = 0\n",
        "prefix = '1_'\n",
        "closure_date = OCD[e]\n",
        "closure_year = closure_date.year\n",
        "closure_month = closure_date.month\n",
        "gtsm_start_date = closure_date\n",
        "met_start_date = closure_date - timedelta(hours=6)\n",
        "\n",
        "# Load GTSM reanalysis surge data\n",
        "file_in = os.path.join(data_dir, '3_CODEC_GTSM', f'{prefix}reanalysis_surge_hourly_{closure_year}_{closure_month:02d}_v3.nc')\n",
        "ds_gtsm = xr.open_dataset(file_in)\n",
        "\n",
        "TS_GT = pd.to_datetime(ds_gtsm['time'].values)\n",
        "X_GT = ds_gtsm['station_x_coordinate'].values\n",
        "Y_GT = ds_gtsm['station_y_coordinate'].values\n",
        "SU_GT = ds_gtsm['surge'].values\n",
        "\n",
        "mask = (X_GT >= XB_GTSM[0]) & (X_GT <= XB_GTSM[1]) & (Y_GT >= YB_GTSM[0]) & (Y_GT <= YB_GTSM[1])\n",
        "X_GT = X_GT[mask]\n",
        "Y_GT = Y_GT[mask]\n",
        "SU_GT = SU_GT[:, mask]\n",
        "\n",
        "# Load ERA5 meteorological data\n",
        "filein_met = os.path.join(data_dir, '4_ERA5', f'{prefix}ERA5_{closure_year}_{closure_month:02d}.nc')\n",
        "ds_met = xr.open_dataset(filein_met)\n",
        "\n",
        "LON = ds_met['longitude'].values\n",
        "LAT = ds_met['latitude'].values\n",
        "P_MET = ds_met['msl'].values\n",
        "U_MET = ds_met['u10'].values\n",
        "V_MET = ds_met['v10'].values\n",
        "TS_MET = pd.to_datetime(ds_met['valid_time'].values)\n",
        "\n",
        "# Reorder to (lon, lat, time)\n",
        "dims = ds_met['msl'].dims\n",
        "if dims[0] == 'valid_time' and dims[1] == 'latitude' and dims[2] == 'longitude':\n",
        "    P_MET = np.transpose(P_MET, (2, 1, 0))\n",
        "    U_MET = np.transpose(U_MET, (2, 1, 0))\n",
        "    V_MET = np.transpose(V_MET, (2, 1, 0))\n",
        "\n",
        "X_MET, Y_MET = np.meshgrid(LON, LAT)\n",
        "X_MET = X_MET.T\n",
        "Y_MET = Y_MET.T\n",
        "\n",
        "lon_mask = (LON >= XB_MET[0]) & (LON <= XB_MET[1])\n",
        "lat_mask = (LAT >= YB_MET[0]) & (LAT <= YB_MET[1])\n",
        "lon_idx = np.where(lon_mask)[0]\n",
        "lat_idx = np.where(lat_mask)[0]"
      ],
      "id": "load-spatial-data-storm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: find-time-indices-storm1\n",
        "#| include: true\n",
        "\n",
        "# Find time indices for GTSM plots\n",
        "time_diffs = np.abs([(t - gtsm_start_date).total_seconds() for t in TS_GT])\n",
        "t_gtsm_base = np.argmin(time_diffs)\n",
        "t_gtsm = [t_gtsm_base, t_gtsm_base + TINT, t_gtsm_base + TINT*2, \n",
        "          t_gtsm_base + TINT*3, t_gtsm_base + TINT*4, t_gtsm_base + TINT*5]\n",
        "t_gtsm = [max(0, min(t, len(TS_GT)-1)) for t in t_gtsm]\n",
        "\n",
        "# Find time indices for meteorological plots\n",
        "max_time_idx = P_MET.shape[2] - 1\n",
        "valid_ts_met = TS_MET[:max_time_idx+1]\n",
        "target_times = [met_start_date + timedelta(hours=h) for h in [0, 12, 24, 36, 48, 60]]\n",
        "t_met = [np.argmin(np.abs([(t - target_time).total_seconds() for t in valid_ts_met])) for target_time in target_times]"
      ],
      "id": "find-time-indices-storm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 16,
        "fig-height": 10
      },
      "source": [
        "#| label: plot-gtsm-surge-storm1\n",
        "#| fig-cap: GTSM reanalysis surge maps at 6 time steps around Storm 1 closure event\n",
        "\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\n",
        "axes = [fig.add_subplot(gs[i // 3, i % 3]) for i in range(6)]\n",
        "\n",
        "for i, t_idx in enumerate(t_gtsm):\n",
        "    ax = axes[i]\n",
        "    scatter = ax.scatter(X_GT, Y_GT, c=SU_GT[t_idx, :], cmap='jet', s=15, vmin=-0.5, vmax=1.0)\n",
        "    ax.set_xlim(XB_GTSM)\n",
        "    ax.set_ylim(YB_GTSM)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.tick_params(labelsize=12)\n",
        "    ax.set_title(TS_GT[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n",
        "    if i >= 3:\n",
        "        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n",
        "    if i % 3 == 0:\n",
        "        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n",
        "\n",
        "cbar_ax = fig.add_subplot(gs[:, 3])\n",
        "cbar = plt.colorbar(scatter, cax=cbar_ax, orientation='vertical')\n",
        "cbar.set_label('Surge (m)', fontweight='bold', fontsize=14)\n",
        "cbar.ax.tick_params(labelsize=12)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.97, 1])\n",
        "plt.show()\n",
        "\n",
        "fig_file = os.path.join(output_dir, 'maps_storm1_gtsm_surge.png')\n",
        "plt.savefig(fig_file, dpi=100, bbox_inches='tight')\n",
        "print(f\"Figure saved to {fig_file}\")"
      ],
      "id": "plot-gtsm-surge-storm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 16,
        "fig-height": 10
      },
      "source": [
        "#| label: plot-era5-met-storm1\n",
        "#| fig-cap: ERA5 reanalysis pressure and wind fields at 6 time steps around Storm 1 closure event\n",
        "\n",
        "int_skip = 3\n",
        "X_sub = X_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\n",
        "Y_sub = Y_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\n",
        "X_vec = X_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\n",
        "Y_vec = Y_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\n",
        "\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\n",
        "axes = [fig.add_subplot(gs[i // 3, i % 3], projection=ccrs.PlateCarree()) for i in range(6)]\n",
        "\n",
        "ims = []\n",
        "for i, t_idx in enumerate(t_met):\n",
        "    ax = axes[i]\n",
        "    ax.add_feature(cfeature.COASTLINE.with_scale('50m'), linewidth=0.8, color='white')\n",
        "    ax.add_feature(cfeature.BORDERS.with_scale('50m'), linewidth=0.5, color='white')\n",
        "    \n",
        "    P_sub = P_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1, t_idx] / 100\n",
        "    U_vec = U_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n",
        "    V_vec = V_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n",
        "    \n",
        "    im = ax.pcolormesh(X_sub, Y_sub, P_sub, cmap='jet', shading='gouraud', \n",
        "                       vmin=967, vmax=1020, transform=ccrs.PlateCarree())\n",
        "    ims.append(im)\n",
        "    ax.quiver(X_vec, Y_vec, U_vec, V_vec, color='k', scale=1200, width=0.002, \n",
        "              transform=ccrs.PlateCarree())\n",
        "    \n",
        "    ax.set_xlim(XB_MET)\n",
        "    ax.set_ylim(YB_MET)\n",
        "    ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.3, linestyle='--')\n",
        "    ax.tick_params(labelsize=12)\n",
        "    ax.set_title(TS_MET[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n",
        "    if i >= 3:\n",
        "        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n",
        "    if i % 3 == 0:\n",
        "        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n",
        "\n",
        "cbar_ax = fig.add_subplot(gs[:, 3])\n",
        "cbar = plt.colorbar(ims[0], cax=cbar_ax, orientation='vertical')\n",
        "cbar.set_label('Pressure (hPa)', fontweight='bold', fontsize=14)\n",
        "cbar.ax.tick_params(labelsize=12)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.97, 1])\n",
        "plt.show()\n",
        "\n",
        "fig_file = os.path.join(output_dir, 'maps_storm1_era5_meteorology.png')\n",
        "plt.savefig(fig_file, dpi=100, bbox_inches='tight')\n",
        "print(f\"Figure saved to {fig_file}\")"
      ],
      "id": "plot-era5-met-storm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Storm 2\n",
        "\n",
        "Storm 2 uses datasets with prefix \"2_\" in the 2_DATA directory. The storm number corresponds directly to the row index in the closure dates table.\n",
        "\n",
        "### Water Level and Surge Analysis\n"
      ],
      "id": "9d3296cc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 8
      },
      "source": [
        "#| label: plot-predicted-high-waters-zoomed-storm2\n",
        "#| fig-cap: Predicted high waters, observed water levels, and barrier closure (zoomed view) for Storm 2 - Eastern Scheldt\n",
        "\n",
        "# Load data\n",
        "data = load_all_data()\n",
        "OCD = data['OCD']\n",
        "TSP = data['TSP']\n",
        "TIP = data['TIP']\n",
        "WLP = data['WLP']\n",
        "\n",
        "# Second storm: closure index 1, prefix '2_'\n",
        "e = 1\n",
        "closure_date = OCD[e]\n",
        "closure_dates = [closure_date]\n",
        "closure_dates_plus_one = [closure_date + timedelta(days=1)]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Plot vertical lines for closure date\n",
        "ax.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\n",
        "ax.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\n",
        "\n",
        "# Plot predicted tides, observed water levels, and surge\n",
        "ax.plot(TSP, TIP, 'r', linewidth=1, label='Predicted tide (TIP)', alpha=0.8)\n",
        "ax.plot(TSP, WLP, 'b', linewidth=1, label='Observed water level (WLP)', alpha=0.8)\n",
        "ax.plot(TSP, WLP - TIP, 'g', linewidth=1, label='Surge (WLP - TIP)', alpha=0.8)\n",
        "\n",
        "# Set zoom window: 2 days before to 3 days after closure\n",
        "ax.set_xlim(closure_date - timedelta(days=2), closure_date + timedelta(days=3))\n",
        "ax.set_ylim(-3, 4)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlabel('Date', fontweight='bold', fontsize=16)\n",
        "ax.set_ylabel('Level (m)', fontweight='bold', fontsize=16)\n",
        "ax.set_title('Storm 2 - Predicted High Waters (Zoomed View)', fontweight='bold', fontsize=18)\n",
        "ax.legend(fontsize=14, loc='best')\n",
        "ax.tick_params(labelsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig_file = os.path.join(output_dir, 'high_waters_storm2_predicted_high_waters_zoomed.png')\n",
        "plt.savefig(fig_file, dpi=150, bbox_inches='tight')\n",
        "print(f\"Figure saved to {fig_file}\")"
      ],
      "id": "plot-predicted-high-waters-zoomed-storm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 10
      },
      "source": [
        "#| label: plot-water-level-surge-storm2\n",
        "#| fig-cap: Water level and surge time series for Storm 2 closure event\n",
        "\n",
        "# Second storm: closure index 1\n",
        "e = 1\n",
        "closure_date = OCD[e]\n",
        "closure_dates_plus_one = [closure_date + timedelta(days=1)]\n",
        "\n",
        "# Calculate surge\n",
        "SUP = WLP - TIP\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
        "\n",
        "# Top panel: Water level and predicted tide\n",
        "ax1.plot(TSP, WLP, 'b', linewidth=2, label='Water level')\n",
        "ax1.plot(TSP, TIP, 'r', linewidth=2, label='Astronomical tide')\n",
        "ax1.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\n",
        "ax1.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\n",
        "ax1.set_ylabel('Water level (m)', fontweight='bold', fontsize=20)\n",
        "ax1.set_title('Storm 2 - Water Level and Surge', fontweight='bold', fontsize=18)\n",
        "ax1.legend(fontsize=14)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.tick_params(labelsize=16)\n",
        "ax1.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n",
        "\n",
        "# Bottom panel: Surge\n",
        "surge_color = np.array([255, 103, 40]) / 255\n",
        "ax2.plot(TSP, SUP, color=surge_color, linewidth=2)\n",
        "ax2.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7)\n",
        "ax2.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7)\n",
        "ax2.set_xlabel('Date', fontweight='bold', fontsize=20)\n",
        "ax2.set_ylabel('Surge (m)', fontweight='bold', fontsize=20)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.tick_params(labelsize=16)\n",
        "ax2.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig_file = os.path.join(output_dir, 'high_waters_storm2_water_level_surge.png')\n",
        "plt.savefig(fig_file, dpi=150, bbox_inches='tight')\n",
        "print(f\"Figure saved to {fig_file}\")"
      ],
      "id": "plot-water-level-surge-storm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spatial Surge and Meteorological Maps\n"
      ],
      "id": "0b275d38"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-spatial-data-storm2\n",
        "#| include: true\n",
        "\n",
        "# Load data\n",
        "OCD = load_master1_data()\n",
        "OCD = np.array(OCD)\n",
        "\n",
        "# Second storm: closure index 1, prefix '2_'\n",
        "e = 1\n",
        "prefix = '2_'\n",
        "closure_date = OCD[e]\n",
        "closure_year = closure_date.year\n",
        "closure_month = closure_date.month\n",
        "gtsm_start_date = closure_date\n",
        "met_start_date = closure_date - timedelta(hours=6)\n",
        "\n",
        "# Load GTSM reanalysis surge data\n",
        "file_in = os.path.join(data_dir, '3_CODEC_GTSM', f'{prefix}reanalysis_surge_hourly_{closure_year}_{closure_month:02d}_v3.nc')\n",
        "ds_gtsm = xr.open_dataset(file_in)\n",
        "\n",
        "TS_GT = pd.to_datetime(ds_gtsm['time'].values)\n",
        "X_GT = ds_gtsm['station_x_coordinate'].values\n",
        "Y_GT = ds_gtsm['station_y_coordinate'].values\n",
        "SU_GT = ds_gtsm['surge'].values\n",
        "\n",
        "mask = (X_GT >= XB_GTSM[0]) & (X_GT <= XB_GTSM[1]) & (Y_GT >= YB_GTSM[0]) & (Y_GT <= YB_GTSM[1])\n",
        "X_GT = X_GT[mask]\n",
        "Y_GT = Y_GT[mask]\n",
        "SU_GT = SU_GT[:, mask]\n",
        "\n",
        "# Load ERA5 meteorological data\n",
        "filein_met = os.path.join(data_dir, '4_ERA5', f'{prefix}ERA5_{closure_year}_{closure_month:02d}.nc')\n",
        "ds_met = xr.open_dataset(filein_met)\n",
        "\n",
        "LON = ds_met['longitude'].values\n",
        "LAT = ds_met['latitude'].values\n",
        "P_MET = ds_met['msl'].values\n",
        "U_MET = ds_met['u10'].values\n",
        "V_MET = ds_met['v10'].values\n",
        "TS_MET = pd.to_datetime(ds_met['valid_time'].values)\n",
        "\n",
        "# Reorder to (lon, lat, time)\n",
        "dims = ds_met['msl'].dims\n",
        "if dims[0] == 'valid_time' and dims[1] == 'latitude' and dims[2] == 'longitude':\n",
        "    P_MET = np.transpose(P_MET, (2, 1, 0))\n",
        "    U_MET = np.transpose(U_MET, (2, 1, 0))\n",
        "    V_MET = np.transpose(V_MET, (2, 1, 0))\n",
        "\n",
        "X_MET, Y_MET = np.meshgrid(LON, LAT)\n",
        "X_MET = X_MET.T\n",
        "Y_MET = Y_MET.T\n",
        "\n",
        "lon_mask = (LON >= XB_MET[0]) & (LON <= XB_MET[1])\n",
        "lat_mask = (LAT >= YB_MET[0]) & (LAT <= YB_MET[1])\n",
        "lon_idx = np.where(lon_mask)[0]\n",
        "lat_idx = np.where(lat_mask)[0]"
      ],
      "id": "load-spatial-data-storm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: find-time-indices-storm2\n",
        "#| include: true\n",
        "\n",
        "# Find time indices for GTSM plots\n",
        "time_diffs = np.abs([(t - gtsm_start_date).total_seconds() for t in TS_GT])\n",
        "t_gtsm_base = np.argmin(time_diffs)\n",
        "t_gtsm = [t_gtsm_base, t_gtsm_base + TINT, t_gtsm_base + TINT*2, \n",
        "          t_gtsm_base + TINT*3, t_gtsm_base + TINT*4, t_gtsm_base + TINT*5]\n",
        "t_gtsm = [max(0, min(t, len(TS_GT)-1)) for t in t_gtsm]\n",
        "\n",
        "# Find time indices for meteorological plots\n",
        "max_time_idx = P_MET.shape[2] - 1\n",
        "valid_ts_met = TS_MET[:max_time_idx+1]\n",
        "target_times = [met_start_date + timedelta(hours=h) for h in [0, 12, 24, 36, 48, 60]]\n",
        "t_met = [np.argmin(np.abs([(t - target_time).total_seconds() for t in valid_ts_met])) for target_time in target_times]"
      ],
      "id": "find-time-indices-storm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 16,
        "fig-height": 10
      },
      "source": [
        "#| label: plot-gtsm-surge-storm2\n",
        "#| fig-cap: GTSM reanalysis surge maps at 6 time steps around Storm 2 closure event\n",
        "\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\n",
        "axes = [fig.add_subplot(gs[i // 3, i % 3]) for i in range(6)]\n",
        "\n",
        "for i, t_idx in enumerate(t_gtsm):\n",
        "    ax = axes[i]\n",
        "    scatter = ax.scatter(X_GT, Y_GT, c=SU_GT[t_idx, :], cmap='jet', s=15, vmin=-0.5, vmax=1.0)\n",
        "    ax.set_xlim(XB_GTSM)\n",
        "    ax.set_ylim(YB_GTSM)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.tick_params(labelsize=12)\n",
        "    ax.set_title(TS_GT[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n",
        "    if i >= 3:\n",
        "        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n",
        "    if i % 3 == 0:\n",
        "        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n",
        "\n",
        "cbar_ax = fig.add_subplot(gs[:, 3])\n",
        "cbar = plt.colorbar(scatter, cax=cbar_ax, orientation='vertical')\n",
        "cbar.set_label('Surge (m)', fontweight='bold', fontsize=14)\n",
        "cbar.ax.tick_params(labelsize=12)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.97, 1])\n",
        "plt.show()\n",
        "\n",
        "fig_file = os.path.join(output_dir, 'maps_storm2_gtsm_surge.png')\n",
        "plt.savefig(fig_file, dpi=100, bbox_inches='tight')\n",
        "print(f\"Figure saved to {fig_file}\")"
      ],
      "id": "plot-gtsm-surge-storm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 16,
        "fig-height": 10
      },
      "source": [
        "#| label: plot-era5-met-storm2\n",
        "#| fig-cap: ERA5 reanalysis pressure and wind fields at 6 time steps around Storm 2 closure event\n",
        "\n",
        "int_skip = 3\n",
        "X_sub = X_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\n",
        "Y_sub = Y_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\n",
        "X_vec = X_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\n",
        "Y_vec = Y_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\n",
        "\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\n",
        "axes = [fig.add_subplot(gs[i // 3, i % 3], projection=ccrs.PlateCarree()) for i in range(6)]\n",
        "\n",
        "ims = []\n",
        "for i, t_idx in enumerate(t_met):\n",
        "    ax = axes[i]\n",
        "    ax.add_feature(cfeature.COASTLINE.with_scale('50m'), linewidth=0.8, color='white')\n",
        "    ax.add_feature(cfeature.BORDERS.with_scale('50m'), linewidth=0.5, color='white')\n",
        "    \n",
        "    P_sub = P_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1, t_idx] / 100\n",
        "    U_vec = U_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n",
        "    V_vec = V_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n",
        "    \n",
        "    im = ax.pcolormesh(X_sub, Y_sub, P_sub, cmap='jet', shading='gouraud', \n",
        "                       vmin=967, vmax=1020, transform=ccrs.PlateCarree())\n",
        "    ims.append(im)\n",
        "    ax.quiver(X_vec, Y_vec, U_vec, V_vec, color='k', scale=1200, width=0.002, \n",
        "              transform=ccrs.PlateCarree())\n",
        "    \n",
        "    ax.set_xlim(XB_MET)\n",
        "    ax.set_ylim(YB_MET)\n",
        "    ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.3, linestyle='--')\n",
        "    ax.tick_params(labelsize=12)\n",
        "    ax.set_title(TS_MET[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n",
        "    if i >= 3:\n",
        "        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n",
        "    if i % 3 == 0:\n",
        "        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n",
        "\n",
        "cbar_ax = fig.add_subplot(gs[:, 3])\n",
        "cbar = plt.colorbar(ims[0], cax=cbar_ax, orientation='vertical')\n",
        "cbar.set_label('Pressure (hPa)', fontweight='bold', fontsize=14)\n",
        "cbar.ax.tick_params(labelsize=12)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.97, 1])\n",
        "plt.show()\n",
        "\n",
        "fig_file = os.path.join(output_dir, 'maps_storm2_era5_meteorology.png')\n",
        "plt.savefig(fig_file, dpi=100, bbox_inches='tight')\n",
        "print(f\"Figure saved to {fig_file}\")"
      ],
      "id": "plot-era5-met-storm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Storm 3\n",
        "\n",
        "Storm 3 uses datasets with prefix \"3_\" in the 2_DATA directory. The storm number corresponds directly to the row index in the closure dates table.\n",
        "\n",
        "### Water Level and Surge Analysis\n"
      ],
      "id": "f398c15c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 8
      },
      "source": [
        "#| label: plot-predicted-high-waters-zoomed-storm3\n",
        "#| fig-cap: Predicted high waters, observed water levels, and barrier closure (zoomed view) for Storm 3 - Eastern Scheldt\n",
        "\n",
        "# Load data\n",
        "data = load_all_data()\n",
        "OCD = data['OCD']\n",
        "TSP = data['TSP']\n",
        "TIP = data['TIP']\n",
        "WLP = data['WLP']\n",
        "\n",
        "# Third storm: closure index 2, prefix '3_'\n",
        "e = 2\n",
        "closure_date = OCD[e]\n",
        "closure_dates = [closure_date]\n",
        "closure_dates_plus_one = [closure_date + timedelta(days=1)]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Plot vertical lines for closure date\n",
        "ax.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\n",
        "ax.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\n",
        "\n",
        "# Plot predicted tides, observed water levels, and surge\n",
        "ax.plot(TSP, TIP, 'r', linewidth=1, label='Predicted tide (TIP)', alpha=0.8)\n",
        "ax.plot(TSP, WLP, 'b', linewidth=1, label='Observed water level (WLP)', alpha=0.8)\n",
        "ax.plot(TSP, WLP - TIP, 'g', linewidth=1, label='Surge (WLP - TIP)', alpha=0.8)\n",
        "\n",
        "# Set zoom window: 2 days before to 3 days after closure\n",
        "ax.set_xlim(closure_date - timedelta(days=2), closure_date + timedelta(days=3))\n",
        "ax.set_ylim(-3, 4)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlabel('Date', fontweight='bold', fontsize=16)\n",
        "ax.set_ylabel('Level (m)', fontweight='bold', fontsize=16)\n",
        "ax.set_title('Storm 3 - Predicted High Waters (Zoomed View)', fontweight='bold', fontsize=18)\n",
        "ax.legend(fontsize=14, loc='best')\n",
        "ax.tick_params(labelsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig_file = os.path.join(output_dir, 'high_waters_storm3_predicted_high_waters_zoomed.png')\n",
        "plt.savefig(fig_file, dpi=150, bbox_inches='tight')\n",
        "print(f\"Figure saved to {fig_file}\")"
      ],
      "id": "plot-predicted-high-waters-zoomed-storm3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 10
      },
      "source": [
        "#| label: plot-water-level-surge-storm3\n",
        "#| fig-cap: Water level and surge time series for Storm 3 closure event\n",
        "\n",
        "# Third storm: closure index 2\n",
        "e = 2\n",
        "closure_date = OCD[e]\n",
        "closure_dates_plus_one = [closure_date + timedelta(days=1)]\n",
        "\n",
        "# Calculate surge\n",
        "SUP = WLP - TIP\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
        "\n",
        "# Top panel: Water level and predicted tide\n",
        "ax1.plot(TSP, WLP, 'b', linewidth=2, label='Water level')\n",
        "ax1.plot(TSP, TIP, 'r', linewidth=2, label='Astronomical tide')\n",
        "ax1.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\n",
        "ax1.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\n",
        "ax1.set_ylabel('Water level (m)', fontweight='bold', fontsize=20)\n",
        "ax1.set_title('Storm 3 - Water Level and Surge', fontweight='bold', fontsize=18)\n",
        "ax1.legend(fontsize=14)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.tick_params(labelsize=16)\n",
        "ax1.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n",
        "\n",
        "# Bottom panel: Surge\n",
        "surge_color = np.array([255, 103, 40]) / 255\n",
        "ax2.plot(TSP, SUP, color=surge_color, linewidth=2)\n",
        "ax2.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7)\n",
        "ax2.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7)\n",
        "ax2.set_xlabel('Date', fontweight='bold', fontsize=20)\n",
        "ax2.set_ylabel('Surge (m)', fontweight='bold', fontsize=20)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.tick_params(labelsize=16)\n",
        "ax2.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig_file = os.path.join(output_dir, 'high_waters_storm3_water_level_surge.png')\n",
        "plt.savefig(fig_file, dpi=150, bbox_inches='tight')\n",
        "print(f\"Figure saved to {fig_file}\")"
      ],
      "id": "plot-water-level-surge-storm3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spatial Surge and Meteorological Maps\n"
      ],
      "id": "7ace979f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-spatial-data-storm3\n",
        "#| include: true\n",
        "\n",
        "# Load data\n",
        "OCD = load_master1_data()\n",
        "OCD = np.array(OCD)\n",
        "\n",
        "# Third storm: closure index 2, prefix '3_'\n",
        "e = 2\n",
        "prefix = '3_'\n",
        "closure_date = OCD[e]\n",
        "closure_year = closure_date.year\n",
        "closure_month = closure_date.month\n",
        "gtsm_start_date = closure_date\n",
        "met_start_date = closure_date - timedelta(hours=6)\n",
        "\n",
        "# Load GTSM reanalysis surge data\n",
        "file_in = os.path.join(data_dir, '3_CODEC_GTSM', f'{prefix}reanalysis_surge_hourly_{closure_year}_{closure_month:02d}_v3.nc')\n",
        "ds_gtsm = xr.open_dataset(file_in)\n",
        "\n",
        "TS_GT = pd.to_datetime(ds_gtsm['time'].values)\n",
        "X_GT = ds_gtsm['station_x_coordinate'].values\n",
        "Y_GT = ds_gtsm['station_y_coordinate'].values\n",
        "SU_GT = ds_gtsm['surge'].values\n",
        "\n",
        "mask = (X_GT >= XB_GTSM[0]) & (X_GT <= XB_GTSM[1]) & (Y_GT >= YB_GTSM[0]) & (Y_GT <= YB_GTSM[1])\n",
        "X_GT = X_GT[mask]\n",
        "Y_GT = Y_GT[mask]\n",
        "SU_GT = SU_GT[:, mask]\n",
        "\n",
        "# Load ERA5 meteorological data\n",
        "filein_met = os.path.join(data_dir, '4_ERA5', f'{prefix}ERA5_{closure_year}_{closure_month:02d}.nc')\n",
        "ds_met = xr.open_dataset(filein_met)\n",
        "\n",
        "LON = ds_met['longitude'].values\n",
        "LAT = ds_met['latitude'].values\n",
        "P_MET = ds_met['msl'].values\n",
        "U_MET = ds_met['u10'].values\n",
        "V_MET = ds_met['v10'].values\n",
        "TS_MET = pd.to_datetime(ds_met['valid_time'].values)\n",
        "\n",
        "# Reorder to (lon, lat, time)\n",
        "dims = ds_met['msl'].dims\n",
        "if dims[0] == 'valid_time' and dims[1] == 'latitude' and dims[2] == 'longitude':\n",
        "    P_MET = np.transpose(P_MET, (2, 1, 0))\n",
        "    U_MET = np.transpose(U_MET, (2, 1, 0))\n",
        "    V_MET = np.transpose(V_MET, (2, 1, 0))\n",
        "\n",
        "X_MET, Y_MET = np.meshgrid(LON, LAT)\n",
        "X_MET = X_MET.T\n",
        "Y_MET = Y_MET.T\n",
        "\n",
        "lon_mask = (LON >= XB_MET[0]) & (LON <= XB_MET[1])\n",
        "lat_mask = (LAT >= YB_MET[0]) & (LAT <= YB_MET[1])\n",
        "lon_idx = np.where(lon_mask)[0]\n",
        "lat_idx = np.where(lat_mask)[0]"
      ],
      "id": "load-spatial-data-storm3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: find-time-indices-storm3\n",
        "#| include: true\n",
        "\n",
        "# Find time indices for GTSM plots\n",
        "time_diffs = np.abs([(t - gtsm_start_date).total_seconds() for t in TS_GT])\n",
        "t_gtsm_base = np.argmin(time_diffs)\n",
        "t_gtsm = [t_gtsm_base, t_gtsm_base + TINT, t_gtsm_base + TINT*2, \n",
        "          t_gtsm_base + TINT*3, t_gtsm_base + TINT*4, t_gtsm_base + TINT*5]\n",
        "t_gtsm = [max(0, min(t, len(TS_GT)-1)) for t in t_gtsm]\n",
        "\n",
        "# Find time indices for meteorological plots\n",
        "max_time_idx = P_MET.shape[2] - 1\n",
        "valid_ts_met = TS_MET[:max_time_idx+1]\n",
        "target_times = [met_start_date + timedelta(hours=h) for h in [0, 12, 24, 36, 48, 60]]\n",
        "t_met = [np.argmin(np.abs([(t - target_time).total_seconds() for t in valid_ts_met])) for target_time in target_times]"
      ],
      "id": "find-time-indices-storm3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 16,
        "fig-height": 10
      },
      "source": [
        "#| label: plot-gtsm-surge-storm3\n",
        "#| fig-cap: GTSM reanalysis surge maps at 6 time steps around Storm 3 closure event\n",
        "\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\n",
        "axes = [fig.add_subplot(gs[i // 3, i % 3]) for i in range(6)]\n",
        "\n",
        "for i, t_idx in enumerate(t_gtsm):\n",
        "    ax = axes[i]\n",
        "    scatter = ax.scatter(X_GT, Y_GT, c=SU_GT[t_idx, :], cmap='jet', s=15, vmin=-0.5, vmax=1.0)\n",
        "    ax.set_xlim(XB_GTSM)\n",
        "    ax.set_ylim(YB_GTSM)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.tick_params(labelsize=12)\n",
        "    ax.set_title(TS_GT[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n",
        "    if i >= 3:\n",
        "        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n",
        "    if i % 3 == 0:\n",
        "        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n",
        "\n",
        "cbar_ax = fig.add_subplot(gs[:, 3])\n",
        "cbar = plt.colorbar(scatter, cax=cbar_ax, orientation='vertical')\n",
        "cbar.set_label('Surge (m)', fontweight='bold', fontsize=14)\n",
        "cbar.ax.tick_params(labelsize=12)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.97, 1])\n",
        "plt.show()\n",
        "\n",
        "fig_file = os.path.join(output_dir, 'maps_storm3_gtsm_surge.png')\n",
        "plt.savefig(fig_file, dpi=100, bbox_inches='tight')\n",
        "print(f\"Figure saved to {fig_file}\")"
      ],
      "id": "plot-gtsm-surge-storm3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 16,
        "fig-height": 10
      },
      "source": [
        "#| label: plot-era5-met-storm3\n",
        "#| fig-cap: ERA5 reanalysis pressure and wind fields at 6 time steps around Storm 3 closure event\n",
        "\n",
        "int_skip = 3\n",
        "X_sub = X_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\n",
        "Y_sub = Y_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\n",
        "X_vec = X_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\n",
        "Y_vec = Y_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\n",
        "\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\n",
        "axes = [fig.add_subplot(gs[i // 3, i % 3], projection=ccrs.PlateCarree()) for i in range(6)]\n",
        "\n",
        "ims = []\n",
        "for i, t_idx in enumerate(t_met):\n",
        "    ax = axes[i]\n",
        "    ax.add_feature(cfeature.COASTLINE.with_scale('50m'), linewidth=0.8, color='white')\n",
        "    ax.add_feature(cfeature.BORDERS.with_scale('50m'), linewidth=0.5, color='white')\n",
        "    \n",
        "    P_sub = P_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1, t_idx] / 100\n",
        "    U_vec = U_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n",
        "    V_vec = V_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n",
        "    \n",
        "    im = ax.pcolormesh(X_sub, Y_sub, P_sub, cmap='jet', shading='gouraud', \n",
        "                       vmin=967, vmax=1020, transform=ccrs.PlateCarree())\n",
        "    ims.append(im)\n",
        "    ax.quiver(X_vec, Y_vec, U_vec, V_vec, color='k', scale=1200, width=0.002, \n",
        "              transform=ccrs.PlateCarree())\n",
        "    \n",
        "    ax.set_xlim(XB_MET)\n",
        "    ax.set_ylim(YB_MET)\n",
        "    ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.3, linestyle='--')\n",
        "    ax.tick_params(labelsize=12)\n",
        "    ax.set_title(TS_MET[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n",
        "    if i >= 3:\n",
        "        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n",
        "    if i % 3 == 0:\n",
        "        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n",
        "\n",
        "cbar_ax = fig.add_subplot(gs[:, 3])\n",
        "cbar = plt.colorbar(ims[0], cax=cbar_ax, orientation='vertical')\n",
        "cbar.set_label('Pressure (hPa)', fontweight='bold', fontsize=14)\n",
        "cbar.ax.tick_params(labelsize=12)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.97, 1])\n",
        "plt.show()\n",
        "\n",
        "fig_file = os.path.join(output_dir, 'maps_storm3_era5_meteorology.png')\n",
        "plt.savefig(fig_file, dpi=100, bbox_inches='tight')\n",
        "print(f\"Figure saved to {fig_file}\")"
      ],
      "id": "plot-era5-met-storm3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/miniconda3/envs/objective1/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}